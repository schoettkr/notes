* DONE Beschreiben Sie IDF und TF
- Side note: In *jeder* Altklausur besteht die erste Aufgabe aus der Beschreibung von IDF. In einigen Klausuren wird zusätzlich noch die Beschreibung von TF verlangt

*Wikipedia Engl Summary*\\
In information retrieval, *tf–idf* or *TFIDF*, short for *term frequency–inverse document frequency*, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf.

Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.

One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.

*TF (Term Frequency)*: Suppose we have a set of English text documents and wish to rank which document is most relevant to the query, "the brown cow". A simple way to start out is by eliminating documents that do not contain all three words "the", "brown", and "cow", but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its term frequency. However, in the case where the length of documents varies greatly, adjustments are often made (see definition below). The first form of term weighting is due to Hans Peter Luhn (1957) which may be summarized as: /The weight of a term that occurs in a document is simply proportional to the term frequency./

*IDF (Inverse Document Frequency)*: Because the term "the" is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word "the" more frequently, without giving enough weight to the more meaningful terms "brown" and "cow". The term "the" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less-common words "brown" and "cow". Hence an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.

Karen Spärck Jones (1972) conceived a statistical interpretation of term specificity called Inverse Document Frequency (idf), which became a cornerstone of term weighting: /The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs./

------

*Wikipedia Ger Summary*\\
Das Tf-idf-Maß (von englisch term frequency ‚Vorkommenshäufigkeit‘ und inverse document frequency ‚inverse Dokumenthäufigkeit‘) wird im Information Retrieval zur Beurteilung der Relevanz von Termen in Dokumenten einer Dokumentenkollektion eingesetzt.

Mit der so errechneten Gewichtung eines Wortes bezüglich des Dokuments, in welchem es enthalten ist, können Dokumente als Suchtreffer einer wortbasierten Suche besser in der Trefferliste angeordnet werden, als es beispielsweise über die Termfrequenz allein möglich wäre.

*IDF:* Die Inverse Dokumenthäufigkeit (englisch Inverse Document Frequency (IDF)) dient beim Information Retrieval zur Bestimmung der Trennfähigkeit eines Wortes bzw. Termes für die Indexierung von Dokumenten. Ein Wort, das nur in wenigen Dokumenten oft vorkommt, ist geeigneter als eines, das in fast jedem Dokument oder nur sehr gering auftritt. Zusammen mit der Termfrequenz (siehe Tf-idf-Maß) wird sie zur Gewichtung von Wörtern bei der Automatischen Indexierung eingesetzt.\\
Die Inverse Dokumenthäufigkeit lässt sich berechnen als $$IDF_t = \log(\frac{N_D}{f_t})$$
wobei N_{D} die Anzahl der Dokumente bezeichnet und f_{t} die Anzahl der Dokumente, die den Term t enthalten. Wenn die Dokumentenhäufigkeit wächst, wird der Bruch kleiner.\\

Die inverse Dokumenthäufigkeit misst die Spezifität eines Terms für die Gesamtmenge der betrachteten Dokumente. Ein übereinstimmendes Vorkommen von seltenen Begriffen ist für die Relevanz aussagekräftiger als eine Übereinstimmung bei sehr häufigen Worten (z. B. "und" oder "ein").
Die inverse Dokumentfrequenz =idf(t)=  eines Terms =t= hängt nicht vom einzelnen Dokument, sondern vom Dokumentkorpus (der Gesamtmenge aller Dokumente im Retrievalszenario) ab.

*TF:* Die Vorkommenshäufigkeit (auch Suchwortdichte genannt) =#(t,D)= gibt an, wie häufig der Term =t= im Dokument =D= vorkommt. Ist beispielsweise das Dokument =D_{i}= der Satz
"/Das rote Auto hält an der roten Ampel./'" dann ist  ~#(rot, D_i)=2~. Um eine Verzerrung des Ergebnisses in langen Dokumenten zu verhindern, ist es möglich, die absolute Vorkommenshäufigkeit =#(t,D)=  zu normalisieren. Dazu wird die Anzahl der Vorkommen von Term =t= in Dokument =D= durch die maximale Häufigkeit eines Terms in =D= geteilt und man erhält die relative Vorkommenshäufigkeit =tf(t,D)=.
* DONE Erläutern Sie das Zipf'sche Gesetz, die Luhn'sche Termgewichtung und ihre praktische Bedeutung
- Side note: Diese Aufgabe ist nur in 2 Altklausren.

*Zipfsch'sches Gesetz*:
- Zipf's law is an empirical law formulated using mathematical statistics that refers to the fact that many types of data studied in the physical and social sciences can be approximated with a Zipfian distribution, one of a family of related discrete power law probability distributions. Zipf distribution is related to the zeta distribution, but is not identical.
- Zipf's law was originally formulated in terms of quantitative linguistics, stating that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.: the rank-frequency distribution is an inverse relation. For example, in the Brown Corpus of American English text, the word the is the most frequently occurring word, and by itself accounts for nearly 7% of all word occurrences (69,971 out of slightly over 1 million). True to Zipf's Law, the second-place word of accounts for slightly over 3.5% of words (36,411 occurrences), followed by and (28,852). Only 135 vocabulary items are needed to account for half the Brown Corpus.
- Die vereinfachte Aussage des Zipfschen Gesetzes lautet: Wenn die Elemente einer Menge – beispielsweise die Wörter eines Textes – nach ihrer Häufigkeit geordnet werden, ist die Wahrscheinlichkeit p ihres Auftretens umgekehrt proportional zur Position n innerhalb der Rangfolge: \(p(n) \sim \frac{1}{n}.\)

*Luhn'sche Termgewichtung*:
- Token (Einzelne Wörter) werden statistisch nach Häufigkeit untersucht. Hochfrequenz Wörter bspw. the, of, to dabei abgeschnitten. Auf der anderen Seite kann man auch Wörter mit einer niedrigeren Frequenz abschneiden, aber dadurch besteht die Gefahr insbesondere sehr spezielle Terme zu vernachlässigen. Wird daher nicht of gemacht.
- Terme in einem Dokument haben einen unterschiedlichen Stellenwert, für die meisten IR-Modelle gilt es daher einen quantitativen Ausdruck für die Wichtigkeit eines Term im Kontext eines Dokuments zu finden (=Termgewichtung).
- Laut Luhn (1957) ist die Häufigkeit eines Terms in einem Text ein Maß für seine Relevanz. Jedoch besagt die These von Luhn /nicht/, dass Termhäufigkeit und Relevanz positiv miteinander korrelieren. Der Ansatz von einfacher Termhäufigkeit weist das Defizit auf, dass er alle Terme als gleichwichtig bei der Relevanzbetrachtung einordnet. Vielmehr wird ein Term, der in allen Dokumenten vorkommt als ein allgemeiner Term angesehen, weil er nicht zur Unterscheidung der Dokumente genutzt werden kann. Daher ist es nötig das Gewicht eines Term der in vielen Dokumenten vorkommt zu reduzieren, hierfür kann die TF-IDF genutzt werden (siehe oben)
* TODO Beschreiben Sie die prinzipiellen Möglichkeiten des CBIR
- Side note: Diese Aufgabe ist in 3 Altklausren.
* TODO Erläutern Sie den Unterschied zwischen N-Gram-Stemmern und Affix-Stemmern.
- Side note: Diese Aufgabe ist in 3 Altklausren und zwar immer dann, wenn die Frage nach Hauptmethoden der Stammformreduktion nicht auftritt (selbes Thema).
* TODO Welche Hauptmethoden der Stammformreduktion gibt es?
- Side note: Diese Aufgabe ist in 4 Altklausuren und zwar immer dann, wenn die Frage nach N-Gram-Stemmern und Affix-Stemmern nicht auftritt (selbes Thema).
- Alternativ: Nennen Sie 3 Arten von Stemmern.
* TODO Erklären Sie die Maße Recall und Precision.
- Side note: Diese Aufgabe ist nur in 2 Altklausuren.
https://de.qwe.wiki/wiki/Precision_and_recall#Recall
* DONE Beschreiben Sie Vorteile und Funktionsweise des twin-comparison-Verfahrens.
- Side note: Diese Aufgabe ist in 3 Altklausuren.
- The twin-comparison algorithm uses the difference between consecutive frames to detect a cut, and the accumulated difference over a sequence of frames to detect gradual transitions
- http://www.cad.zju.edu.cn/home/zhx/DAM/2013/lib/exe/fetch.php?media=pdf:dam2013-15.pdf
- http://www.cse.unsw.edu.au/~cs9519/lecture_notes_08/L11_COMP9519.pdf

Bei der Videoklassifikation ist das Ziel einzelne Szenen aufgrund ihrer Inhalte in bestimmte Kategorien zu unterteilen. Zusammenfassend setzt sich die Videoklassifikation aus mehreren Schritten zusammen. Ausgehend von einem Video wird zunächst eine zeitliche Segmentierung vorgenommen, um anschließend aus einzelnen Sequenzen audiovisuelle Merkmale zu extrahieren. Anhand dieser Informationen und geeigneter Klassifikationsmodelle erfolgt eine Klassifikation von Shots, die abschließend zu Szenen gruppiert werden. Bei der zeitlichen Segmentierung wird ein Video wird in mehrere Shots unterteilt, wobei eine solche Sequenz aus mehrere aufeinander folgende Frames aus einer Kameraperspektive besteht. Es wird angenommen, dass innerhalb dieser Shots wenige inhaltliche Veränderungen
auftreten, wodurch die Beschreibung erleichtert wird. Abhängig von der semantischen Bedeutung bilden mehrere /benachbarte Shots/ eine Szene. Um die benötigten Shots zu bestimmen, gilt es die Grenzen zwischen den einzelnen Sequenzen zu detektieren, wobei verschiedene Arten von Übergängen auftreten können. Basierend auf den Eigenschaften der einzelnen Übergangseffekte existiert bereits eine Vielzahl
an Algorithmen zur Detektion von Shots, wobei entweder pixel-, regionen- oder bewegungsbasierte Informationen herangezogen werden. Ein weit verbreiteter Ansatz zur *zeitlichen Segmentierung* ist /Twin Comparison/.\\
Im Allgemeinen wird bei der Twin-Comparison-Methode zwischen der Detektion von abrupten Schnitten und fortlaufenden Übergängen (Ein-, Aus- und Überblendung) unterschieden. Während Schnitte aufgrund bedeutender inhaltlicher Veränderungen zwischen zwei Frames einfach zu detektieren sind, treten bei fortlaufenden Übergängen über mehrere Frames nur geringe Änderungen auf, wodurch eine Detektion erschwert wird. Die Detektion der verschiedenen Übergänge erfolgt durch die Analyse von benachbarten Frames und deren Differenzen. Für diese Auswertung können sowohl einzelne Pixel, beschränkte Regionen oder globale Histogramme herangezogen werden, wobei histogrammbasierte Methoden aufgrund des guten Kompromisses zwischen Berechnungsaufwand und Genauigkeit am häufigsten verwendet werden.\\
Ausgehend von einzelnen Frames wird zunächst für jedes Bild ein Histogramm gebildet, indem die Intensitätswerte der Farbkanäle einer Quantisierung unterzogen werden. Anschließend gilt es, die Differenz von benachbarten Frames zu berechnen. Je größer die Histogramm-Differenz von benachbarten Frames ausfällt, desto größer ist die inhaltliche Veränderung. Basierend auf den Eigenschaften von *abrupten Schnitten* und der Annahme, dass zwei verschiedene Shots unterschiedlichen Inhalt aufweisen, wird ein Schnitt detektiert, sobald die Differenz einen Schwellwert \(T_h\) übersteigt.\\
Fortlaufende Übergänge können auf ähnliche Weise bestimmt werden. Aufgrund der Überlagerung von zwei benachbarten Shots fällt der inhaltliche Unterschied während des Überganges geringer aus als im Falle eines direkten Schnitts. Jedoch ist die Histogramm-Differenz von Framepaaren innerhalb eines Shots am geringsten. Deshalb wird ein zweiter Schwellwert \(T_l\) eingeführt, mit dessen Hilfe Fades, Dissolves und Wipes detektiert werden können. Übersteigt die Differenz von benachbarten Frames den Schwellwert \(T_l\) , so handelt es sich dabei um den möglichen Beginn eines fortlaufenden Überganges. Anschließend werden die Differenzen von nachfolgenden Framepaaren betrachtet, bis \(T_l\) unterschritten wird und das Ende des Übergangseffekts eintritt. Die einzelnen Differenzwerte innerhalb dieses Bereichs werden aufsummiert. Überschreitet die Summe den Schwellwert zur Schnitterkennung \(T_h\), so wird angenommen, dass es sich dabei tatsächlich um einen fortlaufenden Übergang handelt.\\
Diese Methode zur Detektion von Shot-Grenzen wird aufgrund der beiden eingesetzten Schwellwerte als Twin Comparison bezeichnet und ist in der Lage sowohl Schnitte als auch spezielle Übergangseffekte zu detektieren (Einblendung, Ausblendung, Abrupter Schnitt, Dissolve, Wipe, Fade)
* TODO Es existiert ein Retrievalsystem für Insektenbilder. Sie sollen eine Evaluation durchführen. Beschreiben Sie ihre Vorgehensweise.
- Side note: Diese Aufgabe ist in *jeder* Altklausur und sie bringt immer die meisten Punkte
