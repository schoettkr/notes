* Einleitung
- Welche Techniken nutzte die Menschheit, um sich erstmals durch visuelle Medien auszudrücken?
  - Höhlenmalerei und später Malerei
- Wie elaboriert waren diese Techniken?
- Was ist von diesen Techniken übriggeblieben?
- Welche Formen von Perspektive gibt es?
  - Zentralperspektive
    - Frontalperspektive
    - Eckperspektive
  - Parallelperspektive
  - Farbperspektive
  - Luftperspektive
- Welche ist aus heutiger Sicht relevant?
- Wie ist das Verhältnis von Camera Obscura und Laterna Magica?
  - Camera Obscura (Lochkamera) = durch eine kleine Öffnung fällt ein Lichtstrahl in einen dunklen Raum, woraufhin auf der gegenüberliegenden Seite des Loches das, was sich außerhalb des Loches befindet klar zu sehen ist - seiten- verkehrt und auf dem Kopf
    - entwickelte sich vom wirklich begehbaren Raum mit Loch in der Außenwand zu kleinen kastenförmigen Apparaten mit Linsen in der Öffnung
  - Laterna Magica = ebenfalls ein optisches das zur Vorgeschichte der Fotografie gehoert
    - mit Hilfe einer Lichtquelle, zB Kerze, wurden transparente Streifenbilder auf eine Fläche projiziert und dort abgezeichnet
    - kann als Vorlaeufer des Dia-Projektors angesehen werden
- Was von ihnen findet sich bis heute wieder?
- Was ist Anamorphose und wie wird sie heute genutzt?
  - Bilder die nur unter einem bestimmten Blickwinkel bzw. mittels eines speziellen Spiegels oder Prismensystems zu erkennen sind
- Welche Schritte gab es auf dem Weg zur Fotografie?
  - 1826: Joseph Nicéphore Niépce (1765-1833) gelingt erste Aufnahme
  - 1839: Louis Jacques Mandé Daguerre (1787-1851) verkauft verbessertes Patent an französischen Staat; Basis: versilberte Kupferplatte
  - Henry Fox Talbot (1800-1877): Kalotypie auf Papier als Vorläufer Negativ-Positiv-Prozesse
  - Frederick Scott Archer entwickelt nasses Kollodiumverfahren für Negative aus Glas.
  - James Clerk Maxwell 1861: Farbfotografie
  - 1871 Richard Leach Maddox (1816-1902) entwickelt Gelatinebasiertes Verfahren: Durchbruch für breite Verwendung
- Welche Schritte gab es auf dem Weg zur modernen Analogfotografie?
  - erste „Digitalkamera“ von Steven J. Sasson 1975
  - 1981 entwickelte Sony mit der Mavica den Prototyp einer SVC, mit der man Bilder (noch analog) immerhin schon auf einer Diskette innerhalb der Kamera speichern konnte
    - es folgten danach kommerziell nutzbare Kamerasysteme u.a. von Canon und Nikon
  - Erst 1990 präsentierte Kodak das erste vollständig digitale Kamerasystem, bei dem die analoge Bildinformation vom CCD-Sensor (später auch CMOS-Sensor) sofort einem Analog-Digital-Wandler zugeführt, in digitaler Form gespeichert und nun anschließend mittels EBV weiter verarbeitet werden konnte (drehen, spiegeln, skalieren, verfremden etc.)
- Welche Schritte gab es auf dem Weg zur modernen Kinematographie?
  - Der Ausdruck Kinematographie ist in der Zeit des frühen Films entstanden. Er wurde abgeleitet von dem französischen Cinématographe, womit die Brüder Lumière ihren Kombinationsapparat bezeichneten, der die Funktionen von Kamera, Kopiergerät und Projektor in sich vereinigte. Die erste öffentliche kinematografische Vorführung, die als solche bezeichnet werden konnte, war vermutlich die Vorstellung der Brüder Latham am 20. Mai 1895.[3] In vorhergehenden Vorstellungen, etwa von Ottomar Anschütz (1887 und 1894), Émile Reynaud (erste Vorstellung 1892) oder Edison (ab 1894), wurden Techniken verwendet, die mit der späteren Filmprojektion noch wenig gemein hatten. Meist handelte es sich um Einzelsichtgeräte oder Apparate, die noch mit Fotoplatten arbeiteten.[3]
- Welche Tonsysteme gibt es zu Analogfilm und wie werden Sie gespeichert?
  - Das Lichttonverfahren ist das älteste und noch heute gebräuchliche Tonfilm-Verfahren, bei dem Bild- und Toninformation auf demselben Träger aufgebracht sind. Der Ton eines Kinofilms wird dabei auf einem maximal einen Zehntel Zoll (also maximal 2,54 mm) breiten, Tonspur genannten Streifen zwischen den Einzelbildern und den Perforationslöchern des Films fotografisch gespeichert. Da die Bilder schrittweise weiterbefördert werden während ein analoges Tonsignal vom konstant laufenden Filmstreifen abgetastet werden muss, werden hier Bild und Ton zeitlich versetzt auf dem Träger gespeichert, siehe Zeitversatz. Alternativ zum Lichttonverfahren wird das Magnettonverfahren eingesetzt. Gegenüber dem Magnettonverfahren hat das Lichttonverfahren mehrere Vorteile. Zum Einen wird die Tonspur bei der Filmherstellung mitkopiert, es sind also keine zusätzlichen Schritte erforderlich. Zum Anderen ist die Tonspur zeitlich stabiler und kann nicht versehentlich gelöscht werden. Nachteil ist (wie beim eigentlichen Filmbild auch) die Anfälligkeit für Kratzer, was zu Tonstörungen führen kann.
* Kapitel 1: Bild
Digitale Bilder bestehen aus zahlreichen, in der Regel mehreren Millionen Punkten, die auf einer viereckigen Fläche Farben markieren. Die Größe der Bilddatei hängt dabei zunächst einmal von der Anzahl der Punkte und der Anzahl der möglichen Farben ab. Je mehr Punkte und Farben Sie nutzen, desto mehr Information muss gespeichert werden. Salopp gesagt: Ein Bild, welches nur einen weißen Punkt darstellt, benötigt sehr viel weniger Speicherplatz als eine Karnevalsfotografie.

Daneben ist für die letztlich resultierende Dateigröße maßgeblich, welche Kompressionsverfahren genutzt werden. Standardisierte Dateiformate wie GIF, JPEG oder PNG nutzen unterschiedliche Kompressionsverfahren, welche unterschiedliche Auswirkungen auf die Dateigröße, aber auch auf die verbleibende optische Qualität haben.

Digitale Fotoapparate liefern neben dem JPEG-Format als Alternative das RAW-Format, welches ohne Kompression die Originaldaten der optischen Sensorik wiedergibt. Trotz der enormen Dateigrößen eignet sich dieses Format für bestimmte Zwecke der Weiterverarbeitung.

Dieses Kapitel vermittelt ein elementares technisches Verständnis für digitale Bilder und beschreibt beispielhaft grundlegende Manipulationsmöglichkeiten mit Adobe Photoshop. Photoshop gilt als Klassiker unter den Bildbearbeitungsprogrammen und ist inzwischen auch für den Heimbereich ein nützliches Werkzeug für sämtliche Fotoarbeiten geworden. Zu den Open-Source-Varianten zählt beispielsweise die Software Gimp.

** Pixel
Grundlage digitaler Bilder sind Pixel. Sie sind als Raster angeordnet, weshalb bei digitalen Fotografien auch von Rasterbildern die Rede ist. Die Pixel kodieren Farbwerte. Je mehr Pixel vorhanden sind, desto größer die Genauigkeit der Darstellung und die Dateigröße. Im Englischen spricht man von Picture Elements, was der Einfachheit halber auf das Kunstwort Pixel zusammengezogen wurde. Pixel sind das kleinste darstellbare Element eines digitalen Bildes, in der Regel handelt es sich dabei um kleine Vierecke, die Form kann aber auch als Rechteck definiert werden. Diese Pixel bilden ein zweidimensionales Raster (Höhe x Breite) von der Größe des gesamten Bildes. Sie tragen als Information den Farbwert, der an Ihrer Position erscheint. Die Anzahl der Pixel spiegelt die räumlichen Dimensionen wieder.\\
Dabei gilt: Je mehr Pixel verwendet werden, desto:
- detailreicher und natürlicher erscheint das Bild, und desto
- stärker kann das Bild vergrößert werden, zum Beispiel auf Plakate oder Projektionen, und
- desto größer wird auch die Datei
Üblicherweise werden bei digitalen Kameras »Megapixel« als Maßeinheit für die Auflösung verwendet. Gängige Kameras haben in der Regel folgende Rastergrößen:

| Auflösung | Pixelanzahl | Megapixel |
|-----------+-------------+-----------|
|   640x480 |     307.200 |       0.3 |
|  1024x768 |     786.432 |       0.8 |
|  1152x864 |     995.328 |         1 |
| 1600x1200 |   1.920.000 |         2 |
| 2816x2112 |   5.947.392 |         6 |
| 4048x3040 |  12.305.920 |        12 |
** Farbe
Jedes Pixel beinhaltet einen Farbwert. Je nach Farbtiefe sind unterschiedlich viele Farbwerte möglich. Je mehr Farbwerte möglich sind, desto naturgetreuer die Aufnahme und desto größer die Datei. Dabei sind unterschiedliche Farbräume möglich, die vom Verwendungszweck abhängig sind. Für den Druck findet der CMYK-Farbraum Anwendung, während die Darstellung eines Bildes auf dem Bildschirm die Addition der drei Farbkanäle Rot, Grün und Blau erfordert.
*** Farbtiefe
Neben der Anzahl der Pixel ist für die Qualität des digitalen Bildes die Anzahl der verwendbaren Farben maßgeblich. Dabei ist zunächst die Frage ausschlaggebend, wie viele unterschiedliche Farben vorkommen können. Die tatsächlich verwendete Anzahl der Farben spielt hier noch keine Rolle. Sie wird erst bei der Kompression relevant. Ausschlaggebend für den Kodierungsaufwand hingegen ist die Anzahl der möglichen Farben. Sie wird Farbtiefe genannt. Sind nur zwei Farben möglich, so kann die Farbangabe über ein einzelnes Bit laufen: Wird das Bit auf 0 gesetzt steht es für die eine Farbe, wird es auf 1 gesetzt steht es für die andere Farbe.
| Farbtiefe | darstellbare Farben | Aufteilung                                    |
|-----------+---------------------+-----------------------------------------------|
| 1 Bit     | 2^1 = 2             | monochrom                                     |
| 8 Bit     | 2^8 = 256           | GIF Dateien Rot 3 Bit, Grün 3 Bit, Blau 2 Bit |
| 15 Bit    | 2^15 = 32.678       | Real Color Rot 3 Bit, Grün 5 Bit, Blau 5 Bit  |
| 16 Bit    | 2^16 = 65.536       | High Color Rot 5 Bit, Grün 6 Bit, Blau 5 Bit  |
| 24 Bit    | 16.777.216          | True Color Rot 8 Bit, Grün 8 Bit, Blau 8 Bit  |
| 36 Bit    | 68.719.476.736      | Rot 12 Bit, Grün 12 Bit, Blau 12 Bit          |
Werden 8 Bit (= 1 Byte) für die Kodierung verwendet, so sind bereits 2^8 = 256 verschiedene Farben möglich. Für das menschliche Auge reicht diese Anzahl idR noch nicht aus. Es kann mehrere 100.000 Farbnuancen unterscheiden. Für Fotografien ist daher ein Bildraster von mind. 24 Bit empfehlenswert. Mit ihr stehen pro Farbkanal (Rot Grün Blau) 8 Bit zur Verfügung. Die Auswirkungen der Anzahl der Farben auf die Bildqualität zeigt folgendes Bild (24 Bit, 8 Bit, 1 Bit):

[[./farbanzahl.jpg]]

*** Farbmodelle
*Graustufen*: Das Bild wird auf Graustufen reduziert. So können Sie Farbaufnahmen nachträglich in schwarzweiß-Bilder konvertieren. Bei einer 8-Bit Kodierung stehen 256 Graustufen zur Verfügung.

*Indizierte Farben*: Das Bild wird in maximal 256 Farben konvertiert. Wenn das Ausgangsbild ein Farbbild ist, dann erscheint ein Popup-Fenster Indizierte Farben. Hier können Sie bestimmen, wie viele Farben verwendet werden dürfen. Photoshop generiert automatisch eine Farbtabelle zu dem Bild, die beim Speichern der Datei beigefügt wird.

*RGB Farbe*: Hierbei handelt es sich um das klassische Farbmodell für Bildschirmdarstellungen. Es eignet sich für lichtemitierende Medien wie Bildschirme oder auch Beamer. Die Farbwerte des Bildes werden in Rot-, Grün- und Blauanteile aufgespalten und einzeln kodiert. Die Mischung der Farben erfolgt additiv, d.h. wenn alle drei Farbkomponenten in voller Ausprägung vorhanden sind erscheint das Ergebnis weiß. Der Farbraum des RGB-Modells ist ein Würfel mit Einheitskantenlänge (von 0 bis 1) wie er folgend abgebildet ist. Die Einheitsvektoren sind die Farben Rot, Grün und Blau. Im Ursprung liegt Schwarz, die Grauwerte liegen auf der Hauptdiagonalen.

[[./farbmodell.gif]]

*CMYK-Farbe*: Die im CMY-Farbraum verwendeten Farben Cyan, Magenta und Gelb (Yellow) stehen komplementär zu den Komponenten des RGB-Farbraumes. Entsprechend verhalten sie sich auch komplementär: Mischt man alle drei Farben, so erhält man Schwarz. Das Farbmodell ist also subtraktiv. Man spricht auch von Körperfarben, da sich die Komponenten ähnlich der körperhaften Farben eines Malkastens mischen. Das Modell wird im Druck verwendet.
Auch das CMY-Farbmodell ist ein Würfel folgend gezeigt. In der Praxis hat sich das CMY-Farbmodell als unzureichend erwiesen. Die Mischung aller drei Komponenten führt nur theoretisch zu einem klaren Schwarz, praktisch aber zu einem sehr dunklen Braunton. Die Qualität wird dadurch erhöht, dass tatsächlich zusätzlich Schwarz beigemischt wird. Man spricht dann vom CMYK-Farbmodell, wobei K für Key-Color (i.e. Schwarz) steht.

[[./farbmodell2.gif]]

Ein zusätzlicher Vorteil entsteht dadurch, dass sämtliche Farben durch die Beimischung von Schwarz verdunkelt werden können, also insgesamt deutlich weniger an Farbe auf das Papier aufgetragen werden muss.
** Bildgröße
Anzahl der Pixel und Farbtiefe sind die ausschlaggebenden Variablen für die Menge der entstehenden Daten. Sie werden als Bildgröße bezeichnet.\\
Mit Bildgröße wird in der Digitaltechnik nicht die räumliche Größe eines Bildes bezeichnet, sondern die Menge der Daten, die zu seiner Beschreibung verwendet werden. Diese hängen zum einen von der Anzahl der Pixel und zum anderen von der Farbtiefe ab. Die Größe der Datei wird dabei wie folgt berechnet:
$G = M * N * Fb$ wobei G = Bildgröße, M & N = räumliche Ausdehnung und Fb = Farbtiefe in Byte

Ein Bild der Aufloesung von 1024x768 und einer RGB-Farbtiefe von 24 Bit benötigt beispielsweise 2.36MB Speicherplatz: $G = 1024 * 768 * (24/8) = 2.36$

Die Anzahl der Pixel sagt noch nichts über die räumliche Größe eines Bildes aus, denn die physische Ausdehnung eines Pixels ist nicht standardisiert. Sie muss festgelegt werden. Diese Festlegung erfolgt über den Begriff der Auflösung, die in ppi (pixel per inch) angegeben wird. Hier wird festgelegt, wieviele Pixel auf einem Inch, bzw. Zoll (ca. 2,54 cm) verteilt sind. Da bei Fotografien Pixel in der Regel eine quadratische Form haben, reicht hier eine einzelne Angabe. Es ist aber durchaus möglich in der Horizontalen eine andere Länge als in der Vertikalen anzugeben.\\
Für reine Bildschirmdarstellungen reichen bereits 72 ppi aus. Damit werden auf einem Quadratzoll des Monitors 722 Pixel dargestellt. Das entspricht der Auflösung handelsüblicher Monitore, eine höhere Auflösung macht hier also keinen Sinn. Anders sieht es im Druck aus. Hier werden 300 bis 600 Punkte als Minimum angesehen, um zu verhindern, dass das Bild gerastert erscheint.\\
Beispiel: Ein Bild mit der physischen Ausdehnung 10x15 cm (ca. 3,937 Zoll x 5,91 Zoll) und einer RGB-Farbtiefe von 24 Bit hat eine Auflösung von 300 ppi. Der Speicherbedarf berechnet sich wie folgt:
 - geg: M = 10cm = 3,937 Zoll; N = 15cm = 5,91 Zoll, Fb =  24 Bit, Auflösung A = 300ppi
- ges: G
\begin{equation}
\begin{split}
G = (M*A) * (N*A) * Fb\\
  = (3,937 * 300) * (5,91 * 300) * (24/8)\\
  = 1.181 * 1.773 * 3\\
  = 6.281.739 \text{Byte} = 6.1 \text{MB}
\end{split}
\end{equation}
** Dateiformate
Die Größe einer Bilddatei hängt zum einen von der physischen Bildgröße ab, zum anderen aber auch von den Methoden, die bei der Speicherung verwendet werden. Diese hängen wiederum davon ab, was und mit welchem Kompressionsverfahren gespeichert werden soll.\\
Welche Größe Bilddateien erreichen und welches Dateiformat verwendet werden sollte, hängt von verschiedenen Faktoren ab. Eine der wichtigsten Grundfragen ist beispielsweise, ob die in Photoshop verwendeten Ebenen tatsächlich auch als Ebenen abgespeichert werden sollen. Der Vorteil liegt darin, dass zu einem späteren Zeitpunkt oder sogar auch mit einem anderen Programm an dem Bild auf Basis der Ebenen weitergearbeitet werden kann. Reduziert man jedoch alle Ebenen auf nur eine Ebene oder wählt ein Dateiformat, dass die Speicherung verschiedener Ebenen nicht zulässt, dann können diese auch durch erneutes Öffnen der Datei nicht wieder hergestellt werden. Der Nachteil liegt ganz klar in einer deutlich größeren Datei.\\
Aber nicht nur die Ebenen können verworfen werden, mitunter werden auch gar nicht alle vorhandenen Farben benötigt. Eventuell besteht das Bild ja nur aus wenigen Dutzend Farben oder auch nur einigen Grauwerten - Wozu dann eine 24-Bit Farbtiefe verwenden? Auch hier kann Speicherbedarf gespart werden.
*** Kompressionsart
Schließlich verwenden unterschiedliche Dateiformate auch zum Teil sehr verschiedene Kompressionsverfahren. Solche Verfahren dienen generell dazu, große Datenmengen zu verkleinern. Es wird dabei zwischen verlustfreier und verlustbehafteter Kompression unterschieden. Verlustfreie Verfahren ermöglichen die spätere vollständige Rekonstruktion.\\
Ein Bild von der Größe 800 x 600 Pixel zeigt eine Landschaft mit strahlend blauem Himmel. Der RGB-Wert soll mit 24 Bit (=3 Byte) codiert werden. Der Bedarf an Speicherplatz für das gesamte Bild beträgt 1,44 MB $800*600*3=1.440.000 Byte=1.44Mb$. Die oberste Reihe besteht aus 800 gleichfarbigen blauen Pixeln mit einem RGB-Wert von (50, 50, 250). Der Speicherbedarf liegt also allein für diese Zeile bei 2.400 Byte ($800*3=2400$).\\
Anstatt nun 800mal den RGB-Wert abzuspeichern, könnte das Programm nun einfach einmal die Anzahl der Pixel (800), die mit einem bestimmten Farbwert (50, 50, 250) versehen sind, abspeichern. Das könnte beispielsweise so aussehen: »800/RGB/« Hierfür würden für die Anzahl der Pixel 3 Byte benötigt und für den RGB-Farbwert nochmals 3 Byte. Spezielle Separatoren (»/«) könnten dem Programm zeigen, wann Pixelanzahl und RGB-Wert wechseln. Die gleiche Information (also 800 Pixel in Blau) würde nun mit insgesamt 8 Byte auskommen.\\
Die Ersparnis in diesem Beispiel ist enorm. Dennoch kann das ursprüngliche Bild exakt wieder so dargestellt werden, wie es vor dem Speichern aussah. Der Kniff liegt einfach darin, dass eine zusammenfassendere Art der Codierung verwendet wurde (= *verlustfreie Kompression*). Leider sieht die Wirklichkeit ein wenig lebendiger aus: Fotografien vom Himmel bestehen nicht nur aus einem exakt definierten Blauton, sondern einer ganzen Reihe leicht unterschiedlicher Blautöne, oft sogar mehrere Tausend.\\
Anders operieren *verlustbehaftete Kompressionsverfahren* für Bilddaten: Hier geht verloren, was nicht wichtig ist. Und das sind bestimmte Farbinformationen, die einfach nicht mehr vollständig codiert werden. Im Idealfall sind diese Verluste für den Menschen nicht sichtbar, da die Auflösung und Farbwahrnehmung des menschlichen Auges bei der Kompression beachtet wurde. Im Realfall kommt es allerdings häufig zu sichtbaren Fehlern im Bild, sogenannten Artefakten.
*** Dateiformat TIFF
TIF/TIFF - Tagged Image File Format: ursprünglich für den Datenaustausch zwischen Computer, Scanner und Drucker konzipiert, hat sich das Tagged Image File Format seit den 1980er Jahren zu einem der beständigsten und wichtigsten Formate entwickelt, um qualitativ hochwertige Bilder abzuspeichern. Auch wenn es nicht offiziell von einem Gremium zum Standard erhoben wurde, so hat es sich doch als defacto Standard für den Austausch von Bildern entwickelt.\\
TIFF ist sehr universell angelegt. So können hier unterschiedliche Farbmodelle von 1-Bit bis 24-Bit eingesetzt werden. Die Daten können einerseits ohne jegliche Kompression abgespeichert werden, andererseits stehen aber je nach Bedarf auch eine ganze Reihe von Kompressionsverfahren zur Verfügung. TIFF bietet ferner die Möglichkeit, mehrere Bilder oder mehrere Versionen eines Bildes in einer Datei abzuspeichern. Wenn Sie aus Photoshop heraus TIFF-Dateien abspeichern, bleiben aus Wunsch die Ebenen erhalten.\\
Diese außergewöhnliche Breite an Möglichkeiten bringt einen vergleichsweise komplexen Aufbau der Dateien mit sich. Welche der angesprochenen Möglichkeiten in der einzelnen Datei tatsächlich genutzt wird, kennzeichnen sogenannte Tags, kleine beschreibende Elemente - daher der Name. Es gibt in TIFF ca. 80 verschiedene Tag-Typen, welche Farbtiefen, Kompressionsverfahren, Bildmaße etc. angeben.\\
TIFF eignet sich sehr gut um hochwertige Bilder abzuspeichern. Wenn Sie Dateien weiterverarbeiten wollen, eignet sich TIFF deutlich besser als GIF oder JPEG, da diese verlustbehaftet komprimieren. In der Regel sind die Dateien allerdings auch deutlich größer als beispielsweise JPEG-Dateien. Ferner können TIFF-Dateien nicht in Web-Browsern dargestellt werden, sondern benötigen eigene Programme.
*** Dateiformat JPEG
JPEG/JPG  - Joint Photographers Expert Group: Anfang der 1990er Jahre wurde durch die Joint Photographers Expert Group (JPEG) das gleichnamige Kompressionsverfahren entwickelt und 1992 als ISO 10918 standardisiert. Der Name JPEG steht dabei sowohl für die Gruppe, als auch das Verfahren. In der Regel wird er auch für das Dateiformat verwendet, obwohl JPEG nur ein Kompressionsverfahren, aber kein Dateiformat ist.\\
Ebenso wie TIF kann auch JPEG verschiedene Farbräume verarbeiten. In der Regel wird dies der 24-Bit RGB-Farbraum sein. Dabei werden die Kompressionsverfahren für jede Teilkomponente des Farbraums separat durchgeführt. Die Kompression nach JPEG durchläuft vier Schritte, die hier überblicksartig erläutert werden:
1. Schritt Bildaufbereitung
   - Das Bild wird in seine Komponenten zerlegt. Im RGB-Farbraum entsprechen diese Komponenten den Farbanteilen Rot, Grün und Blau. In einem Graustufenbild existiert nur eine Komponente. In JPEG wird hier auch von Ebenen gesprochen, die aber nicht mit den Ebenen aus Photoshop zu verwechseln sind. Die Komponenten bzw. Ebenen werden anschließend in 8 x 8 Pixel große Blöcke, sogenannte Macroblöcke aufgeteilt.
2. Schritt Bildverarbeitung
   - Nun werden die Makroblöcke sukzessive weiterverarbeitet. Auf alle 64 Pixel der 8 x 8 Pixel großen Makroblöcke wird die Diskrete Kosinustransformation (discrete cosinus transformation DCT) angewendet und so von einem Ortsbereich in einen Frequenzbereich umgewandelt. Flächige und farblich homogene Darstellungen haben eine geringe Frequenz und scharfe Kanten eine hohe Frequenz. Besonders in Fotografien gibt es generell mehr weiche Farbverläufe als harte Kanten. Sie werden in niedrige Frequenzen transformiert, die sich wiederum sehr effektiv codieren lassen.
3. Schritt Quantisierung
   - Um die Frequenzen noch effektiver abspeichern zu können, werden sie mit Hilfe von vorgegebenen Tabellen quantisiert. Quantisierung bedeutet, dass eine Frequenz nicht mehr als Welle, sondern als Treppe dargestellt wird. Je gröber die Stufung der Treppe, desto gröber das Ergebnis und desto kleiner die resultierende Datei. Hier treten auch die eigentlichen Verluste beim JPEG-Verfahren auf. Eine grobe Quantisierung führt zu einem nicht mehr vollständig rekonstruierbaren Code.
4. Schritt Entropiekodierung
   - Der letzte Schritt besteht in einer verlustfreien Kompression. Die speziellen Varianten in JPEG sind die Huffmankodierung oder wahlweise die Lauflängencodierung. Beide Varianten lassen eine vollständige Rekonstruktion der quantisierten Ausgangsdaten zu.

Die Aufteilung des Bildes in 8 x 8 Pixel große Blöcke sowie die nachfolgende Quantisierung hinterlassen bei JPEG eindeutige Spuren wie sie im folgenden Bild gezeigt werden. Es entstehen deutlich sichtbare quadratische Kästchen. Während fließende Farbverläufe in JPEG hervorragend codiert werden können, bringen vor allem scharfe Kanten das Verfahren in Schwierigkeiten.

[[./jpeg-quali.gif]]

In der Detailansicht werden die *Artefakte* sehr deutlich. Wollte man Fließtext so darstellen, würde die Lesbarkeit stark eingeschränkt. JPEG ist also immer dann gut geeignet, wenn fließende Farbverläufe, wie sie beispielsweise in Fotografien in der Regel vorherrschen, codiert werden sollen. Für harte Kanten, wie sie bei Schriften oder Diagrammen auftreten, ist JPEG nicht ausgerichtet.

*** Dateiformat GIF
Hier wiederum findet das Graphics Interchange Format GIF Verwendung. Dieses wurde Mitte der 1980er Jahre vom Online-Dienst Compuserve entwickelt und 1987 herausgegeben. Die Komprimierung durch GIF folgt zwei Methoden:
1. Schritt - Einschränkung der Farbvielfalt
   - GIF kann nur maximal 256 Farben (also 8 Bit) abspeichern. Welche Farben das sind, bestimmen aber Sie. So stehen zwar verschiedene Farbpaletten zur Verfügung, die beispielsweise eine optimale Darstellung in verschiedenen Betriebssystemen (System-Palette) oder in verschiedenen Browser (Browser-save-Palette) garantieren. Sie können aber auch Ihre eigene Palette anlegen. In der Regel übernimmt dabei die Auswahl der Farben Ihr Bildverarbeitungsprogramm. Sie müssen nur angeben, wieviele Farben verwendet werden dürfen. Da die Palette im Bild mitgespeichert wird, gilt: Je weniger Farben Sie im Bild zulassen, desto kleiner wird die Datei.
2. Schritt - Kompression nach LZW - Algorithmus
   - Dieser ursprünglich von Lempel, Zev und Welch definierte Algorithmus fasst zeilenweise gleiche Farben zusammen. Je weniger Farbunterschiede es in der Horizontalen gibt, desto kleiner die Datei. Folgendes Bild zeigt den Effekt: Beide Quadrate sind als 8-Bit GIF mit 256 Farben abgespeichert. Das Quadrat mit vertikalem Farbverlauf (links) benötigt 34 KB Speicher, das mit horizontalem Farbverlauf (rechts) nur 18KB.

[[./lzw.jpg]]

GIF kann seine Vorteile immer dann ausspielen, wenn wenige Farben mit harten Kanten vorkommen. Zwar sind die Stufen im Farbverlauf immer deutlich zu erkennen, aber die Kanten der Schrift hingegen bleiben scharf. GIF eignet sich also sehr gut bei Diagrammen und Graphiken, bei Fotografien allerdings nur in Ausnahmefällen.\\
GIF weist als Besonderheit die Möglichkeit von Transparenzen auf. Sie können eine frei wählbare Bildfarbe als transparent, also durchsichtig einstellen. Transparenzen sind vor allem für das Web-Design wichtig. Hier stehen Designer immer wieder vor dem Problem, dass digitale Bilder technisch bedingt immer viereckig sind. Oft genug möchte man aber einer Graphik eine runde oder komplexere Form geben, beispielsweise für Logos oder Wolken für eine Wetterkarte. Dazu setzen Sie einfach die Bereiche des Bildes zwischen Logo und Bildrand auf transparent.
*** Vergleich von TIFF, JPEG und GIF
Das nachstehende Bild zeigt einen Farbverlauf mit integriertem Text. Diese Kombination ist für Kompressionsverfahren eine besondere Herausforderung, da sehr viele Farben mit gleichmäßigem Übergang neben den harten Kanten der Schrift stehen.

Als TIFF (erste Reihe) ist sie mit 463 KB verhältnismäßig groß, zeigt aber eine sehr gute Qualität. Als JPEG (zweite Reihe) nimmt die Datei eine Größe von 17 KB an. Die Qualität der Farbverlaufs ist für das menschliche Auge völlig ausreichend. An den harten Kanten der Schrift entstehen aber unschöne Artefakte in der Detailansicht. Die GIF-Datei (dritte Reihe) wurde äquivalent zu der JPEG Version mit 17 KB abgespeichert. Deutlich sind Stufen im Farbverlauf zu erkennen. Die Kanten der Schrift hingegen bleiben scharf.

[[./vergleich.jpg]]
*** Dateiformat PNG
Das Portable Network Graphics Format wurde Mitte der 1990er Jahre als Ersatz für das damals noch unter Copyright stehende GIF konzipiert. PNG vereint die Vorteile von GIF und JPEG bei verlustfreier Kompression.\\
So sind ähnlich wie bei GIF auch in PNG Transparenzen möglich, allerdings können hier über einen separaten Alphakanal graduelle Unterschiede gemacht werden. Während GIF nur transparent und nichttransparent (engl.: opaque) zulässt, kann der Grad der Transparenz in PNG völlig frei bestimmt werden.\\
Auch in Bezug zur Farbtiefe nutzt PNG sowohl die aus GIF bekannte Möglichkeit einer Farbpalette als auch die in JPEG genutzte volle 24-Bit-Farbtiefe. So kann der Farbraum ideal an das Ausgangsbild angepasst werden.

*** Dateiformat RAW - roh
Digitale Fotoapparate ab dem semiprofessionellen Bereich lassen den Anwendern die Wahl, ob Bilder als JPEG oder im RAW-Format abgespeichert werden. RAW ist hierbei keine Abkürzung, sondern tatsächlich das englische Wort für „roh“ gemeint. Roh sind die Bilder in dem Sinne, als dass sie die Daten des bildgebenden Sensors weitgehend unbearbeitet lassen.

Werden die Bilddaten als JPEG ausgegeben, so haben sie schon eine ganze Reihe von Verarbeitungsschritten hinter sich. Wie im folgenden Kapitel noch gezeigt wird, sind die Bildpunkte, die ein Sensor aufnimmt bei weitem nicht identisch mit den später verwendeten Pixeln. Diese müssen zunächst einmal konstruiert werden. Aber selbst dann noch erscheinen RAW-Bilder für das menschliche Auge eher gewöhnungsbedürftig. Weißabgleich, Kontrast, Farbsättigung sowie Schärfung des Bildes werden vor der eigentlichen JPEG-Komprimierung automatisch optimiert.\\
Diese Schritte werden beim RAW-Format nicht durchgeführt. Hier werden die Daten des Sensors direkt ohne Optimierung gespeichert. Der Vorteil liegt darin, dass sämtliche Optimierungsschritte später manuell beeinflussbar am Computer durchgeführt werden können. Für den Alltagsgebrauch mag diese Möglichkeit der Nachbearbeitung eher unbedeutend sein, aber für professionelle Fotografen ist sie besonders wichtig. Sie können die Bilder manuell wesentlich besser optimieren als es die automatischen Kameraeinstellungen vermögen.\\
Der Vorteil der Nachbearbeitung bringt eine Reihe von Nachteilen mit sich. So liegen die Rohdaten des Sensors vor. Diese sind nicht über alle Kameras einheitlich. RAW-Daten sind also nicht gleich RAW-Daten, sondern immer speziell von der verwendeten Hardware abhängig. Um RAW-Bilder betrachten und verarbeiten zu können, benötigen Sie spezielle Konvertierungswerkzeuge. Nur professionelle Bildverarbeitungssoftware wie Adobe Photoshop beherrschen zumindest die gängigsten Formate. Schließlich sind die entstehenden Dateien um ein vielfaches größer als die Alternative JPEG.
** Animated GIFs
nimated GIFs sind kleine Bilddateien, in denen wie bei einem Daumenkino mehrere Bilder nacheinander gezeigt werden und so kleine Animationen entstehen können. Sie eignen sich gut für kleine Demonstrationen, in denen Graphiken zu einfach sind und Videos oder Flash-Animationen zu aufwändig.\\
Der Begriff Animation wurde im Internet lange Zeit sehr zwiespältig aufgenommen: Einerseits ermöglichen gelungene Animationen sehr gut, Abläufe und Funktionsweisen zu erklären. Textliche Beschreibungen oder auch statische Bilder sind hier oft genug im Nachteil. Andererseits sprudeln den Anwendern immer wieder schlecht gemachte Mini-Animationen entgegen, die völlig sinnlos eingesetzt irritieren. Synonym dafür sind die Begriffe „Animated GIF“ und „Flash-Intro“.\\
Animated GIFs sind im Wesentlichen - wie ein Daumenkino - aus mehreren Einzelbildern zusammengesetzte GIF-Dateien. Hier werden in der Regel nicht mehr als ein Dutzend Bilder nacheinander abgespielt und so kleine Animationen erzeugt. Interaktiv sind diese zwar nicht, aber es können Bewegungsprinzipien, die nur kurze Sequenzen lang sind, ohne großen Aufwand dargestellt werden. Beispiele sind Flügelschlag von Vögeln und Bienen oder die Drehbewegungen von Dampfmaschinen und anderen Motoren. Doch Vorsicht: Animierte GIFs wollen wohl überlegt eingesetzt werden! Überflüssige oder schlecht gemachte Animationen werten Ihre Website eher ab als auf. Schnell werden Anwender durch die entstehende Unruhe abgeschreckt. Überlegen Sie deshalb gut, ob ein Standbild nicht ausreicht.
* Kapitel 2: Licht
Fast nichts ist uns Menschen so selbstverständlich, wie die Fähigkeit, Dinge und andere Lebewesen in der Welt sehen zu können. Unsere Welt ist voller Lichtstrahlen, die - wenn wir sehen - in den Linsen unserer Augen gesammelt und von dort auf die lichtempfindliche Netzhaut geworfen werden. Von dort werden die Informationen in das Gehirn eingespeist.

Die meisten von uns können sich nur schwer vorstellen, wie es wäre, nicht zu sehen. Durch unsere beiden Augen wird es uns möglich, räumlich zu sehen. Die Seherlebnisse selbst nennt man auch "visuelle Wahrnehmung". Dank der Informationen, die vom Sehnerv zum Gehirn gelangen, kann im Geist ein Modell davon entstehen, wie die Welt da "draußen" eigentlich beschaffen ist. Das Auge liefert dem Gehirn das "Rohmaterial", welches anschließend weiter verarbeitet wird.

Voraussetzung für das Sehen ist das Licht. Es trifft das Auge entweder direkt von der Lichtquelle her oder indirekt, indem es von Körpern abprallt. In der Welt gibt es ganz unterschiedliche Lichtquellen, jedoch erzeugt jede von ihnen Strahlen, die sich in alle Richtungen ausbreiten. Lichtwellen gehören zusammen mit den "Radiowellen" zur Familie der "elektromagnetischen Wellen". Die Lichtgeschwindigkeit beträgt 300.000 Kilometer in der Sekunde.

Abhängig von der Frequenzhöhe einer Lichtwelle werden unterschiedliche Farben wahrgenommen - die Frequenz ist die Maßeinheit für die Häufigkeit, in der eine Welle innerhalb einer Zeiteinheit auf- und abschwingt.

Eine ebenso wichtige Maßeinheit ist die Wellenlänge, die die Entfernung zwischen dem höchsten und dem niedrigsten Punkt der Wellenbewegung angibt. Je höher die Frequenz beim Licht, desto niedriger die Wellenlänge (umgekehrt proportional).

Der für den Menschen sichtbare Bereich der elektromagnetischen Strahlung ist relativ eng begrenzt. Ultraviolett (hohe Frequenz und niedrige Wellenlänge) und Infrarot (niedrige Frequenz und hohe Wellenlänge) werden zwar von manchen Tieren, aber nicht vom Menschen wahrgenommen - dazwischen liegen die für uns sichtbaren Farbtöne Violett, Blau, Grün, Gelb, Orange und Rot.

Wenn Lichtstrahlen auf andere Gegenstände treffen, werden sie normalerweise zurückgeworfen - Licht fällt auf Gegenstände und macht sie sichtbar. Für unser Leben auf der Erde ist es natürlich vor allem die Sonne, die alles in Licht taucht. Im Universum gibt es unendlich viele Sonnen - weil sie so weit weg sind, sehen wir sie allerdings nur als Lichtpunkte. Aber auch ein Feuer in der Nacht oder mit Strom gespeiste Laternen sind Lichtquellen, die uns helfen, zu sehen.

Wenn Lichtstrahlen ins Auge eintreten, sehen wir entweder die Lichtquelle selbst, oder aber den Gegenstand, von dem sie abgelenkt werden. Das eintretende Licht fällt dabei mit unterschiedlichem Einfallswinkel in das Auge - bei späteren Verarbeitungen im Gehirn kann "berechnet" werden, wie groß Gegenstände sind, welche Farbe sie haben, wie stark sie leuchten und in welcher Ferne sie sich befinden.

Licht ist auch beim Drehen einer Szene eines der wichtigsten Elemente. Die Hauptaufgabe ist dabei, die Kulisse so auszuleuchten, dass alle Teile des Bildes von der Kamera erfasst werden können. Es können auch gezielt Lichter in die Szene gesetzt werden, um den Szeneneindruck zu verstärken. Abhängig von verschiedenen Faktoren, wie der Einfallsrichtung, der Qualität oder der Farbtemperatur des Lichts, lassen sich unterschiedliche Stimmungen vermitteln.
** Aufbau Auge
Das Sehen findet nicht im Auge, sondern im Gehirn statt. Das eigentliche Sehorgan ist das Auge. Zu ihm zählen der Augapfel, der Sehnerv, der Tränenapparat, die Augenmuskulatur und die Augenlider. Zwei Augen sind besser als eines - durch das gleichzeitige Sehen von zwei unterschiedlichen Punkten aus wird es nämlich erst möglich, Objekte in der näheren Umgebung von solchen in der Ferne zu unterscheiden ("Räumliches Sehen").

Die meisten Elemente des menschlichen Auges sind dafür zuständig, ein Bild aus der Umgebung - wie eine Kamera - scharf auf eine lichtempfindliche Fläche abzubilden. Dabei wird das ins Auge einfallende Licht durch lichtbrechende Organe (Hornhaut, Linse, Glaskörper und Kammerwasser) gebündelt und scharf gestellt. Die Iris formt in der vorderen Augenkammer das in seiner Größe variable Sehloch (Pupille). Jedes dieser Elemente trägt dazu bei, dass ein scharfes, nicht zu helles Bild auf die Netzhaut (Retina) abgebildet wird. Dort befinden sich die hell- und dunkelempfindlichen Stäbchen und die farbempfindlichen Zapfen, die beide das Licht aufnehmen. Die Zapfen liegen besonders dicht in der Sehgrube (Fovea), der Zone der größten Sehschärfe in der Mitte des gelben Flecks. Weitere Bestandteile des Auges, wie das komplexe Nervensystem, das aufgenommene Bilder an das Gehirn weiterleitet, sollen an dieser Stelle nicht weiter interessieren.

[[./aufbau-auge.jpg]]

*** Dunkeladaptiertes Sehen
Die Zapfen und die Stäbchen erfüllen unterschiedliche Zwecke. Rund 125 Millionen Stäbchen sind über die Retina verteilt, die geringste Konzentration befindet sich in der Sehgrube (Abb. 2). Sie sind etwa 10.000 Mal lichtempfindlicher als die Zapfen und sind für das dunkeladaptierte Sehen (Nachtsicht) zuständig. Dabei können die Stäbchen keine Farben unterscheiden, daher auch die Redewendung „Nachts sind alle Katzen grau”.

[[./verteilung.jpg]]
*** Helladaptiertes Sehen
Beim helladaptierten Sehen sind die Zapfen zuständig, die in der Sehgrube ihre höchste Dichte erreichen. Liegt die Leuchtdichte größenordnungsmäßig unter 1 cd/m², können nur noch die Stäbchen zum Sehen beitragen. Zapfen reagieren empfindlich auf langwelligeres Licht (Rot), mittlere Wellenlängen (Grün) und kurzwelligeres Licht (Blau) (Abb. 3). Diese Tatsache erklärt, wieso die Farbwahrnehmung im Wesentlichen als Kombinationen der drei Farbstimuli Rot, Grün und Blau beschrieben werden kann (Additive Farbmischung). Es ist möglich, zu jeder wahrgenommenen Farbe eine Kombination von Rot-, Grün- und Blau-Anteilen in jeweils bestimmten Intensitäten angeben zu können. So entsteht z. B. durch die Mischung von rotem und grünem Licht der Farbeindruck Gelb.

Zapfen reagieren am lichtempfindlichsten bei einer Wellenlänge von 555 Nanometern (Grün/Gelb). Stäbchen hingegen erreichen ihre höchste Empfindlichkeit bei etwa 507 Nanometern (ein leicht bläuliches Grün). Aus diesem Grund reagiert das menschliche Auge auf Grün am empfindlichsten.

[[./spektrum.jpg]]
** Art und Qualität
Licht kann natürlich auftreten oder künstlich erzeugt werden. Lichter unterscheiden sich nicht nur nach ihrer Art, sondern auch qualitativ: Beleuchtungsdauer, Farbe bzw. Farbtemperatur, Richtung des Lichtes, Lichtqualität und räumlicher Lichtumfang spielen eine wichtige Rolle. Im Folgenden werden nur vier grundlegende Eigenschaften erläutert.

*** Arten des Lichtes
*Natürliches Licht* = Jenes Licht, das in unserer Umgebung ohne Zuhilfenahme von Strom entsteht, wird natürliches Licht oder auch Tageslicht genannt. Dazu zählen unter anderem das Sonnen- oder Mondlicht und auch das Licht der Sterne. Die Qualität des natürliches Lichtes lässt sich nur schwer beeinflussen, da es von verschiedenen Faktoren wie beispielsweise der Tageszeit oder dem Wetter abhängig ist, die wir nur bedingt steuern können. Auch Schmutzpartikel in der Atmosphäre tragen zu einer kaum kontrollierbaren Streuung der Lichtstrahlen bei. Das natürliche Licht nimmt je nach Tageszeit und Witterung verschiedene Farben an. Jedoch bemerken wir diese starken Unterschiede mit unseren Augen kaum, da das menschliche Gehirn die Farbwahrnehmung manipuliert.

*Kunstlicht* = Kunstlicht entsteht nicht natürlich, sondern wird durch elektrische Spannung künstlich erzeugt und nachgeahmt. Glühlampen, Leuchtstoffröhren oder Energiesparlampen ermöglichen uns eine komfortable Sicht und geben uns Sicherheit in der Dunkelheit der Nacht. Künstliche Leuchtmittel finden beispielsweise in einem TV-Studio zur individuellen Lichtgestaltung oder auch in der Natur zur Verbesserung der jeweiligen Lichtsituation bei einem Videodreh Verwendung. Kunstlicht hat unterschiedliche Farben, die sich jedoch i.d.R. kaum verschieben (Ausnahme: dimmbare Leuchtmittel und Energiesparlampen).

*** Lichtrichtung
Die Stimmung eines Bildes kann sich durch die Richtung, aus der das Licht strahlt, verändern. Sie spielt auch eine entscheidende Rolle, wenn eine natürliche Situation künstlich nachgeahmt werden soll. Die Sonne kann beispielsweise je nach Tageszeit unterschiedliche Höhen einnehmen, wodurch ihr Licht aus unterschiedlichen Winkeln einfällt. Ebenso verschieden kann auch die Höhe eines Kunstlichtes im Studio eingestellt werden.

Die Lichtstrahlen können frontal in Blickrichtung der Kamera Vorderlicht, Gegenlicht gegen die Blickrichtung der Kamera, als Seitenlicht von rechts- oder links- Vorne, als Streiflicht von links oder rechts oder als Licht von oben oder unten einfallen (siehe Bild):\\
[[./lichtrichtung.gif]]\\
Aus welcher Richtung das Licht eintrifft, können Sie am einfachsten an den Schatten in einem Bild „ablesen”. Bildet sich beispielsweise ein Schatten nur als kleiner Fleck unter Gegenständen ab, dann weist das auf weiches Licht von oberhalb des Gegenstands hin.

*** Lichtqualität
Mit der Qualität des Lichts ist hartes oder weiches Licht gemeint. Die Lichtcharakteristik einer Lichtquelle ist abhängig von ihrer tatsächlichen Größe, nicht von ihrer Helligkeit. Im Allgemeinen gilt: Je weiter das Licht streut, umso weicher ist das Licht. Das Auge kann allerdings nicht mehr als drei bis vier Qualitätsgrade unterscheiden. Am einfachsten können Sie die Lichtqualität einschätzen, indem Sie überprüfen, ob die Schatten hart oder weich erscheinen.

*Hartes Licht* = Punktförmige und flächige (aber weit entfernte) Lichtquellen produzieren hartes Licht und strahlen nur von einem einzigen Punkt aus (z. B. Sonne). Hartes Licht zeichnet sich durch hohe Kontraste und klar erkennbare Schatten aus (klar kontuierte Schatten - Bild unten). Schon kleinste Hautmakel einer Schauspielerin können Schatten werfen, wodurch sich hartes Licht nicht für Beautyaufnahmen eignet, sondern eher Anwendung findet, wenn ein maskulines Aussehen verstärkt oder Sonnenlicht nachgeahmt werden soll. Zudem können Sie nur eine Lichtquelle verwenden, denn durch die Verwendung weiterer harter Lichter erhalten Sie automatisch auch weitere harte Schatten. Hartes Licht wird beispielsweise von Fresnel- oder Open-Face-Scheinwerfern ausgestrahlt.\\
[[./hartes-licht-schatten.jpg]]\\

*Weiches Licht* = Weiches Licht wird von einer größeren Oberfläche abgestrahlt, wodurch der entstehende Schatten weniger stark definiert ist (Bild unten - Der bei weichem Licht entstehende Schatten bildet sich weniger stark definiert ab.). Weiches Licht kann durch Lichtformer (Reflektoren, Softboxen) bzw. Frostfilter erzeugt werden, die das harte Licht zerstreuen. Ein bedeckter Himmel dient beispielsweise als riesige Softbox für die punktförmige Sonne. Das gestreute Licht können Sie mit Hilfe eines Richtgitters (z. B. montiert an einer Softbox) besser kontrollieren. Da das Licht nicht nur aus einer Richtung eintrifft, entstehen bei Unebenheiten weniger Schatten, wodurch weiches Licht gern bei Beautyaufnahmen Verwendung findet, denn dadurch erscheint die Haut ebener und weicher.\\
[[./weiches-licht-schatten.jpg]]\\

*** Farbtemperatur
Unser Auge ist in der Lage, die verschiedenen Lichtarten nahezu perfekt an unsere Wahrnehmung anzupassen. Das kann die Kamera nicht und es kann zu Farbstichen oder –verschiebungen kommen, die bei Tageslicht je nach Tageszeit zu- oder abnehmen (siehe Bild).\\
[[./sonnenstand.jpg]]\\
Filmmaterial und Videokameras sind i.d.R. auf Tages- oder Kunstlicht sensibilisiert, woraus sich  unterschiedliche Farbtöne ergeben. Kunstlicht (engl.: tungsten light) tendiert ins Gelbliche bei ca. 3.200 Grad Kelvin. Tageslicht (engl.: daylight) tendiert ins Bläuliche bei ca. 5.500 Grad Kelvin (siehe Bild -  Kunstlicht wirkt gelblich bei ca. 3.200 Grad Kelvin. Tageslicht wirkt bläulich bei ca. 5.500 Grad Kelvin).\\
#+attr_html: :width 400px
[[./farbtemp.jpg]]\\
Ist die Kamera auf Kunstlicht abgeglichen und die Aufnahme wird im Kunstlicht gemacht, ist alles normal und Weiß bleibt Weiß (1). Filmen Sie nun aber ein Fenster, erscheint es im Vergleich bläulich (2). Umgekehrt verhält es sich ähnlich (siehe Bild unten). Ist die Kamera auf Tageslicht abgeglichen und Sie filmen im Tageslicht, ist alles normal und Weiß bleibt Weiß (3). Befindet sich nun Kunstlicht in der Szene, wird es Gelb oder Orange erscheinen. Eine auf Kunstlicht abgeglichene Kamera stellt Tageslicht im Vergleich bläulich dar (4).\\
#+attr_html: :width 400px
[[./tageskunst.jpg]]\\
Die Wahl der Farbtemperatur hängt von der Stimmung ab, die Sie vermitteln möchten. Grundsätzlich gilt aber, dass Sie für die gesamte Szene bei dieser Entscheidung bleiben sollten, da im Regelfall eine Mischung verschiedener Farbtemperaturen nicht zu empfehlen ist (es sei denn, Sie möchten damit besondere Effekte erzeugen oder verwenden zusätzlich Filterfolien um die Farbtemperaturen anzugleichen). Beachten Sie bei der Verwendung verschiedener Lichtquellen (z. B. Leuchtstoffröhren und Glühbirnen), dass sich diese in der Farbtemperatur unterscheiden können. Der Bildeindruck kann sich besonders bei der Aufhellung von Tageslichtlicht-Szenen mit Kunstlicht stark verändern. Unterschiedliche Lichtfarben können durch ihren Farbkontrast aber auch als Gestaltungsmittel dienen. Beispielsweise erscheint eine winterliche Hütte mit warmem Licht im Innenraum und die durch die Fenster bläulich kalt aussehende Außenwelt dem menschlichen Auge natürlicher als ein mit kaltem Licht übertrieben aufgehellter Innenraum. Als Faustregel gilt: Drehen sie Ihre Szene mit dem Weißabgleich des vorherrschenden Lichtes.\\
Lesen Sie bitte ergänzend folgende Seiten mit Unterkapiteln:
http://www.heise.de/foto/artikel/Temperaturabhaengig-Weissabgleich-und-Farbwiedergabe-226910.html
** Leuchtmittel
Die an einem Set sonst übliche Lichttechnik umfasst weitaus mehr als die folgenden Leuchten und das Zubehör, doch an dieser Stelle soll nur einige Leuchtmittel wie Leuchtstofflampen bzw. Flächenleuchten, Stufenlinsenleuchten oder Cyc Lights erläutert werden.
*** Leuchtstofflampen
Bei vielen Produktionen wird inzwischen mit Leuchtstofflampen (engl.: Fluorescents), wie z. B. von Kino Flo, gearbeitet (1). Früher waren weiche Lichtquellen, wie z. B. Lichtwannen, recht groß und ließen sich nicht so leicht im Set verstecken. In den achtziger Jahren wurden für Architektur- und Grafikbüros Fluoreszenzlampen entwickelt, die einen tageslichtähnlichen Charakter hatten und ein kontinuierliches Licht abgaben und dabei auch noch platzsparend angebracht werden konnten. So haben heutzutage praktisch alle Hersteller von Licht-Equipment, wie z. B. Sachtler, weiterentwickelte Leuchtstoff-Flächenleuchten im Programm (2). Die Leuchtstofflampen sind in Tageslicht- und Kunstlichtfarbtemperatur erhältlich, geben Farben i.d.R. gut wieder und erzeugen kein Brummen. Sie geben ein weiches und gleichmäßiges Licht ohne Hotspot ab, wodurch ein sehr weicher Kontaktschatten zwischen Objekt und Fußboden entsteht. Zudem werden Leuchtstofflampen im Betrieb kaum heiß, verbrauchen nur wenig Strom und sind langlebig.

Leuchtstofflampen aus dem Baumarkt sind üblicherweise die, die Sie aus Büros oder Arztpraxen kennen (3). Die einfachen Leuchtstofflampen geben im Regelfall kein kontinuierliches Lichtspektrum ab, sondern weisen deutliche Spitzen im Gelb- und Blaubereich auf, wodurch auf Foto- und Filmaufnahmen ein Grünstich entsteht. Zudem kann durch eine Leuchtfrequenz von 50 Hertz Flackern hervorgerufen werden, das aber durch Veränderung des Shutters an der Kamera korrigiert werden kann. Leuchtstofflampen lassen sich jedoch nur begrenzt dimmen.\\
#+attr_html: :width 400px
[[./leuchtstofflampen.jpg]]
*** Cyc Lights
Cyc Lights oder auch Zykloramen sind speziell für die Beleuchtung von Hintergründen und Rundhorizonten gefertigte Flutlichter. Sie haben einen asymmetrischen Reflektor, der ein weiches und breitgestreutes Licht abgibt (Bild unten - Der asymmetrische Reflektor des Cyc Lights ermöglicht eine tiefere Ausleuchtung.). Mit Hilfe eines Cyc Light kann z. B. mehr Licht auf den Boden eines Green Screens geworfen werden. Cyc Lights haben normalerweise die Farbtemperatur von Kunstlicht, können aber mit Folien in Tageslicht umgewandelt werden. Sie werden zudem sehr heiß. Derartige Lichttechnik gibt es in verschiedenen Ausführungen, beispielsweise bei Altman Lighting.\\
#+attr_html: :width 400px
[[./cyc.jpg]]
*** Flächenleuchten
Eine Flächenleuchte dient zum gleichmäßigen und weichen Ausleuchten von Objekten. Ihre Form und Bauweise ermöglicht die Abgabe des Lichtes über eine relativ große Leuchtfläche. Eine Tageslicht-Flächenleuchte (Bild unten rechts) besitzt i.d.R. eine flache Lichtwanne mit silber reflektierender Innenbeschichtung. Als Leuchtmittel dienen meist längliche Tageslichtlampen mit 5.400°K, welche für einen flimmerfreien Einsatz mit elektronischen Vorschaltgeräten betrieben werden.

Außerdem gibt es seit geraumer Zeit innovative LED-Flächenleuchten, sog. LED Light Panels (Bild unten links), die sich moderne LED-Technologie zunutze machen. Derartige Flächenleuchten bieten Fotografen einige Vorteile gegenüber Studioleuchten, die auf herkömmlichen Fotostudio-Tageslichtlampen basieren. LED-Leuchten weisen einen geringeren Stromverbrauch auf. Das flimmerfreie Licht ist für Foto und Video geeignet und lässt sich sehr fein dosieren. Daneben können viele LED-Flächenleuchten mit standardisierten Akkus betrieben werden, wodurch sich diese Leuchten hervorragend für den mobilen Einsatz eignen.\\
#+attr_html: :width 400px
[[./flaechenleuchte.jpg]]
*** Stufenlinsenleuchten
Stufenlinsenleuchten, kurz Stufenlinsen oder auch Fresnel-Linsenscheinwerfer genannt,  sind universal einsetzbare Leuchten mit einem weichen Lichtrand. Eine der Besonderheiten ist die eingesetzte Fresnel-Linse oder genauer Fresnelsche Stufenlinse (Abb. 4). Erfunden um 1822 von dem französischen Physiker Augustin Jean Fresnel, verringert dieses Bauprinzip entscheidend das Gewicht und Volumen großer Linsen.  Dies geschieht durch eine Aufteilung in ringförmige Bereiche. In jedem dieser Bereiche wird die Dicke verringert, sodass die Linse eine Reihe ringförmiger Stufen erhält. Da das Licht nur beim Passieren der Linsen-Oberflächen gebrochen wird, ist der Brechungswinkel nicht von der Dicke, sondern nur vom Winkel zwischen beiden Oberflächen abhängig. Die Linse behält ihre Brennweite bei.

Das Leuchtmittel liegt im Brennpunkt eines kreisförmigen Reflektors. Da der Reflektor auf einer Schiene montiert ist, ist der Abstand zur Linse verstellbar.  Hierdurch wird es möglich, den Lichtaustritt von einem schmalen Lichtstrahl (engl.: Spot) hin zu einem breiten Lichtstrahl (engl.: flood) stufenlos zu regulieren. Dieser Abstrahlwinkel einer Stufenlinse ist je nach Modell über einen Bereich von ca. 8°-60° einstellbar. Das Licht nimmt dabei nach außen hin langsam ab, wodurch ein weicher Übergang entsteht (weiche Kante).

Da Stufenlinsen, im Gegensatz zu PC-Linse keinen störenden Hot-Spot haben, eignen sie sich besonders gut zur gerichteten, gleichmäßigen, flächigen aber dennoch akzentuierten Ausleuchtung.

#+attr_html: :width 400px
[[./fresne.jpg]]
** Lichtformer
Mit Lichtformern wie Reflektoren, Richtgitter oder Softboxen lassen sich Lichtstrahlen beeinflussen.
*** Softbox
Eine Softbox (engl.: Soft Bank), oder auch Lichtwanne genannt, ist ein Lichtformer, der helles und kontrastreiches Licht (z. B. Studioblitz) in weiches, umschmeichelndes Licht umwandelt. Softboxen verringern harte Übergänge zwischen hellen und dunklen Bereichen und weichen scharfe Schatten auf. Sie sind faltbar und daher einfach zu transportieren. Softboxen bestehen i.d.R. aus einer nicht lichtdurchlässigen Haut, die außen meist schwarz und innen mit einer speziellen silbernen Reflexionsschicht versehen ist. Sie kann durch Stangen, die im Inneren des Lichtformers verlaufen, aufgespannt und durch einen Front- und/oder Zwischendiffusor ergänzt werden (Abb. 1). Softboxen sind in verschiedenen Formen und Größen sowie weiterem Zubehör wie Masken erhältlich: Je größer, desto mehr weicheres Licht wird erzeugt.\\
#+attr_html: :width 400px
[[./softbox.jpg]]
*** Richtgitter
Richtgitter, oder auch Louver, Lichtraster, Grid oder Wabengitter genannt, werden an Lichtquellen montiert. Sie bündeln und richten das Licht aus, sodass alle Lichtstrahlen nahezu parallel fallen und Sie durch die Verminderung der Streuung das Licht besser kontrollieren können (Abb. 2). Nachteilig an der Verwendung von Richtgittern ist allerdings, dass sie viel Licht verschlingen. Manche Leuchten sind bereits mit Lichtrastern ausgestattet, andernfalls können Sie diese auch käuflich erwerben, z. B. bei Kino Flo oder Arri. Die Lichtraster sollten schwarz sein und nicht reflektierend.\\
#+attr_html: :width 400px
[[./richtgitter.jpg]]
*** Reflektoren
Als Reflektor (engl.: Reflector), oder auch Reflektorschirm oder Flächenreflektor genannt, wird ein Aufheller (engl.: Bouncer) sowie ein Lichtformer für Studioblitzgeräte bezeichnet. Ein Reflektor lenkt das Licht in eine bestimmte Richtung. Auf Grund verschiedener Konstruktionen lässt sich die Weichheit des Lichts und der Lichtaustrittswinkel variieren. Reflektoren können unterschiedlich groß sein, haben aber meist einen Durchmesser von zehn bis zu 60 Zentimetern. Die Seiten sind je nach Typ weiß, silbern oder gold und beeinflussen die Lichtfarbe: Die silberne Seite erzeugt ein kaltes, die goldene ein warmes und die weiße ein neutrales, weiches Licht (Abb. 3). Die transparente Seite dient zur Streuung von hartem Licht und die schwarze Seite ist für dessen Absorption zuständig.

Der Abstrahlwinkel liegt zwischen fünf und 120 Grad. Längere Bauformen bewirken ein stärker gerichtetes Licht. Eine weiche und gleichmäßige Ausleuchtung des Schauspielers entsteht auch durch die Reflektion von hartem Licht an einer weißen Zimmerdecke oder einem Tuch. Beliebt sind auch große weiße Styroporplatten aus dem Baumarkt.
#+attr_html: :width 400px
[[./reflektoren.jpg]]

*** Filter
Filter bestehen i.d.R. aus Glas oder Folie und werden vor die jeweilige Leuchte gespannt, um deren Farbtemperatur zu verändern. Schauen Sie sich hierzu bitte folgende Video an:
http://www.video2brain.com/de/tutorial/lichtfolien

** Beleuchtungsmethode
Um dreidimensionale Formen optimal in einem zweidimensionalen Bild (z. B. Foto oder Bildschirm) abbilden zu können, wird eine einfache Beleuchtungsvariante - die Dreipunktbeleuchtung - verwendet. Um eine optimale Ausleuchtung zu garantieren, sind drei Lichtquellen von Nöten. Mit dieser Anordnung kann außerdem eine Low-Key- sowie auch eine High-Key-Beleuchtung erzielt werden.

*** Dreipunktbeleuchtung
Als grundlegende Ausleuchtungsvariante findet die klassische Dreipunktbeleuchtung bei vielen Gelegenheiten Anwendung. Auch Aktionslicht genannt, besteht es aus einem Führungslicht, einem schwächeren Füll-Licht und einem Gegenlicht (Abb. 1). Nicht selten werden zusätzliche Lichtquellen für die Beleuchtung des Hintergrunds aufgestellt. Zusätzlich können für die Nachahmung einer Lichtstimmung weitere Lichter aus verschiedenen Richtungen aufgestellt werden.\\
#+attr_html: :width 400px
[[./3punkt.jpg]]
**** Führungslicht
Das Führungslicht, oder auch Hauptlicht (engl.: Key-Light, Main Light) genannt, ist i.d.R. die lichtstärkste Lichtquelle in der Ausleuchtungsanordnung bzw. die Lichtquelle, die innerhalb der Szene die höchste Beleuchtungsstärke erzeugt. Beim Einrichten jeder Ausleuchtung ist das Festlegen der Führungslichtquelle der erste Schritt.

Dieses dominante Licht wirft starke Schatten. Ein Führungslicht von oberhalb der Szene verursacht tiefe Schatten in den Augenhöhlen, Licht von unten kann sehr drastische, unheimliche Eindrücke vermitteln. Dies gilt ebenfalls für ein Gegenlicht als Führungslicht. Das Führungslicht leuchtet nicht nur die im Mittelpunkt der Szene stehenden Personen und Objekte aus, sondern auch das Umfeld. Es wird bevorzugt dazu verwendet, den Blick des Betrachters auf das Wesentliche zu lenken. Die Qualität des Lichtes kann hart oder weich sein.
**** Fülllicht
Das Füll-Licht, oder auch Aufhellung (engl.: Fill-In, Fill-Light) genannt, hellt bzw. füllt die durch das Führungslicht verursachten Schatten auf. Es ist ein weiches, kaum Schatten werfendes Licht, dass dem Führungslicht allerdings nicht seine Dominanz nimmt, vielmehr ist es dem Führungslicht untergeordnet. Es wird meist in einem Winkel von 45 Grad zum Führungslicht aufgestellt (Abb. 2). Als Aufhellung können künstliche Lichtquellen oder Reflektoren eingesetzt werden.
**** Spitzlicht
Das Spitzlicht, oder auch Kante, Effektlicht oder Haarlicht (engl.: Back Light, Counterkey) genannt, ist ein zumeist von hinten auf das Motiv gerichtetes Licht. Es wird in der Tiefe möglichst hoch angebracht und gegen die Kamera ausgerichtet. Es soll helfen, das Objekt klarer vom Hintergrund abzuheben und Glanzpunkte im Haar oder an reflektierenden Flächen zu erzeugen. Die Lichtstärke ist i.d.R. ähnlich stark wie die des Führungslichts und die Lichtqualität hart.\\
#+attr_html: :width 400px
[[./aufstellung.jpg]]

*Verhältnis zwischen Führungslicht und Füll-Licht*\\
Das Beleuchtungsstärkeverhältnis zwischen Führungs- und Füll-Licht richtet sich nach dem für die Szene gewünschten Stil. Durch geringe Kontrastverhältnisse wird Motiven beispielsweise in der Portraitfotografie geschmeichelt. Die Aufhellung der anderen Gesichtshälfte sollte nur halb so hell sein wie das Führungslicht. In der Praxis findet also ein Verhältnis von 2:1 Anwendung.
Einen stimmungsvolleren Eindruck vermittelt dagegen 3:1. Ein Low-Key (4:1) entspricht dem Stil der Gangsterfilme der 40er und 50er Jahre und 5:1 erscheint sehr dramatisch. Bei höheren Werten wie 7:1 oder 8:1 nähern Sie sich bereits dem Schattenbereich, bei dem Nachtszenen denkbar wären.

Schauen Sie sich bitte abschließend das folgende Video an: https://www.youtube.com/watch?v=UyiwfeARuno
** Selbsttest
1. Was bezeichnet der Begriff tungsten light?
   - @@html:<font color = "red">@@X Kerzen- und Sonnenlicht@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Farbtemperatur: ca. 3.200° Kelvin@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Kunstlicht@@html:</font>@@
   - @@html:<font color = "red">@@X natürliches Licht@@html:</font>@@
   - @@html:<font color = "red">@@X Farbtemperatur: ca. 5.500° Kelvin@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark entsteht durch elektrische Spannung@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Leuchtstoffröhren und Glühbirnen@@html:</font>@@
   - @@html:<font color = "red">@@X entsteht natürlich@@html:</font>@@
2. Im Schatten wirkt das Licht
   - @@html:<font color = "red">@@X grünlich@@html:</font>@@
   - @@html:<font color = "red">@@X gelblich@@html:</font>@@
   - @@html:<font color = "red">@@X rosa@@html:</font>@@
   - @@html:<font color = "red">@@X rötlich@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark bläulich@@html:</font>@@
3. Aufbau des Auges 2 - Lückentext
   - Die meisten Elemente des menschlichen Auges sind dafür zuständig, ein Bild aus der Umgebung - wie eine   *Kamera*   - scharf auf eine lichtempfindliche Fläche abzubilden. Dabei wird das ins Auge einfallende   *Licht*  durch lichtbrechende Organe wie Hornhaut, *Linse*  und  *Glaskörper*  gebündelt und scharf gestellt. Die *Iris*  formt in der vorderen Augenkammer das in seiner Größe variable Sehloch (  *Pupille*  ). Jedes dieser Elemente trägt dazu bei, dass ein scharfes, nicht zu helles Bild auf die Netzhaut (*Retina*) abgebildet wird. Dort befinden sich die hell- und dunkelempfindlichen   *Stäbchen*  und die farbempfindlichen   *Zapfen*  , die beide das Licht aufnehmen. Die Zapfen liegen besonders dicht in der Sehgrube (Fovea), der Zone der größten Sehschärfe in der Mitte des *gelben Flecks* .
4. Was ist die Dreipunktbeleuchtung?
   - @@html:<font color = "green">@@\checkmark Beleuchtungsvariante, um dreidimensionale Formen in einem zweidimensionalen Bild besser abbilden zu können@@html:</font>@@
   - @@html:<font color = "red">@@X Beleuchtungsvariante mit drei Taschenlampen, um im Dunkeln besser filmen zu können@@html:</font>@@
   - @@html:<font color = "red">@@X Mit nur einer Lampe können zeitgleich drei Punkte beleuchtet werden.@@html:</font>@@
   - @@html:<font color = "red">@@X besteht aus drei Laserpointern@@html:</font>@@
   - @@html:<font color = "red">@@X Führungslicht hellt durch Fülllicht verursachte Schatten auf@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Das Führungslicht ist die lichtstärkste Quelle.@@html:</font>@@
   - @@html:<font color = "red">@@X Einer der drei Laserpointer soll dabei helfen, das Objekt besser vom Hintergrund abzuheben.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark besteht aus Führungs-, Füll- und Spitzlicht@@html:</font>@@
5. Leuchtmittel 2 - Was ist richtig?
   - @@html:<font color = "red">@@X Softboxen werfen hartes Licht.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Mit Filtern kann die Farbtemperatur des Lichts beeinflusst werden.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Die Lichtqualität lässt sich an Hand der Schatten überprüfen.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Punktförmige Lichtquellen produzieren hartes Licht.@@html:</font>@@
   - @@html:<font color = "red">@@X Weiches Licht ist erkennbar an klar konturierten Schatten.@@html:</font>@@
   - @@html:<font color = "red">@@X Flächenleuchten können nicht gedimmt werden.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Hartes Licht zeichnet sich durch hohe Kontraste und klar erkennbare Schatten aus.@@html:</font>@@
   - @@html:<font color = "red">@@X Hartes Licht eignet sich besonders gut für Beautyaufnahmen.@@html:</font>@@
6. Leuchtmittel - Lückentext
   - *Stufenlinsenleuchten* oder auch *Fresnel-Linsenscheinwerfer* genannt,  sind universal einsetzbare Leuchten mit einem weichen Lichtrand. Eine der Besonderheiten ist die eingesetzte *Fresnel-Linse* . Erfunden um 1822 von dem französischen Physiker *Augustin Jean Fresnel* , verringert dieses Bauprinzip entscheidend das Gewicht und Volumen großer Linsen.  Dies geschieht durch eine Aufteilung in ringförmige Bereiche. In jedem dieser Bereiche wird die Dicke verringert, sodass die Linse eine Reihe ringförmiger Stufen erhält. Da das Licht nur beim Passieren der Linsen-Oberflächen *gebrochen* wird, ist der Brechungswinkel nicht von der Dicke, sondern nur vom *Winkel* zwischen beiden Oberflächen abhängig. Die Linse behält ihre *Brennweite* bei. Das Leuchtmittel liegt im Brennpunkt eines kreisförmigen *Reflektors* . Da der Reflektor auf einer Schiene montiert ist, ist der Abstand zur Linse verstellbar.  Hierdurch wird es möglich, den Lichtaustritt von einem schmalen Lichtstrahl (engl.: *Spot*) hin zu einem breiten Lichtstrahl (engl.: *flood*) stufenlos zu regulieren. Dieser Abstrahlwinkel einer Stufenlinse ist je nach Modell über einen Bereich von ca. 8°-60° einstellbar. Das Licht nimmt dabei nach außen hin langsam *ab*, wodurch ein *weicher* Übergang entsteht. Da Stufenlinsen, im Gegensatz zu PC-Linse keinen störenden
   *Hot-Spot*  haben, eignen sie sich besonders gut zur gerichteten, gleichmässigen, flächigen aber dennoch akzentuierten Ausleuchtung.
7. Auf welche Farbe reagieren die Zapfen am empfindlichsten?
   - @@html:<font color = "green">@@\checkmark Gelb bzw. Grün bei 550 Nanometern@@html:</font>@@
   - @@html:<font color = "red">@@X Blau bei 32 Nanometern@@html:</font>@@
   - @@html:<font color = "red">@@X Rot bzw. Lila bei 109 Nanometern@@html:</font>@@
8. Was ist ein Lichtformer?
   - @@html:<font color = "red">@@X Lichtformer ist die deutsche Bezeichnung von englischen Beruf gaffer.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Mit Lichtformern lassen sich Lichtstrahlen lenken.@@html:</font>@@
   - @@html:<font color = "red">@@X Retina@@html:</font>@@
   - @@html:<font color = "red">@@X Animated GIF@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Filter@@html:</font>@@
   - @@html:<font color = "red">@@X Ein Lichtformer ist ein Beruf, bei dem der Arbeitnehmer unterschiedliche Formen von Lampen kreiert.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Softbox@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Reflektor@@html:</font>@@
9. Durch was wird sowohl hell- als auch dunkeladaptiertes Sehen möglich?
   - @@html:<font color = "green">@@\checkmark Stäbchen@@html:</font>@@
   - @@html:<font color = "red">@@X Stalaktiten@@html:</font>@@
   - @@html:<font color = "red">@@X Ringe@@html:</font>@@
   - @@html:<font color = "red">@@X Kügelchen@@html:</font>@@
   - @@html:<font color = "red">@@X Reflektoren@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Zapfen@@html:</font>@@
   - @@html:<font color = "red">@@X Würfelchen@@html:</font>@@
   - @@html:<font color = "red">@@X Grübchen@@html:</font>@@
10. Was ist die "Blaue Stunde"?
    - @@html:<font color = "green">@@\checkmark Stunde, in der das Licht in der Dämmerung bläulich wirkt @@html:</font>@@
    - @@html:<font color = "red">@@X Stunde, in der es besonders kalt ist@@html:</font>@@
    - @@html:<font color = "red">@@X Stunde der Aufzeichnung mit einem Blue Screen @@html:</font>@@
    - @@html:<font color = "red">@@X Stunde, in der ein Mensch zu viel getrunken hat@@html:</font>@@
    - @@html:<font color = "red">@@X Sperrstunde bei Fernsehsendern@@html:</font>@@
11. Welche zwei Arten des Lichts gibt es in der Beleuchtungstechnik?
    - @@html:<font color = "red">@@X Xenonlicht@@html:</font>@@
    - @@html:<font color = "red">@@X Seitenlicht@@html:</font>@@
    - @@html:<font color = "red">@@X Streiflicht@@html:</font>@@
    - @@html:<font color = "red">@@X Vorderlicht@@html:</font>@@
    - @@html:<font color = "red">@@X Blaulicht@@html:</font>@@
    - @@html:<font color = "green">@@\checkmark Kunstlicht@@html:</font>@@
    - @@html:<font color = "green">@@\checkmark Natürliches Licht@@html:</font>@@
12. Nennen Sie fünf Bestandteile des menschlichen Auges!
    - @@html:<font color = "green">@@Sehnerv, Tränenapparat, Augapfel, Augenlider, Augenmuskulatur@@html:</font>@@
* Kapitel 3: Video
Das digitale Video erweitert das Kapitel Bild: Digitale Bilder werden als Bildfolge um eine zeitliche Dimension bereichert und später mit Audio unterlegt. Waren schon Bilder problematisch im Datenaufkommen, so verschärft sich die Fragestellung nach geeigneten Kompressionsverfahren bei Video zusätzlich.

Ursprünglich stammt das Wort Video vom E-konjugierten, lateinischen Wort "videre" ab und bedeutet "ich sehe". Die Entwicklung des Videos reicht in die 30er Jahre zurück und ist, was die klassische analoge Videotechnik betrifft, stark durch die historische Entwicklung geprägt. Video hat in den 70er Jahren von der Studiotechnik her kommend auch den Konsumbereich erobert als mehrere konkurrierende inkompatible Verfahren auf den Markt kamen. Dazu gehörten als Video-Aufzeichnungsverfahren VHS, Video 200, Beta, VCR, Video-8 und Hi8, um nur die bekanntesten zu nennen. Durchsetzen konnte sich nur das VHS-System und später im mobilen Bereich auch das Video-8-System. Beide Systeme sind heute weltweiter Standard, aber stark rückläufig.

Die technische Entwicklung hat in relativ kurzer Zeit erstaunliche Fortschritte gemacht und führte neben der Miniaturisierung auch von den analogen Videoformaten hin zu DV-Formaten für Digitalvideo. Das wirkte sich auch auf die Speichermedien aus. So sind als Datenträger für Videoinformationen neben den VHS- und Video-8-Bandgeräten die Bildplatte, die DVD und Blu-Ray-Disc, die gegenüber der Bildplatte wesentliche Vorteile aufweist. Während vor nicht allzu langer Zeit noch Bandformate zur Aufzeichnung genutzt wurden, werden diese mittlerweile fast vollständig von den digitalen Aufnahmevarianten wie SD-, Compact Flash-,SXS- oder mittlerweile auch XQD-Karten verdrängt. Dadurch entfällt das meist stundenlange Überspielen von Rekorder auf den PC und die Daten können nahezu überall angesehen werden.

Schauen Sie sich als Einführung in die Video- und Fernsehtechnik bitte folgendes Video an: https://youtu.be/H9bV5LdlIhs
** Definition Video
Video beruht auf einer Täuschung des menschlichen Auges. Dabei wird die Trägheit der Wahrnehmung ausgenutzt: Schnell aufeinanderfolgende Sinnesreize verschwimmen und können nicht mehr unterschieden werden. Beim Video werden in schneller Folge einzelne Bilder präsentiert. Veränderungen in diesen Bildern werden als Bewegung wahrgenommen. Das Prinzip ist das des Daumenkinos. Je mehr Bilder pro Sekunde gezeigt werden, desto weicher erscheinen die Bewegungen. Einigermaßen gleichförmige Bewegungen entstehen bereits bei 16 Bildern pro Sekunde, klassische Kinofilme verwenden 24 Bilder pro Sekunde und bei dem hauptsächlich in Europa standardisierten Verfahren PAL sehen wir eine Bildfolge von 25 Bildern (Bildwechselfrequenz).

Ähnlich wie bei einem einzelnen Bild haben auch Videos - die aus mehreren Einzelbildern zusammengesetzt werden - bestimmte Seitenverhältnisse (Abb. 1). Dabei ist das Verhältnis zwischen der Bildbreite und -höhe jedoch völlig unabhängig vom eigentlichen Fernsehformat.\\
Bei SDTV stehen Breite und Höhe in einem Verhältnis von entweder 4:3 oder 16:9. In den vergangenen Jahren wurde aber in Verbindung mit HDTV vermehrt zu einem Bildformat von 16:9 übergegangen, sodass es nur noch wenige Fernsehsender gibt, die ein fast quadratisches Bildseitenverhältnis zeigen.

#+attr_html: :width 400px
[[./sd-raster.jpg]]
#+attr_html: :width 400px
[[./hd-raster.jpg]]

Die Bildauflösung gibt die Größe eines Bildes bzw. Videos an und orientiert sich dabei an einer Rastergrafik. Entweder wird die Gesamtzahl der Bildpunkte benannt oder die Anzahl der Spalten und Zeilen angegeben. Eine Auflösung von beispielsweise 1920x1080 Pixeln ist 1920 Bildpunkte breit und 1080 Bildpunkte hoch. Hierbei handelt es sich übrigens um das oft angepriesene Full HD.

Das Standard Definition Television (SDTV) war lange Zeit unser täglicher Begleiter. Üblich war hier ein Bildseitenverhältnis von 4:3 und einer Bildauflösung von 720x576 und erscheint uns nach einer Gewöhnung an das hochauflösende Fernsehen eher "matschig" und unscharf. Das High Definition Television (HDTV) löst das bisherige SDTV ab und bezeichnet Formate, die mehr als 576 Zeilen pro Bild haben. Dieser Standard wurde gemäß unseren Sehgewohnheiten und dank neuer technischer Möglichkeiten entwickelt, denn das menschliche Auge sieht in der Horizontalen rund 70 Grad mehr als in der Vertikalen, wodurch sich ein breiteres als höheres Bild ergibt (siehe Tabelle).

| Bezeichnung            | 720p50      | 1080i25     | 1080p25     | 1080p50     |
|------------------------+-------------+-------------+-------------+-------------|
| Aufloesung             | 1280x720    | 1920x1080   | 1920x1080   | 1920x1080   |
| Bildrate pro Sekunde   | 50Hz        | 25Hz        | 25Hz        | 50Hz        |
| Vollbilder pro Sekunde | 50          | 50          | 25          | 50          |
| Bildverarbeitung       | progressiv  | interlaced  | progressiv  | progressiv  |
| Pixelseitenverhältnis  | quadratisch | quadratisch | quadratisch | quadratisch |
| Bildseitenverhältnis   | 16:9        | 16:9        | 16:9        | 16:9        |

Eine Besonderheit stellt die Streckung eines Bildseitenverhältnisses von 4:3 auf 16:9 dar. Dabei handelt es sich um ein anamorphes Format namens HDV, das vortäuscht, ein "echtes" 16:9-Bild zu sein. HDV löste als erste High Definition-Variante das lang genutzte DV-Format ab (720x576 Pixel). Zwar zeichnet HDV die digitalen HD-Signale ebenfalls auf Band auf, stellt dabei aber eine höhere Auflösung zur Verfügung: Durch eine Komprimierung im MPEG2-Verfahren entsteht eine Bildauflösung von 1440x1080 Pixel in 25i. Das entspricht aber einem Verhältnis von 4:3. Daher werden bei HDV die quadratischen Bildpunkte horizontal gestreckt, um ein Bildseitenverhältnis von 16:9 zu erreichen (Bild unten).\\
#+attr_html: :width 400px
[[./streckung.jpg]]

Wir wissen nun, dass ein Video in unseren Breitengraden i.d.R. aus einer Aneinanderreihung von 25 Bildern (frames) pro Sekunde besteht. Jedoch muss das nicht zwangsläufig bedeuten, dass jedes dieser einzelnen Bilder vollflächig angezeigt wird. Hier unterscheidet man in progressiv und interlaced.

*Progressive Scan*\\
Beim Vollbildverfahren wird - wie der Name schon sagt - ein Bild vollständig gezeigt. Dabei wird es Zeile für Zeile von der ersten bis zur letzten Zeile von links nach rechts nacheinander aufgebaut. Da dieser Vorgang für unser menschliches Auge mit 1/25 Sekunden viel zu schnell ist, bemerken wir diesen Aufbau nicht. Diese Abtastung ist zwar in vertikaler Richtung höher aufgelöst, hat aber den Nachteil, dass für uns durch nur eine Bewegungsphase ein leicht sichtbares Ruckeln entsteht. Eine Lösung beschäftigt sich mit dem Einfügen weiterer Zwischenbilder, wofür jedoch auch mehr Bandbreite zur Verfügung stehen müsste, die mehr Geldressourcen erfordert.

*Interlaced Scan*\\
Um das Ruckeln des Vollbildverfahrens zu verbessern, aber gleichzeitig das Problem einer größeren Datenrate zu umgehen, wurde das Zeilensprungverfahren entwickelt. Das Bild baut sich dabei ebenfalls von links nach rechts und oben nach unten auf. Das passiert zwar auch Zeile für Zeile, aber in der gleichen Zeit werden zuerst die ungeraden Zeilen des ersten Bildes und danach die geraden Zahlen des nachfolgenden Bildes abgetastet. Kurz gesagt wird immer die Hälfte des Bildes weggelassen und durch die andere Hälfte des nachfolgenden Bildes vervollständigt. Das bedeutet, dass unseren Augen 50 Bilder in der Sekunde geliefert werden. Diese Halbbilder sind ineinander verschachtelt, sodass durch die Trägheit des Auges und des Monitors der Eindruck eines vollständigen Bildes ergibt.

https://youtu.be/gbtBP8QxGg8

Zwischen diesen zwei Bilder ist im Normalfall eine im Bild sichtbare Bewegung schon leicht vorangeschritten, wodurch ein flüssiger Eindruck bekommen entsteht (zwei Bewegungsphasen). Schalten wir aber beispielsweise einen Videorekorder auf Standbild, werden wir feststellen, dass das gezeigte Bild nicht mehr klar erkennbar ist und sich kleine Treppen bilden und deren Kanten flimmern, da es sich ja um zwei ineinander verkämmte Bilder handelt. Besonders deutlich wird dieses Problem bei Diagonalen im Bild.\\
Heutzutage stellen aber viele Displays mittlerweile Vollbilder dar und "wandeln" derartige Aufnahmen in Vollbilder um. Das Zusammenfügen von Bilder aus dem Zeilensprungverfahren nennt man Deinterlacing.

Um nun Bildauflösung, Abtastverfarhen und Bildrate kurzzufassen, wurde eine Nomenklatur eingeführt, die jedoch nicht überall einheitlich verwendet wird. Zum einen werden Zeilenzahl, Bildaufbauverfahren und Bildwiederholrate genannt, zum anderen beschränkt man sich auf die Nennung von Zeilenzahl und Bildaufbauverfahren. Daraus enstehen zwei mögliche Varianten:
- z. B. 1080i25oder 1080i
Mit der Zeilenzahl ist also die Höhe eines Videobildes in Pixel, mit Bildaufbau verfahren progressiv (p) oder interlaced (i) und mit Bildwiederholrate die Anzahl der gezeigten Bilder (25 oder 50) gemeint.

Schauen Sie sich zusammenfassend bitte folgendes Video an: https://youtu.be/OxoFJDCyLSI

** Dateigröße
Für die Codierung von Video gelten die in den vorangegangenen Kapiteln vorgestellten Eigenschaften von Bild und und dem später vorgestellten Ton natürlich weiter - vor allem in Bezug zum Verhältnis von Qualität und Speicheraufwand. Für den Bildbereich kommt aber noch die zeitliche Dimension hinzu: wir codieren nun nicht mehr ein Bild, sondern eine ganze Sequenz von Bildern.

Ein Video ist aufgeteilt in eine Folge von Einzelbildern (engl.: frame). Der Speicherbedarf für die Einzelbilder berechnet sich zunächst ganz klassisch nach der räumlichen Ausdehnung, also der Anzahl der verwendeten Pixel und der genutzten Farbtiefe (siehe Bild). Wie bei Bildern gilt natürlich auch hier: Je mehr Pixel verwendet werden und je größer die Farbtiefe, desto besser die Qualität aber auch desto größer der Speicheraufwand.\\
#+attr_html: :width 400px
[[./sequenz.jpg]]

Hinzu kommt die zeitliche Dimension. Damit ist zum einen natürlich gemeint, wie lang das Video dauert, aber auch wie viele Bilder pro Sekunde gezeigt werden, bezeichnet als Bildwiederholfrequenz oder Bildwiederholrate. Auch hier gilt: Je mehr Bilder pro Sekunde gezeigt werden, desto besser die Qualität, aber auch desto größer der Speicherbedarf. Dieser lässt sich leicht wie folgt berechnen:\\
#+attr_html: :width 400px
[[./video-berechnung.jpg]]

Eine Minute Analogvideo soll mit VGA-Auflösung (640 · 480 px) digitalisiert werden. Die anvisierte Farbtiefe beträgt 8 Bit, die Bildwiederholrate 25fps. Der Audioanteil ist mit einer Größe von 10 MB gegeben. Wie groß ist die entstehende Datei?\\
#+attr_html: :width 400px
[[./video-berechnung-beispiel.jpg]]

Eine Minute VGA-Video benötigt unkomprimiert also ca. 471 MB Speicherplatz.

Es ist ziemlich offensichtlich, dass wir bei Videos vor der gleichen Problematik stehen wie bei Bildern und den später besprochenen Audiodateien: Das Rohmaterial ist für eine sinnvolle Arbeit einfach zu datenintensiv. Speichern von Videos und Transport über Netzwerke kann nur sinnvoll geschehen, wenn wir entsprechende Kompressionsverfahren verwenden.

** Kompression und Codec
Große Videodaten werden durch unterschiedliche Verfahren verkleinert und für eine schnelle Dekompression vorbereitet, um große Datenmengen besser transportieren zu können.\\
Das zentrale Konzept im Umgang mit Videos ist das des Codec. Hierbei handelt es sich um ein Verfahren zu Kompression und Dekompression von Videodaten. Der Begriff Codec selbst ist ein Kunstwort und setzt sich aus dem englischen compression und decompression zusammen. Codecs können als Software realisiert werden, wie es in der Regel bei Abspielgeräten (z. B. Windows Media Player) der Fall ist. Für zeitkritische Anwendungen, beispielsweise bei Kameras oder Editierwerkzeugen, werden sie auch als Hardware realisiert, da sie dann leistungsfähiger sind.

Codecs arbeiten asymmetrisch: Der Kompressionsvorgang, der i.d.R. nur einmal vorgenommen wird, dauert länger als der Dekompressionsvorgang, welcher bei jedem Abspielen eines Videos durchgeführt wird. Ziel der Kompression ist eine Datei, die klein und leicht zu transportieren ist, aber auch schnell wieder dekomprimiert werden kann.

Dafür gibt es zahlreiche gängige Codecs:
- Cinepak: Hierbei handelt es sich um einen Klassiker für die Kompression von Videos für CD-ROMs. Die Videoqualität ist eher mäßig, aber der Codec funktioniert auch mit alten Computersystemen.
- Indeo: Dieser Codec ist ebenfalls gut für ältere System und CD-ROMs geeignet. Er wurde bereits in den 1980er Jahren von der Firma Intel entwickelt.
- Motion JPEG: Hierbei wir jedes Einzelbild vollständig als JPEG codiert. Dies ist eine vergleichsweise speicherintensiv Verfahrensweise und eignet sich nur für spezielle Bedürfnisse, wie z. B. Editieren.
- Außerdem: H.261, H.263, H.264 und den MPEG-Codec
Schauen Sie sich bitte das folgende Video an, um mit Hilfe dieser vereinfachten Darstellung die Grundlagen des En- undDecodierens zu verstehen: https://youtu.be/Js47ZWnwGVo

Da in den vergangenen Jahren unzählige Codecs entwickelt worden sind und immer neue hinzukommen, sollen an dieser Stelle nur die folgenden drei Codecs beispielhaft dargestellt werden.

*** H.261
Der Standard H.261 gilt als zentraler Meilenstein in der Entwicklung der Videocodecs. Er wurde durch die International Telecommunication Union ITU (Internationale Fernmeldeunion) 1984 bis 1990 entwickelt. Als Anwendungsgebiet sind Videokonferenzen und Videotelefonie über die damals aufkommenden ISDN-Leitungen definiert. Entsprechend durfte die genutzte Bandbreite nur 64 kBit/s, bzw. ein Vielfaches davon betragen. Das entspricht der Leistung eines B-Kanals herkömmlicher ISDN-Anschlüsse. ISDN erlaubt die Kopplung von bis zu 30 solcher Kanäle.

Um eine möglichst reibungslose Kommunikation zu ermöglichen darf die durch die Kodierung und Dekodierung entstehende Verzögerung des Video- und Audiosignal maximal 150ms betragen. Die Bildwiederholfrequenz muss 29,97 fps erreichen, was dem amerikanischen Fernsehstandard entspricht. Die Auflösung beträgt entweder 352x288 Pixel im Common Intermediate Formate (CIF) oder 176x144 Pixel im Quarter CIF (QCIF). CIF ist ein Austauschformat für verschiedene Fernsehstandards, so dass mit einem Videokonferenzsystem beispielsweise sowohl amerikanische als auch europäische Teilnehmer bedient werden können. QCIF liefert ein Viertel (engl.: quarter) der Auflösung.

Der Clou des H.261-Standards liegt aber nicht in Auflösung und Bildrate sondern in der unterschiedlichen Behandlung der Einzelbilder: nicht alle Bilder werden vollständig codiert (Abb. 1). Es wird zwischen zwei Typen von Einzelbildern unterschieden: Die Intraframes (I-Frame) werden vollständig codiert. Das Verfahren entspricht dem der JPEG-Codierung. Die Qualität ist entsprechend hoch. Daneben existieren Interframes, bzw. Predicted Frames (P-Frame), die nur die Veränderungen zwischen den Einzelbildern festhalten. Das Prinzip kennen wir bereits aus der Bildcodierung: Veränderungen im Signal sind in der Regel klein und daher sparsamer in der Codierung.\\
#+attr_html: :width 400px
[[./h261.jpg]]

Wenn ein P-Frame so viel weniger an Datenvolumen benötigt als ein I-Frame, warum wird dann nicht einfach ein I-Frame am Anfang der Datei gesetzt und dann das restliche Video vollständig mit P-Frames codiert? Dafür gibt es zwei Gründe: Zunächst einmal ist der Codec in erster Linie für den Transport von Videodaten konstruiert. Transportkanäle sind aber immer anfällig für Fehler. Wenn nun ein Einzelbild bei der Übertragung ausfällt, fehlen dessen Information für die korrekte Decodierung aller folgenden Bilder. Daher muss für den Transport in bestimmten Abständen ein vollständiges Bild eingebaut werden.

Davon abgesehen ist aber die Codierung nie hundertprozentig korrekt, sondern unterliegt wiederum Quantisierungsfehlern, die wir auch bei der Audiocodierung noch kennenlernen werden. Für die Bildvorhersage werden auf der Basis eines Referenz-Frames, also des letzten I-Frames, Aussagen getroffen über Veränderungen hin zum aktuellen Frame, dem gerade abzuspielenden oder zu codierenden P-Frame. Dazu wird bei der Codierung das aktuelle Bild in 16x16 Pixel große, sogenannte Makroblöcke zerlegt. Im aktuellen Zielframe wird anschließend ein Makroblock nach dem anderen betrachtet und es werden im Referenz-Frame Regionen gesucht, die diesem Makroblock möglichst ähnlich sind. In Abbildung 2 wird gerade der dritte Block in der zweiten Reihe untersucht. Im Referenz-Frame befindet sich eine zu ihm ähnliche Region in der letzten Reihe zwischen dem ersten und zweiten Block. Anschließend wird der Verschiebungsvektor festgehalten. Dieser bezeichnet, wie weit die im Referenz-Frame gefundene Region von der Position des aktuell bearbeiteten Makroblocks entfernt ist.\\
#+attr_html: :width 400px
[[./prediction.jpg]]

Hier könnte das Verfahren beendet werden, wenn der untersuchte Makroblock und die gefundene Region im Referenz-Frame identisch sind. In der Regel gibt es aber kleine Unterschiede, die im letzten Schritt, der Differenzbildung beschrieben werden.

Der Standard H.263 ist eine Weiterentwicklung von H.261. Er wurde 1992 veröffentlicht und bietet bei einer Halbierung der Datenrate zusätzliche Auflösungen und eine bidirektionale Vorhersage, wie sie auch von MPEG verwendet wird.

*** MPEG
Die Moving Pictures Experts Group wurde 1988 gegründet mit dem Ziel Video- und Audiocodierung zu standardisieren. Seit dem sind verschiedene Standards veröffentlicht worden. Für die direkte Codierung on Videomaterial sind zunächst einmal die Standards MPEG-1, MPEG-2 und MPEG-4 wichtig:
- MPEG-1 (ISO 11172): Coding of moving pictures and Audio up to 1,5 Mbit/s wurde 1992 vorgestellt. Es definiert einen Software-Decoder von niedriger Qualität. Die Bandbreite ist auf 1,5 MBit/s beschränkt. Davon sind 1,25 MBit/s für Video reserviert, der Rest wird für zwei Audiokanäle benötigt. Es werden zahlreiche Auflösungen unterstützt, die von den amerikanischen und europäischen Fernsehnormen benötigt werden. Eine typische Auflösung wäre beispielsweise 352x288 Pixel, was er halben TV-Auflösung unserer Fernsehgeräte entspricht. MPEG-1 wird gerne für die Videokompression von Spielfilmen auf CD-ROMs verwendet.
- MPEG-2 (ISO 13818): Generic coding of moving pictures and associated Audio ist in der Funktionalität ähnlich wie MPEG-1, wobei einig zusätzliche Auflösungen und auch Interlacing ermöglicht wird. Audio kann in CD-Qualität abgespeichert werden. Der Codec ist für eine Implementierung als Software und auch als Hardware definiert.
- MPEG-4 (ISO 14496): Coding of audio-visual objects ist ein Standard für Multimedia-Applikationen. Der Aufbau von MPEG-4 ist grundsätzlich anders als bei den beiden Vorgängern. Hier werden sogenannte Audiovisuelle Objekte (AVO) beschrieben. Diese können natürlich klassische Videoströme sei. Für sie bietet MPEG-4 eine Weiterentwicklung des Kompressionsverfahrens, welche eine qualitativ hochwertige und dennoch sehr stark komprimierte Codierung beschreibt. Der Standard geht aber weiter in Richtung Multimedialität. So kann es sich bei den AVOs auch um ganz andere Objekte wie zum Beispiel virtuelle Welten handeln. Diese AVOs können in MPEG-4 beliebig miteinander kombiniert werden. So können komplexe interaktive Szenen entstehen.

Grundlage der MPEG-Codierung bildet wieder das Prinzip, nur die Veränderungen zu codieren und nicht die vollständige Bildinformation. Gegenüber H.261 gehen die MPEG-Standards aber noch weiter: Ziel ist es die Spannbreite zwischen den vollständig codierten I-Frames möglichst weit zu vergrößern. Dazu werden zusätzlich zu den P-Frames noch sogenannte Bidirectionaly Predicted Frames (B-Frames) eingebunden (siehe Bild).\\
#+attr_html: :width 400px
[[./mpeg.jpg]]

Wie bei H.261 enthalten I-Frames die vollständigen Bilddaten in JPEG kodiert. Bei MPEG werden diese Bildtypen meist alle 16 Bilder eingebaut. Jedes vierte Bild dazwischen wird als P-Frame codiert. Zwischen zwei I-Frames liegen also typischerweise 3 P-Frames. Da die Qualität der P-Frames mit zunehmender Zahl sinkt, ist dies eine sinnvolle Strategie, um den Qualitätsverlust zu verringern. Würden alle 15 Bilder zwischen zwei I-Frames als P-Frame codiert, würde sich die Qualität sichtbar verschlechtern.

Zusätzlich müssen nun aber noch die verbleibenden Bilder zwischen den P-Frames codiert werden. Hierfür wurde ein neuer Bildtypus geschaffen: der Bidirectionally Predicted Frame (B-Frame): Wie bei den P-Frames wir auch hier die Veränderung durch Bewegungsvorhersage codiert. Allerdings bezieht diese ihre Informationen aus zwei Richtungen: dem vorhergehendem und nachfolgendem P- oder I-Frame. Typischerweise sind zwei bis drei B-Frames zwischen zwei P- / I-Frames verankert.

Im obigen Bild nicht dargestellt ist ein zusätzlicher Bildtyp, der für das eigentliche Video nicht von Bedeutung ist. Der D-Frame (engl. DC: direct coded picture) ist ein Index-Bild für den Suchvorlauf. Es handelt sich um eine sehr grobe JPEG-Codierung die verwendet wird um den Suchvor- und -rücklauf darzustellen.

*** H.264
H.264 bedeutet in der Videokomprimierung einen großen Schritt nach vorn. Durch verbesserte Vorhersageverfahren und geringere Fehleranfälligkeit kann der neue Standard Verfahren mit höherer Komprimierungswirkung bereitstellen. Er eröffnet neue Möglichkeiten zur Programmierung besserer Video-Encoder, die Videoströme höherer Qualität, höhere Bildfrequenzen und höhere Auflösungen bei (im Vergleich zu älteren Standards) gleicher Bitrate oder umgekehrt, dieselbe Bildqualität bei reduzierter Bitrate bieten.

H.264 ist der erste gemeinsame, internationale Standard der Normierungsorganisationen ITU, ISO und IEC für die Videokomprimierung. Aufgrund seiner Flexibilität kommt H.264 in unterschiedlichen Bereichen wie High Definition-DVD (z. B. Blu-ray), digitales Fernsehen einschließlich HDTV, Online-Videospeichern (z. B. YouTube), in der Mobiltelefonie der dritten Generation und in Software wie QuickTime, Flash und dem Apple-Betriebssystem MacOS X zum Einsatz, aber auch in Videospielkonsolen wie der PlayStation 3. Mit der Unterstützung vieler Branchen und Anwendungen für den Kunden- und Profibedarf wird H.264 nach Meinung von Experten an die Stelle von heute üblichen Komprimierungsstandards und -verfahren treten. Durch die wachsende Verfügbarkeit von H.264 in Netzwerk-Kameras, Video-Encodern und Videoverwaltungssoftware müssen Systemdesigner und -integratoren bei der Auswahl von Produkten und Anbietern vermehrt auf eine Unterstützung dieses neuen offenen Standards achten. Bis dahin sind Netzwerk-Videoprodukte, die sowohl H.264 als auch Motion JPEG unterstützen, die ideale Wahl für maximale Flexibilität und Integrationsmöglichkeiten.
** Selbsttest
1. Was ist interlaced scan?
   - @@html:<font color = "red">@@X Vollbildverfahren@@html:</font>@@
   - @@html:<font color = "red">@@X Interlaced Scan gibt es nicht.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Der Aufbau des Bildes erfolgt zeilenweise von links nach rechts und oben nach unten, allerdings nur jede zweite Zeile.@@html:</font>@@
   - @@html:<font color = "red">@@X Der Aufbau des Bildes erfolgt spaltenweise von rechts nach links und oben nach unten, allerdings nur jede zweite Spalte.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Zeilensprungverfahren@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Zerlegung eines Videos in Halbbilder@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Durch die Zerlegung in Halbbilder verdoppelt sich zwar die Bildrate, jedoch bleibt die Bandbreite nahezu gleich und das Bild wirkt für unsere Augen durch zwei Bewegungsphasen flüssiger.@@html:</font>@@
2. Wie berechnet sich der Speicherbedarf eines Videos?
   - $FS = (f * r * c * t) + A_S$ wobei f=Pixelanzahl, r=frame-rate, c=Farbtiefe, t=zeitliche Dimension und A_s = Groesse der Audiodatei
3. Was ist ein Codec und wofür wird er benötigt?
   - @@html:<font color = "green">@@\checkmark Je mehr Details, Bewegung und Zeit ein Video enthält, desto stärker wird auch der Codec gefordert und desto größer werden die komprimierten Dateien.@@html:</font>@@
   - @@html:<font color = "red">@@X Ein Codec ist ein Übersetzer, der die im Video hörbare Sprache in in einer beliebigen anderen Sprache selbststädig nachsynchronisiert.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Ein Codec wird benötigt, um aus speicherintensivem Material kleine Dateien zu erzeugen.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Verfahren zur Kompression und Dekompression von Video- und Audiodaten@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Codec steht für compression und decompression.@@html:</font>@@
   - @@html:<font color = "red">@@X Je mehr Details, Bewegung und Zeit ein Video enthält, desto weniger wird auch der Codec gefordert und desto kleiner werden die komprimierten Dateien.@@html:</font>@@
   - @@html:<font color = "red">@@X Ein Codec bezeichnet ein Fernsehformat.@@html:</font>@@
   - @@html:<font color = "red">@@X Codec steht für Computer Operated Digital Encoded Cluster.@@html:</font>@@
   - @@html:<font color = "red">@@X Codec steht für Cubic Object Difference Extended Cycle.@@html:</font>@@
4. Was ist HDTV?
   - @@html:<font color = "green">@@\checkmark hochaufloesendes Bildformat@@html:</font>@@
   - @@html:<font color = "red">@@X hydrochlorofluorocarbon Television@@html:</font>@@
   - @@html:<font color = "red">@@X wurde entwickelt, um Videoformate an menschliche Sehgewohnheiten anzupassen, da das menschliche Auge in der Horizontalen rund 2 Grad mehr sehen kann als in der Horizontalen@@html:</font>@@
   - @@html:<font color = "red">@@X HDTV ist ein Codec.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark High Definition Television@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark übliches Bildseitenverhältnis 16:9@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Auflösung mehr als 576 Zeilen@@html:</font>@@
5. Nennen Sie drei Farbcodierverfahren!
   - SECAM, NTSC, PAL
6. Was ist ein Video?
   - @@html:<font color = "red">@@X ist die Abkürzung für Visual Interfaced Dynamic European Organisation@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark schnelle Abfolge einzelner Bilder, durch die Bewegung entsteht@@html:</font>@@
   - @@html:<font color = "red">@@X ist die Abkürzung für Virtual International Distributed Electronic Object@@html:</font>@@
   - @@html:<font color = "red">@@X ist ein Katteeker@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark beruht auf Täuschung und Trägheit des menschlichen Auges@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark parallel können Audiodaten übertragen werden@@html:</font>@@
   - @@html:<font color = "red">@@X Chartstürmer von 1987@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark unterscheidet sich in Bildformat, -auflösung sowie Bildaufbauverfahren@@html:</font>@@
7. Was ist SDTV?
   - @@html:<font color = "red">@@X hochauflösendes Fernsehen@@html:</font>@@
   - @@html:<font color = "red">@@X Sine Dato Television@@html:</font>@@
   - @@html:<font color = "red">@@X ein hochmodernes Fernsehgerät@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Standard Definition Television@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Bildformat mit Seitenverhältnis 4:3@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Auflösung von 720 x 576 Pixeln (PAL-Länder)@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Bildformat mit Seitenverhältnis 16:9@@html:</font>@@
   - @@html:<font color = "red">@@X ein Fernsehsender@@html:</font>@@
8. Was bedeutet progressive scan?
   - @@html:<font color = "green">@@\checkmark Der Aufbau erfolgt zeilenweise von links nach rechts und von oben nach unten.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Progressive scan ist ein Abtastverfahren, dass die Einzelbilder eines Videos als Vollbilder aufbaut.@@html:</font>@@
   - @@html:<font color = "red">@@X Der Aufbau erfolgt spaltenweise von rechts nach links und von oben nach unten.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Durch die Vollbilder gibt es nur eine Bewegungsphase, die unser Auge als Ruckeln wahrnehmen kann.@@html:</font>@@
   - @@html:<font color = "red">@@X Das Videomaterial muss dazu in einen Drucker mit Scanfunktion eingelegt werden.@@html:</font>@@
   - @@html:<font color = "red">@@X Durch die Zerlegung eines Einzelbildes in Halbbilder kann beim progressive scan die doppelte Datenmenge bei gleichbleibender Bandbreite genutzt werden.@@html:</font>@@
9. Was sind I-, P- und B-Frames und welche Aufgabe haben sie?
   - @@html:<font color = "green">@@\checkmark I-Frames sind für die vollständige Codierung eines Einzelbildes zuständig.@@html:</font>@@
   - @@html:<font color = "red">@@X B-Frames sind nur Lückenfüller. Sie haben keine weitere Funktion.@@html:</font>@@
   - @@html:<font color = "red">@@X I-Frames codieren nur das jeweils fünfte Einzelbild.@@html:</font>@@
   - @@html:<font color = "red">@@X Ein P-Frame codiert den I-Frame.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Ein B-Frame betrachtet nur die Veränderungen aus einem ihm vorangegangenen und nachkommenden P- oder I-Frame.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark I = Intraframe@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Ein P-Frame registriert nur die Veränderungen zwischen den einzelnen Bildern.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark B = Bidirectionally Predicted Frames@@html:</font>@@
   - @@html:<font color = "red">@@X I = Ingest Frame@@html:</font>@@
   - @@html:<font color = "red">@@X P = Permisson Frame@@html:</font>@@
   - @@html:<font color = "red">@@X B = Broadband Frame@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark P = Predicted Frame@@html:</font>@@
10. Was bedeutet 1080i25?
    - Zeilenzahl, Bildaufbauverfahren und Bildwiederholrate
11. Nennen Sie drei Codecs!
    - Motion JPEG, H.264, INDD
* Kapitel 4: Kameratechnik
Die Videokamera dient der Erfassung und Aufnahme von Bewegtbild und Ton. Was für uns heutzutage selbstverständlich ist, begann seine Entwicklung vor rund 150 Jahren und war für die damalige Generation noch undenkbar.

1873 entdeckte C. May die Lichtempfindlichkeit des Sehens. Es wurde möglich elektrische Ströme in Abhängigkeit von der Lichtintensität zu steuern.

Paul Nipkow patentierte 1884 sein Konzept zur Bildzerlegung und Informationsreduktion, die Nipkow-Scheibe. Eine drehbare Scheibe wurde mit Löchern in spiralförmiger Anordnung versehen, deren Anzahl der Zeilenanzahl des zerlegten Bildes entsprach. Mit einer Photozelle, die hinter der Scheibe montiert wurde, konnten Helligkeitsinformationen in elektrische Signale umgesetzt und an einen Empfänger mit einer reaktionsschnellen Lichtquelle weitergeleitet werden. Durch eine weitere (synchron bewegte) Scheibe konnte das zuvor abgetastete Bild wiedergegeben werden.

Karl Ferdinand Braun erfand 1897 die Kathodenstrahlröhre, Braunsche Röhre genannt, sie ist entscheidend für die spätere Entwicklung elektronischer Aufnahme- und Wiedergabesysteme. Ende des 19. Jahrhunderts wurden außerdem bereits die Grundlagen für die Magnetaufzeichnung (MAZ) geschaffen. Valdemar Poulsen etwa patentierte 1898 sein „Telegraphon“. In den 1920er Jahren kam es zu einer Intensivierung der Fernsehforschung. Die Synchronisation der Sendesignale wurde noch mechanisch (per Handbremse) vorgenommen.

1935 eröffnete in Berlin der erste regelmäßige Fernsehdienst (180 Zeilen-Norm). Als Zwischenstufe vor der Bildwandlung wurde meist auf Film aufgezeichnet und über Filmabtaster mittels Nipkow-Scheibe umgesetzt.

Die erste elektronische Kamera, auf Basis des Ikonoskops (elektronischer Bildabtaster), wurde 1936 vorgestellt. Die mechanischen Elemente verschwanden aus den Bildwandlungssystemen. Aufgrund des zweiten Weltkrieges wurden die USA zum Träger weiterer wichtiger Entwicklungen, 1941 wurde hier die bis heute in den USA gültige Fernsehnorm mit 525 Zeilen eingeführt. Die Einführung des vollelektronischen S/W kompatiblen NTSC-Farbfernsehsytems (National Televisions Sytems Commitee) folgte 1953. In Europa wurden in den 60er Jahren alternative Systeme eingeführt: SECAM (séquentiel couleur à mémoire) in Frankreich und PAL (Phase Alternation Line) in Deutschland.

1956 erschien der erste einsatzfähige Video Tape Recorder (VTR) für Sendezwecke von der Firma AMPEX. Erst Ende der 6oer Jahre aber wurden die Geräte, durch die Entwicklung von Kassetten-Systemen (Video Cassette Recorder, VCR), reif für den Consumermarkt. 1972 stellte Sony den ersten U-Matic Videokassettenrekorder vor. 1976 folgte das Format Video Home System (VHS) von JVC, das sich bis in die späten 90er als Standard halten sollte. Die Entwicklung des Consumer-Sektors wirkte auf die professionelle Seite zurück. Als Sony 1983 das Betacam-System vorstellte, vollzog sich auch im Produktionsbereich der Wechsel vom einkanaligen PAL-Signal zu dem bei Betacam verwendeten dreikanaligen Komponentensignal.

Auch die Digitalisierung der Heimanwendergeräte hatte Rückwirkung auf den professionellen Sektor. 1995 führte Sony das DV-Format ein, das in seinen Varianten DVCPro und DVCam für die professionelle Anwendung adaptiert wurde. Durch Firewire (IEEE 1394) wurden neue Möglichkeiten zur verlustfreien digitalen Datenübertragung auf den Computer geschaffen.

Gegenwärtig ist die Digitalisierung des Produktionsbereichs abgeschlossen, d.h. alle Geräte arbeiten digital und der Datenaustausch erfolgt meist in Echtzeit über das seriell digitale Interface. Die Gesamtstruktur ähnelt aber noch der überkommenen und man spricht davon, dass die erste Phase der Digitalisierung abgeschlossen ist, während die weite Phase vom verstärkten Einsatz datenreduzierter Signale und der weitgehenden digitalen Vernetzung der Produktionskomplexe geprägt ist.

Eine Kamera ist ein reines Bilderfassungsgerät. Die Speicherung/Aufzeichnung der Bilder erfolgt nicht in der Kamera, sondern im Rekorder. Eine Kombination dieser beiden Geräte nennt man Camcorder. Durch das Objektiv trifft Licht auf eine lichtempfindliche Einheit, den Aufnahmechip, der das Licht in ein elektrisches Signal umsetzt (Bildwandler). Die Kameraelektronik enthält verschiedene elektronische Schaltungen zur normgerechten Signalerzeugung, -veränderung und -wandlung (siehe Bild).
#+attr_html: :width 400px
[[./camcorder.png]]
** Objektiv
Das Objektiv gehört zu den wichtigsten Bestandteilen einer Kamera. Es justiert das Licht, das auf den Sensor fällt. Es steuert die gesamte Menge des Lichts über die Blende, filtert Ausschnitte über die Brennweite und definiert die Bereiche der Szenerie, die scharf dargestellt werden. Zusammen mit der Aufnahmezeit, die über den Verschluss der Kamera geregelt wird, umfasst es damit nahezu die volle Bandbreite der Einstellungsmöglichkeiten.

Neben dem Sensor gehört das Objektiv zu den zentralen Komponenten einer Kamera. Pocketkameras und Handykameras verfügen über recht einfache Objektive mit wenig bis keinen Einstellungsmöglichkeiten. Diese Objektive sind darauf ausgelegt, möglichst klein zu sein und gleichzeitig scharfe Bilder zu produzieren. Anwender können hier keine eigenen Einstellungen vornehmen, sondern sind auf die Einstellungen durch den Hersteller angewiesen.

Spiegelreflexkameras hingegen verfügen über Objektive, die eine ganze Reihe von Einstellungen erlauben. Diese sind:
- Fokussierung: Mit der Fokussierung bezeichnet man das Scharfstellen eines Bildes. Die Teile der fotografierten Szenerie, die scharf und nicht verschwommen erscheinen, liegen im Fokus.
- Brennweite: Mit der Brennweite wird die Eigenschaft eines Objektivs benannt, die Objekte näher oder ferner erscheinen lässt.
- Blende: Die Blende beeinflusst die Menge des Lichts, das auf den Film oder Sensor fällt. Je weiter die Blende geöffnet ist, desto mehr Licht kommt hindurch.

*** Fokussierung
Die Fokussierung, also das Einstellen der Schärfe, ist in der Fotografie von besonderer Bedeutung: Die Bildauswahl kann noch so schön sein - Ist das Bild unscharf, ist es ein Fehlschuss. Um zu verstehen, was bei der Fokussierung passiert, müssen wir einen Blick auf die Physik der Lichtbrechung werfen. Abb. 1 zeigt eine stark vereinfachte Darstellung dieses Phänomens, wie es sich in jeder Kamera abspielt. Von einem Objekt in einer Szenerie werden Lichtstrahlen reflektiert und treffen auf die Kameralinse. Diese Lichtstrahlen werden von der Kameralinse gebrochen, und zwar so, dass sie gebündelt auf den Sensor oder Film im Inneren der Kamera treffen. Ist diese Bündelung optimal, dann treffen alle Lichtstrahlen, die beispielsweise von einem Jackenknopf ausgehen, auf dem Sensor auf der gleichen Stelle ein und bilden so das scharfe Abbild des Jackenknopfes.\\
#+attr_html: :width 400px
[[./refl.jpg]]

Wird aber die Position zwischen der Kamera und dem Jungen verändert, so verändert sich natürlich auch der Brechungswinkel des Lichts. Im Beispiel (Abb. 1) wurde die Distanz zwischen Linse und Jungen erhöht. Damit das Bild scharf wird, müsste nun auch die Distanz zwischen Linse und Sensor bzw. Film erhöht werden. Bleibt die Distanz wie hier gezeigt aber gleich, dann werden die Strahlen nicht mehr optimal gebündelt und treffen an unterschiedlichen Stellen auf den Sensor bzw. Film. Das Bild wird unscharf.

In Wirklichkeit stehen Sie aber vor der Situation, dass Sie den Abstand zwischen Linse und Sensor bzw. Film nicht ändern können. Beide sind fest im und am Kameragehäuse positioniert. Was Sie aber verändern können ist der Brechungswinkel der Linse. Die obige Abbildung ist stark schematisierend. Was hier als einzelne Linse dargestellt ist, wird bei realen Kameraobjektiven durch eine ganze Reihe von Linsen verwirklicht, oft mehr als ein Dutzend. Deren Position zueinander können Sie durch Drehen am Entfernungskranz des Objektivs beeinflussen. Dadurch ändert sich der Brechungswinkel des Objektivs und das zu fotografierende Objekt erscheint wieder scharf.

Den Fokus der Kamera manuell einzustellen ist eine mitunter lästige Pflicht, die man zumindest als Gelegenheitsanwender ohne künstlerische Ambitionen gerne der Kamera überlässt. Diese hat hierfür den sogenannten Autofokus. Beim Autofokus unterscheiden wir passive und aktive Verfahren. Das bekannteste Verfahren ist die passive Kontrastmessung.

Die Kontrastmessung macht sich den Umstand zunutze, dass Motive, die unscharf sind, auch gleichzeitig geringere Kontraste aufweisen als scharfe Motive. Der Kontrast gibt an, in wieweit sich benachbarte Bildregionen in ihrer Helligkeit voneinander unterscheiden. Bei unscharfen Bildern werden die Lichtstrahlen nicht mehr auf den Punkt gebündelt. Auf jedem Sensorelement treffen also Strahlen auf, die eigentlich zu den Nachbarelementen gesteuert werden sollten. So vermischen sich die Helligkeitswerte - der Kontrast sinkt.

Um eine Kontrastmessung durchzuführen, sind immer zwei Messungen notwendig. Dabei wird die Einstellung des Objektivs zwischen den beiden Messungen leicht verändert. Anschließend wird automatisch ausgerechnet, welche der beiden Messungen ein kontrastreicheres Ergebnis hatte. Üblicherweise wird die Messung nur in kleinen Regionen vorgenommen. Eine Messung ist kontrastreich, wenn sich die Helligkeitswerte des gemessenen Bereichs stärker unterscheiden.

Dargestellt ist ein abstraktes Objekt bestehend aus einer hellgrauen Fläche und einer schwarzen Fläche. Die Flächen berühren sich und es entsteht eine harte Kante. Rechts sind die gleichen Flächen gezeigt, nur unscharf: Die Kante ist verwaschen (Abb. 2). Die Kontrastmessung erfolgt nun an einem bestimmten Bereich der hier zufällig direkt auf der Kante liegt (rotes Viereck). Vergleicht man dazu jeweils eine 1000fache Vergrößerung dieses Bereichs, fällt der deutlich geringere Kontrastumfang des unscharfen Bildes auf.\\
#+attr_html: :width 400px
[[./kontrast1.jpg]]

Und hier die Vergrößerung:\\
#+attr_html: :width 400px
[[./kontrast2.jpg]]

Dieses Beispiel zeigt auch gleichzeitig die Probleme auf, welche die Kontrastmessung mit sich bringt. So ist der Messbereich hier tatsächlich sehr glücklich gewählt. Liegt der Messbereich aber direkt auf einer Fläche, so ergibt sich kein Unterschied zwischen scharfem und unscharfem Bild. Der Autofokus versagt. In der Praxis können Sie das leicht erkennen, wenn das Objektiv beim Scharfstellen das gesamte Spektrum der Einstellungen auf- und abfährt.

Ein anderes Problem liegt darin, dass für die Autofokuseinstellung über Kontrastmessung die Signale des Bildsensors benötigt werden. Dies wird bei Spiegelreflexkameras zum Problem, da hier der Spiegel das Licht in den Sucher umleitet und der Sensor erst beim eigentlichen Auslösen belichtet wird. Abhilfe schafft hier ein zusätzlicher Sensor, der ausschließlich für die Fokussierung genutzt wird.

Eine Alternative zur Kontrastmessung ist der so genannte Phasenvergleich oder auch Triangulation. Dabei sind hinter der Linse zwei Autofokussensoren angebracht, die auf das entfernte Objekt zielen. Je nach Winkel, den das Objekt zu ihnen einnimmt, kann die Entfernung berechnet werden. Der Vorteil gegenüber der Kontrastmessung liegt in der Geschwindigkeit. Die Kontrastmessung benötigt immer mindestens zwei Messungen mit einer zusätzlichen Neujustierung des Objektivs. Die Triangulation kann bereits bei der ersten Messung die Schärfe beurteilen. Allerdings ist die Technologie etwas aufwändiger, weshalb die Triangulation vorwiegend bei den teureren Spiegelreflexkameras verwendet wird.

Beide Verfahren gelten als passiv. Daneben gibt es eine Reihe von aktiven Verfahren, die nicht einfach auf einer Messung des einfallenden Lichtes beruhen. Üblicherweise senden diese Verfahren selbst Licht aus. Oft wird dabei zum Beispiel das Blitzlicht der Kamera aktiviert und ein Messblitz ausgesandt. Eleganter sind spezielle sichtbare rote oder unsichtbare infrarote Hilfslichter, die von der Kamera oder dem Blitzgerät emittiert werden. Diese Hilfslichter sind in der Regel vertikal gestreift. Das unterstützt wiederum sowohl Kontrast- als auch Phasenvergleich.

Hilfslichter gelten zwar als aktive Messung und damit als grundlegend anders als die passiven Verfahren Kontrast- und Phasenvergleich. Aber im Prinzip bringen sie nur Licht ins Dunkel, die eigentliche Messung läuft wieder über die passiven Verfahren. Ein rein aktives Verfahren ist die zum Beispiel bei Polaroid-Kameras verwendete Ultraschallmessung. Hier wird ein Ultraschall von der Kamera ausgesandt und von den zu fotografierenden Objekten als Echo zurück geworfen. Die Zeit, die dieses Echo benötigt, dient als Grundlage für die Entfernungsberechnung. Je länger die benötigte Zeit, desto weiter entfernt das Objekt.

*** Brennweite
Fallen Lichtstrahlen von vorn parallel auf die Linse auf, werden sie durch durch deren gekrümmte Oberfläche gebrochen und verlassen ihre parallele Bahn. Der Brennpunkt F (engl. Focal length) ist die Stelle, an der die Lichtstrahlen auf einen Punkt auf der Sensorebene gebündelt werden (Bild unten).

Die Brennweite f gibt die Entfernung zwischen dem Linsenmittelpunkt und ihrem Brennpunkt in Millimetern an. Dabei gilt: Je kleiner die Brennweite, desto größer ist der abgebildete Bildausschnitt. Zudem hat sie Einfluss auf gestalterische Aspekte wie Einstellungsgröße, Bildwinkel oder Schärfentiefe.|\
#+attr_html: :width 400px
[[./brennweite.png]]

Den Brennpunkt einer Linse kann man leicht visualisieren. Nehmen Sie eine Lupe und halten Sie sie über einem Blatt Papier in der Sonne. Wenn Sie nun die Höhe der Lupe wechseln, dann werden Sie feststellen, dass es eine bestimmte Höhe gibt, bei der auf dem Papier ein sehr heller und sehr kleiner Punkt entsteht. Hier wird das Licht auf den Brennpunkt gebündelt, die Höhe der Lupe entspricht ihrer Brennweite. Das Papier fängt übrigens an dieser Stelle tatsächlich an zu brennen.

Parallele Strahlen kommen in der Fotografie nicht vor. Zwar kommt das reflektierte Licht sehr weit entfernter Objekte wie der Sonne nahezu parallel an, aber eben nur nahezu. Diese werden scharf dargestellt, wenn der Abstand der Linse zum Sensor der Brennweite entspricht.

Die meisten übrigen fotografierten Objekte sind wesentlich näher. Das hat Auswirkungen auf die Brechung ihrer Strahlen. Um die Objekte scharf darzustellen, muss die Linse neu positioniert werden. Je näher ein Objekt zu der Linse positioniert ist, desto weiter weg muss der Sensor positioniert werden.

Der Begriff Normalobjektiv wird zum einen mit den menschlichen Sehgewohnheiten begründet, zum anderen über Verhältnis Brennweite zu Sensorgröße. Je nach verwendeter Brennweite erscheinen die entstandenen Bilder näher oder weiter entfernt als in der Originalszenerie. Ursprünglich wurde dieser Begriff in der Porträtfotografie definiert: Portraits, die 1:1 vergrößert werden und denselben perspektivischen Eindruck hinterlassen wie bei der Betrachtung des Originals (z.B. gut zu erkennen an der Position der Ohren), werden mit einem Normalobjektiv aufgenommen.

Bezogen auf die Brennweite lässt sich dieser perspektivische Eindruck auch etwas mathematischer formulieren: Objektive mit einer Brennweite, die in etwa der Diagonalen des Sensors entspricht, gelten als Normalobjektive.

Wie auch immer man bei der Definition vorgeht, in der Regel werden Objektive für Kleinbildkameras mit einer Brennweite von ca. 45-50 mm als Normalobjektive angesehen. Die Diagonale d lässt sich einfach aus den räumlichen Maßen der Sensorkanten berechnen: $$d=\sqrt{\text{Breite}^2 + \text{Hoehe}^2}$$\\
Bei einem klassischen Kleinbildfilm ist die Höhe 24 mm und die Breite 36 mm. Die Diagonale berechnet sich wie folgt:\\
#+attr_html: :width 400px
[[./bw2.jpg]]

Die Brennweite eines Objektivs für Kleinbildfilme liegt also bei ca. 43 mm. In der Praxis ist dies eher ein ungefährer Wert, die meisten Normalobjektive für Kleinbildkameras liegen bei Brennweiten von 45 mm oder 50 mm.

**** Weitwinkel
Weitwinkelobjektive sind Objektive, deren Bildwinkel größer ist als es dem menschlichen Auge entspricht. Das heißt, sie nehmen an den Rändern mehr Bildinformationen auf. Typische Weitwinkelobjektive für Kleinbildkameras haben eine Brennweite von 35 mm oder 28 mm. Je kürzer die Brennweite, desto stärker der Effekt. Ab einer Brennweite von 20 mm und darunter sprechen wir von Fischaugenobjektiv. Die Verzerrung ist hier sehr groß, gerade Linien erscheinen stark gekrümmt (siehe Bild - Weitwinkelaufnahme von einer Digitalkamera mit einer Brennweite von 24 mm).\\
#+attr_html: :width 400px
[[./weitwinkel.jpg]]

**** Teleobjektiv
Mit einem Teleobjektiv erreichen Sie den gegenteiligen Effekt: Ferne Dinge können sehr detailliert erscheinen. Der Bildwinkel und damit der abgelichtete Bildausschnitt ist deutlich kleiner als bei normalen Objektiven. Die Brennweite ist hier deutlich länger (Bild unten). Klassische Brennweiten für Teleobjektive sind 135 mm oder 200 mm. Spätestens ab einer Brennweite von 300 mm können Teleobjektive nicht mehr sinnvoll ohne Stativ eingesetzt werden, da sie stark verwackeln.\\
#+attr_html: :width 400px
[[./tele.jpg]]

Wir haben bisher von Objektiven und deren Brennweite gesprochen. Vor einigen Jahren noch, als das analoge Kleinbildformat für Spiegelreflexkameras der Standard war, hätten wir die Ausführungen so stehen lassen können. Seit dem Einzug der Digitalfotografie haben sich aber zahlreiche unterschiedliche Sensorgrößen etabliert, die unglücklicherweise Auswirkungen auf die Brennweitenberechnung haben (Bild unten - Digitalkamera link, analoge Kleinbildkamera rechts).\\
#+attr_html: :width 400px
[[./vergleichh.jpg]]

Zu sehen sind zwei Fotografien desselben Gebäudes. Beide Fotografien wurden mit einer Canon EOS mit einem 50 mm Objektiv aufgenommen. Der Unterschied liegt darin, dass die rechte Aufnahme mit einer analogen Kleinbildkamera gemacht wurde. Ein 50 mm Objektiv gilt hier als Normalobjektiv. Der Bildausschnitt ist deutlich größer. Die linke Aufnahme hingegen wurde mit einer Digitalkamera gemacht. Sie verfügt über einen 22,5 mm x 15 mm CMOS-Sensor. Damit ergibt sich eine Bilddiagonale von ca. 27 mm, welches auch die Brennweite eines Normalobjektivs für diese Kamera wäre. Ein 50 mm-Objektiv gilt hier bereits ein Teleobjektiv.

Zu Zeiten der klassischen 35 mm Kleinbildkamera galt die Brennweite noch als verlässliches Kriterium für Objektive. Da der Kleinbildfilm in Spiegelreflexkameras als Standard galt, gab es hier keine Verständigungsprobleme. Heute wissen Sie nur dann, was sich hinter einer Brennweite von 80 mm verbirgt, wenn Sie gleichzeitig auch die Größe des Bildsensors kennen.

Es bleibt der Ruf nach einer einheitlichen Benennung. Einige wenige Kamerahersteller geben daher neben der tatsächlichen Brennweite noch zusätzlich das Äquivalent für einen Kleinbildfilm an. Das ist eine gute Idee für alle, denen die alten Brennweiten noch etwas sagen.

Es gibt aber noch einen zweiten Weg: Der Bildwinkel gestaltet sich unabhängig von der Brennweite. Abbildung unten zeigt die gleiche Aufnahme mit zwei Kameras, die sich in ihren Brennweiten und Abbildungsgrößen unterscheiden. Im ersten Beispiel haben wir eine 35 mm Kleinbildkamera mit einer 50 mm Brennweite. Das entspricht einem Normalobjektiv. Das zweite Beispiel zeigt eine Kamera mit einem 7,4 mm Bildsensor. Aber nicht nur der Sensor ist deutlich kleiner, auch die Brennweite ist deutlich kürzer. Auch hierbei handelt es sich im Endeffekt um ein Normalobjektiv. Bei all den Unterschieden gibt es aber eine Gemeinsamkeit: Beide Kameras haben einen Bildwinkel von 40 Grad. Offensichtlich ist der Bildwinkel die Konstante im Spiel der unterschiedlichen Brennweiten und Sensorgrößen.\\
#+attr_html: :width 400px
[[./winkel.jpg]]

Die Berechnung des Bildwinkels erfolgt über das Verhältnis von Sensordurchmesser d zur doppelten Brennweite f. Aus diesem Verhältnis wird der Arkustangens gezogen. Dieser hat einen Wertebereich von -90 bis 90 Grad, nutzt also per Definition nur die Hälfte eines Kreises. Wir multiplizieren das Ergebnis daher mit 2: $$\alpha = 2 * \text{arctan}(\frac{d}{2*4})$$
Die Berechnung des Durchmessers d aus Breite und Höhe des Sensors haben wir bereits kennen gelernt: $$d=\sqrt{\text{Breite}^2 + \text{Hoehe}^2}$$
Bei einem klassischen Kleinbildfilm ist die Höhe 24 mm und die Breite 36 mm. Die Diagonale berechnet sich wie oben beschrieben und der Bildwinkel eines Objektivs von 45mm Brennweite wie folgt:\\
#+attr_html: :width 400px
[[./bw3.jpg]]
Der Bildwinkel beträgt also 51,35 Grad.

Doch was nutzen einem nun diese Winkelangaben? Abb. 6 zeigt, wie sich die Winkelangaben in das Zusammenspiel von Sensorgrößen einbetten. Die oberste Zeile gibt die Brennweiten 7, 8, 9, ..., 400, 600 mm an, die Objektive üblicherweise haben können. Sie finden diese Angaben in der Regel auf dem Objektiv. Unter den Brennweiten stehen die Bildwinkel, die durch die Brennweite (Spalte) und der Sensorgöße (Zeile) definiert werden. Eine analoge Kleinbildkamera hat bei einer Brennweite von 50 mm also einen Bildwinkel von 56,8 Grad und bei einer Brennweite von 200 mm einen Bildwinkel von 12,3 Grad. Der Bildwinkel und mit ihm der Bildausschnitt wird kleiner, wodurch Entferntes näher erscheint.

Der im Beispiel berechnete Winkel einer Kleinbildkamera mit 45mm Brennweite kann nun genutzt werden, um beispielsweise eine äquivalente Brennweite für die Digitalkamera Canon EOS 20D zu finden. Suchen Sie in der Reihe »EOS 20D« nach einer Winkelangabe, die den berechneten 51,35 Grad am nächsten kommt. In diesem Fall ist es die Angabe »51,6«. Wenn Sie jetzt in die Kopfzeile dieser Spalte schauen, können Sie die äquivalente Brennweite »28« ablesen. Die EOS 20D fotografiert also mit einem 28mm Objektiv den gleichen Bildausschnitt wie eine klassische Kleinbildkamera mit einem 45mm Objektiv.\\
#+attr_html: :width 400px
[[./eos.jpg]]
*** Blende
Die Blende (engl.: aperture), auch Irisblende oder Lamellenblende genannt, übernimmt die Aufgabe der Steuerung der Lichtmenge. Sie funktioniert analog zur Iris des menschlichen Auges. Eine kreisförmige Öffnung im Objektiv der Kamera kann je nach Bedarf vergrößert oder verkleinert werden und lässt entsprechend mehr oder weniger Licht durch. Bei Kameras funktioniert dies über speziell geformte Lamellen, die ineinander verschoben werden können (Abb. 1). Um eine Kreisform zu erreichen, sollte die Blende aus mindestens acht Lamellen bestehen. Im gestalterischen Hinblick kann mit ihr neben der Bildhelligkeit auch die Tiefenschärfe reguliert werden.\\
#+attr_html: :width 400px
[[./blende.jpg]]

Wozu benötigt die Kamera eine Blende? Bildsensoren sind nur bedingt flexibel, was die Menge des Lichts angeht, die sie verarbeiten können. Um ein optimales Bildergebnis zu erhalten, muss eine ganz bestimmte Menge Licht auf den Sensor fallen. Nun herrschen aber in Abhängigkeit von Tageszeit und Räumlichkeit völlig unterschiedliche Lichtsituationen. Gibt es zu viel Licht, wird der Sensor geblendet. Gibt es zu wenig Licht, erkennt er nichts. Die Analogie zum menschlichen Sehen ist offensichtlich.

Je nachdem, welche Lichtverhältnisse herrschen, müssen wir also die richtige Blendenöffnung einstellen. Abb. 2 zeigt, wie sich Blende und Licht auf dem Sensor bzw. Film bemerkbar machen. Bei gleichbleibenden Lichtverhältnissen werden drei Fotografien gemacht.

1. Die erste Fotografie wird mit vollständig geöffneter Blende gemacht. Es wird zu viel Licht aufgenommen und das Bild ist überbelichtet. Das Bild wird zu hell. Dass kann im Extremfall so weit gehen, dass das Bild komplett weiß wird.
2. Im zweiten Beispiel ist die Blendenöffnung ideal eingestellt. Es wird nicht alles vorhandene Licht, aber dennoch genug zum Sensor geleitet. Das Ergebnis ist ein ideal belichtetes Bild.
3. Im dritten Beispiel ist die Blende zu weit geschlossen. Nur sehr wenig Licht wird aufgenommen. Die Menge des Lichts reicht nicht aus, um ein optimales Ergebnis zu erzielen: Das Bild ist unterbelichtet und damit sehr dunkel.\\
#+attr_html: :width 400px
[[./blende.png]]

Die Blende wird durch die Blendenzahl (engl.: f-stop) definiert. Sie beschreibt das Verhältnis zwischen Brennweite und Blendenöffnung in festgelegten Werten (Bild unten). Dabei gilt: Je größer die Blendenzahl (16 rechts), desto kleiner ist die Blendenöffnung und desto weniger Licht fällt in das Objektiv (aber desto größer wird der Bereich der Schärfentiefe). Im Umkehrschluss bedeutet dies, dass bei kleinerer Blendenzahl (2.8 links) die Blendenöffnung größer ist und mehr Licht einfallen kann (aber der Bereich der Schärfentiefe abnimmt).\\
#+attr_html: :width 400px
[[./zsmhang.jpg]]

Da der Gegensatz von Blendenöffnung und Blendenzahl zur Verwirrung führen kann, hier eine kleine Eselsbrücke: Die Blendenzahl gleicht dem Lichtschutzfaktor einer Sonnencreme - Je höher die Zahl, desto weniger Licht gelangt in das Objektiv, wodurch der Sensor besser "geschützt" wird.

Die Blendenzahl K bezeichnet das Verhältnis der Brennweite f zum Durchmesser D der Blendenöffnung: $$\text{Blendenzahl} = \frac{\text{Brennweite}}{\text{Durchmesser}}$$ $$K= \frac{f}{D}$$

Bei einem 50 mm Objektiv mit einer Blendenöffnung von 50 mm ist das Öffnungsverhältnis 1:1. Die in der Kameratechnik eingesetzten Blendenzahlen sind normiert und bilden folgende Reihe: 1:1, 1:1.4, 1:2, 1:2.8, 1:4, 1:5.6, 1:8, 1:11, 1:16 etc.

Die Zahlen lassen sich leicht mit dieser Eselsbrücke memorieren: Sie merken sich die ersten beiden Zahlen 1 und 1,4. Die Reihe wird fortgesetzt, indem sie immer das Doppelte des Vor-Vorgängers einfügen. Der nächste Schritt ist also 2*1=2, danach 2*1,4=2,8, danach 2*2=4, danach 2*2,8=5,6, etc. Ab Blendenzahl 11 wird gerundet.

Die Schritte dieser Reihe sind dabei so festgelegt, dass sich von einem Schritt zum nächsten die Blende so weit öffnet bzw. schließt, dass sich das durch die Blende gelangende Licht verdoppelt bzw. halbiert. Eine Blendenöffnung von 1:1,4 lässt also doppelt soviel Licht durch wie eine Blendenöffnung 1:2. Bei einer Änderung der Blendenöffnung von 2 auf 2,8 halbiert sich die Lichtmenge.

Die Hauptaufgabe der Blende ist sicherlich die Steuerung der Lichtmenge. Zum Teil lässt sich diese aber auch durch die Belichtungszeit übernehmen. In der Praxis wird die Blende daher häufig verwendet, um einen besonderen gestalterischen Effekt zu erzielen: Schärfentiefe.

Wie wir beim Thema Fokussierung gesehen haben, erscheinen Objekte schnell unscharf, wenn ihr Abstand zur Kamera und die Einstellung des Entfernungskranzes nicht miteinander übereinstimmen. In vielen Situationen möchte man den Bereich, der scharf gezeigt werden soll, aber beeinflussen. Betrachten Sie beispielsweise den leeren Aufzugschacht in Abb. 3. Ziel der Aufnahme war es, den kompletten Schacht scharf zu stellen, was nach dem bisher besprochenen ja eigentlich nicht möglich sein sollte. In der Fotografie lassen sich somit zwei Effekte erzeugen: entweder soll eine möglichst große Tiefenschärfe erreicht werden (der scharf dargestellte Bereich dehnt sich stark in die Tiefe aus, sodass nahezu alle Objekte die gleiche Schärfe besitzen) oder der Hintergrund soll bewusst vom Vordergrundmotiv abgegrenzt werden (der scharf gestellte Bereich liegt nur auf dem Hauptmotiv während der Hintergrund verschwimmt).\\
#+attr_html: :width 400px
[[./schacht.jpg]]

In der Praxis müssen sie dazu folgende Einstellungen vornehmen: Für die erste Variante müssen Sie die Blende möglichst klein einstellen, also eine möglichst große Blendenzahl wählen. Denn je kleiner die Blendenöffnung, desto kleiner auch die Streuung des Lichts. Je kleiner die Streuung des Lichts, desto schärfer die Erscheinung der Abbildung. Für den zweiten Fall ist ein umgekehrtes Vorgehen erforderlich.
** Sensor
Um in der digitalen Fotografie bzw. Videografie Bilder aufzunehmen, werden die Bildsensoren CCD (Charged Coupled Device) oder CMOS (Complementary Symmetry Metal Oxide Semiconductor) verwendet. Beide Verfahren bedürfen spezieller Filter, um Farbe wahrnehmen zu können. Ausschlaggebend für die Auflösung des Bildes ist die Größe des Sensors.

Der Bildsensor einer digitalen Kamera besteht aus einem lichtempfindlichen Raster. Die einzelnen Elemente dieses Rasters können Helligkeiten messen. Als Bildsensoren werden in der Regel CCDs (Charged Coupled Device) verwendet. Mitunter finden sich auch Kameras mit der CMOS-Technologie (Complementary Symmetry Metal Oxide Semiconductor). Diese ist allerdings nur in sehr teuren Geräten qualitativ befriedigend.

Kameras verwenden das Additive Farbsystem, d.h. alle Farben zusammengemischt ergeben Weiß (Bild unten).
Rot + Grün = Gelb ;     Rot + Blau = Magenta  ;    Grün + Blau = Cyan

#+attr_html: :width 400px
[[./additiv.jpg]]

*Ein-chip-Bayer-CCD-Sensoren (Bayerfilter)*\\
Der CCD-Chip agiert als Bildwandler, der das durch die Optik wandernde und auf dem Rasterchip eintreffende Lichtimpulse in Ströme umwandelt. Jedes Rasterelement wird durch das einfallende Licht unterschiedlich stark aktiviert und liefert entsprechende Helligkeitswerte.

Sowohl CCD als auch CMOS sind "farbenblind", benötigen also einen Trick, um in Farbkameras eingesetzt zu werden. Dieser Trick besteht im sogenannten Bayer-Filter, ein Farbfilter, der bereits in den 1970er Jahren zum ersten Mal eingesetzt wurde.

Der Bayer-Filter setzt vor die einzelnen Rasterelemente des Sensors einen der drei Farbfilter Rot, Grün und Blau, so dass das jeweilige Sensorelement nur noch Lichtimpulse im Wellenlängenbereich des Farbfilters empfängt. Der Filter orientiert sich dabei am Aufbau des menschlichen Auges. Auch hier gibt es speziell auf diese drei Farben ausgerichtete Rezeptoren (Bild unten).\\
#+attr_html: :width 400px
[[./raster.jpg]]

Es mag zunächst erstaunen, dass jeder zweite Farbfilter grün, während die roten und blauen Filter nur halb so oft vorkommen. Diese Verteilung richtet sich ebenfalls nach dem Aufbau des menschlichen Auges, welches Grüntöne deutlich besser differenzieren kann als Rot oder Blau.

Dieses Verfahren ist allerdings auch mit Problemen behaftet. Wenn Sie die RAW-Daten des Bildes nutzen, dann verwenden Sie genau die Daten, welche die einzelnen Sensorpunkte produzieren, also eine Reihe von Helligkeitswerten für die grün-, rot-, und blaugefilterten Sensorelemente. Damit haben Sie allerdings noch zwei Probleme, die für eine geeignete Darstellung behoben werden müssen: Erstens sind die Farbwerte unabhängig voneinander in ihre Komponenten aufgeteilt. Sie benötigen also ein Verfahren, um aus den drei Farbkomponenten eine einzelne Farbe zu mischen. Zweitens sind die einzelnen Sensorelemente an unterschiedlichen Positionen. Sie müssen also aus mehreren Positionen eine neue gemeinsame Position ermitteln.

Die Behebung dieser beiden Probleme nennt man Demosaicing. An jedem Schnittpunkt, an dem sich vier Sensorelemente (2 grüne, 1 rotes, 1 blaues) treffen, wird der resultierende Farbwert als RGB ermittelt. Genau an dieser Position wird ein neues künstliches Pixel für das entstehende Bild geschaffen, welches den gemischten Farbwert zugewiesen bekommt.\\
Hier ist nur eine einfache Darstellung des Verfahrens angebracht. Zahlreiche Schwierigkeiten gilt es zu bewältigen:
- Aufgrund der Rasterung des Sensors entstehen Artefakte bei Kreisen und Diagonalen, die schlimmstenfalls als Moirémuster völlig verfälschende optische Eindrücke produzieren. Die Übertragung der Bilddaten vom Sensorraster zum Bildraster verschärft diese Problematik zusätzlich.
- Da für jeden Bildpixel mehrere Sensorelemente zusammengefasst werden, und mitunter ein Sensorelement zur Bestimmung von vier Bildpixeln herangezogen wird, ist das resultierende Bild von Unschärfen betroffen.
- Diese Probleme werden in der Praxis noch verschärft: Die Elemente des Sensors liegen nicht so dicht beieinander, wie hier vereinfachend dargestellt, da zwischen ihnen Raum für den Datentransport liegen muss.

*3fach CCD*\\
Bei drei Bildwandlern (3fach CCD) wird das Bild über ein dichroitisches Prisma in die drei Grundfarben Rot, Grün und Blau zerlegt und je ein Rasterchip ermittelt dabei die Helligkeit, wodurch der RGB-Wert bestimmt werden kann (Abb.). Zwar nimmt das Prisma einigen Platz in der Kamera ein,  dafür nutzen aber die drei Chips das eingefangene Licht optimal aus und liefern schon bei kleinen Sensordiagonalen qualitativ gute Farbergebnisses und ein gutes Signal-Rausch-Verhältnis .


*Sensorgrößen*\\
In der klassischen analogen Fotografie gilt der 35mm-Film (Kleinbild) als Standard. Spiegelreflexkameras nutzen ausschließlich dieses Format. In der Digitalfotografie hat sich keine einheitliche Sensorgröße durchgesetzt. Hier ist die Entwicklung noch in vollem Gang. Die Sensorgröße ist zunächst einmal ausschlaggebend dafür, wieviele Bildpunkte aufgenommen werden können. Daneben hat die Sensorgröße aber auch direkte Auswirkungen auf die Optik. Während die Brennweiten in analogen Kameras miteinander leicht zu vergleichen sind, ist das bei digitalen Kameras nur bei Kenntnis der Sensorgröße möglich. Die Auflistung in untigem Bild ist bei weitem nicht erschöpfend.\\
#+attr_html: :width 400px
[[./sens.gif]]

Die Sensorgröße hängt nur bedingt mit der Auflösung zusammen. So können auch kleinere Sensorgrößen Auflösungen größerer erreichen. Sensoren der Größe 1/2,7'' erreichen heute eine Auflösung von 2-8 Megapixel. Die bereits deutlich größeren 1/1,7'' Sensoren erreichen Auflösungen von 3-12 Megapixel. Im Vollformat KB (für Kleinbild, 35mm) werden sogar über 20 Megapixel möglich.

*Verschlusszeiten*\\
Wie auch in der analogen Fotografie wird die Belichtung der Bildsensoren in Spiegelreflexkameras mechanisch gesteuert. Licht wird nur für einen sehr kurzen Zeitraum auf den Sensor geworfen (Abb. 4). Im linken Bild ist der Normalzustand gezeigt. Licht tritt durch das Objektiv ein und wird durch einen Spiegel nach oben reflektiert (daher der Name Spiegelreflex). Über ein Prisma wird das Licht in den Sucher weitergeleitet. Das Prisma ist notwendig, damit das resultierende Sucherbild nicht spiegelverkehrt dargestellt wird. Im rechten Bild ist der Zustand der Kamera während der Aufnahme zu sehen: Der Spiegel klappt hoch und ein hinter dem Spiegel angebrachter Verschluss (hier nicht eingezeichnet) öffnet sich und lässt das Licht auf den Sensor fallen.\\
#+attr_html: :width 400px
[[./aufbau.jpg]]

Dieser Vorgang dauert in der Regel nur den Bruchteil einer Sekunde. Anschließend klappt der Verschluss wieder zu und der Spiegel herunter. Im Moment der Aufnahme kann durch den Sucher kein Bild gesehen werden. Daher erscheint übrigens bei den meisten Spiegelreflexkameras auf dem Display auch kein Sucherbild: Der Sensor wird im Normalzustand gar nicht belichtet und liefert daher kein Bild.

Die Wahl der Verschlusszeit hängt von Licht ab, welches zur Verfügung steht und vom Motiv, das aufgenommen wird. Sportaufnahmen beispielsweise werden üblicherweise mit einer sehr kurzen Verschlusszeit jenseits der 1000stel Sekunde aufgenommen, da ansonsten die Bewegung der Sportler zu Verwischungen im Motiv führt. Für Aufnahmen ohne Stativ gilt die 30stel Sekunde als Grenzwert. Längere Verschlusszeiten lassen das gesamte Bild verwackeln, da die Hand des Fotografen beim Auslösen nicht ruhig genug ist. In der Nacht und mit Stativ können Verschlusszeiten schon einmal mehrere Sekunden oder gar Minuten dauern, um schwach ausgeleuchtete Szenerien zu fotografieren. Die Verschlusszeit steht in engem Wechselspiel zur Blende. Diese ist üblicherweise im Objektiv eingebaut und wird im Folgenden besprochen.
** Funktionen
Verschiedene Funktionen in der Kamera ermöglichen uns eine bessere Kontrolle über die Lichtverhältnisse vor Ort. Diese unterscheiden sich von Kamera zu Kamera. Im Folgenden wird daher nur auf die grundlegendsten Funktionen eingegangen, die standardmäßig in den semiprofessionellen bzw. professionellen Kameras integriert sind.

Der Zebramustergenerator (engl.: zebra pattern), oder auch kurz Zebra genannt, bezeichnet eine Pegelanzeige in Suchermonitoren von Kameras und dient als Hilfsfunktion zu Ermittlung der korrekten Belichtung. Sie kann sowohl manuell aktiviert als auch deaktiviert werden. Werden Teile eines Bildes zu stark belichtet, entstehen sogenannte Hot Spots: zu helle Bildbereiche werden "ausgebrannt" und sind nur noch als weißer Fleck sichtbar, da durch die Überbelichtung jegliche Unterschiede im Helligkeitsumfang verschwinden. Dem Kameramann werden derartig gefährdete Bereiche durch ein überlagertes Linienmuster angezeigt (Abb. 1). Auch wenn sie im Sucher bzw. Display sichtbar ist, wird die Zebrafunktion nicht aufgezeichnet, d.h. auf den Aufnahmen erscheint kein Linienmuster.\\

#+attr_html: :width 400px
[[./zebra.jpg]]

Von Kamera zu Kamera unterscheidet sich die Anzahl der werkseitig voreingestellten Werte und deren mögliche Veränderungen. Zumeist stehen dem Kameramann 100 oder 70 Prozent zur Verfügung. Hier sollte man den niedrigeren Wert wählen und beispielsweise bei einem Dreh mit Personen die hellste Stelle im Gesicht begutachten und anhand dieser Referenz die Aufnahme belichten. Übrigens liegt der menschliche Hautton i.d.R. bei ca. 53 Prozent. Haben Sie die Möglichkeit beispielsweise zwei unterschiedliche Werte an der Kamera zu definieren, wählen Sie 53 Prozent für den ersten und 70 Prozent für den zweiten Zebrawert. Damit sollten alle "Problemzonen" gut abgedeckt sein

*Gain*\\
Bei der Gain-Funktion handelt es sich um einen Schalter an der Kamera, der zumeist in mindestens drei Stufen (L, M, H)eine Anhebung des Videopegels angegeben in Dezibel vornimmt. Da eine Kamera trotz vollständig geöffneter Blende nicht wie unser Auge in der Lage ist, sich an die Dunkelheit zu gewöhnen und zu "sehen", kann die Lichtempfindlichkeit der Kamera verstärkt werden, um beispielsweise an lichtschwachen Orten trotzdem Aufnahmen machen zu können. Übrigens ist die Lichtempfindlichkeit ein Kriterium, dass die guten von den weniger guten Kameras unterscheidet.\\
#+attr_html: :width 400px
[[./pegel.png]]

Die voreingestellten Werte schwanken von Kamera zu Kamera wie auch die Möglichkeit, die Werte nach den eigenen Bedürfnissen anzupassen. Aber Vorsicht! Durch die künstliche Verstärkung des Videopegels wird zeitgleich auch das Bildrauschen verstärkt. Setzen Sie die Gain-Funktion daher nur in Ausnahmefällen ein.

*ND-Filter*\\
Der Neutraldichtefilter (engl.: neutral density) ist ein zumeist im Kameraobjektiv eingebauter Graufilter, der aktiviert werden kann, wenn das Bild trotz maximal geschlossener Blende durch eine zu helle Aufnahmeumgebung überbelichtet wird. Der Filter nimmt die Funktion einer Sonnenbrille an einem Sommertag mit strahlender Sonne ein und reduziert die eintreffende Lichtmenge. Meist kann der Kameramann zwischen zwei Stufen wählen, die je nach Kamera manuell verändert werden können. Wie auch die Gain-Funktion sollten Sie den Neutraldichtefilter jedoch sparsam einsetzen und auch nur, wenn unbedingt nötig!\\
#+attr_html: :width 400px
[[./nd.png]]

*Weißabgleich*\\
Der Weißabgleich (engl. White Balance) gehört zu den wichtigsten Funtkionen einer Kamera, da er die Aufgabe übernimmt, die unsere Augen automatisch steuern: Die Farben richtig darstellen (Chromatische Adaption). Gehen wir von einem Raum mit Kunstlicht ins Freie, gleichen unsere Augen die Farbverschiebungen aus und Weiß bleibt für uns Weiß. Die Kamera ist dazu nicht in der Lage und benötigt einen Referenzwert, um die Farben richtig darstellen zu können. Wie in Kapitel 2 dargestellt, kann Licht unterschiedliche Farbtemperaturen haben. Der Weißabgleich greift diese Kelvinwerte auf und korrigiert die RGB-Werte (Additives System - alle Farben ergeben Weiß), sodass sich das aufgezeichnete Material der Wirklichkeit annähert.\\
#+attr_html: :width 400px
[[./temp.png]]

Als Kameramann hat man zwei Möglichkeiten den Weißabgleich anzupassen: Beim automatischen Anpassen nimmt die Kamera ein kontinuierliches Abgleichen durch die Bildung eines Farbmittelwertes während der Aufnahme vor. Dabei orientiert sie sich an den jeweils hellsten Bereichen im Bild, die jedoch nicht zwangsläufig das gesuchte Weiß sein müssen, weshalb die automatische Anpassung des Weißabgleichs in den seltensten Fällen die tatsächliche Lichtstimmung trifft. Daher empfiehlt es sich, die Werte manuell zu steuern. Das geschieht i.d.R. mit einer Weiß- bzw. Graukarte oder einem weißen Blatt Papier als Referenz: Dabei hält man die Graukarte in die Kamera, die möglichst den gesamten Bildbereich ausfüllen sollte. Der Kameramann betätigt den Schalter White Balance, wodurch die Kamera den Weißwert ermittelt und abspeichert. Die meisten Kameras bieten zwei Speicherplätze und einen voreingestellten Kunst- oder Tageslicht-Wert. Wichtig ist, dass bei jedem Wechsel des Drehorts ein neuer Weißabgleich vorgenommen wird. Das gilt beispielsweise auch schon bei einem Wechsel vom Sonnenlicht in den Schatten!

https://youtu.be/v-fdW8kzxvU

Zwar findet man nahezu immer und überall ein weißes Blatt Papier für den Weißabgleich, jedoch sind in den meisten Papieren Aufheller enthalten, die durch UV-Licht in der Kamera blau erscheinen können. Benutzen Sie daher möglichst die Graukarte, um farbstichige Aufnahmen zu umgehen. Diese können in der Technikausleihe der Professur Medieninformatik ausgeliehen werden.

** Selbsttest
1. Bayer-Filter Lückentext
   - Sowohl CCD als auch CMOS sind *farbenbild* , benötigen also einen Trick, um in Farbkameras eingesetzt zu werden. Dieser Trick besteht im sogenannten   *Bayer-Filter*  , ein Farbfilter, der bereits in den 1970er Jahren zum ersten Mal eingesetzt wurde. Dierser setzt vor die einzelnen Rasterelemente des Sensors einen der drei Farbfilter   Rot  , Grün und   Blau  , sodass das jeweilige Sensorelement nur noch Lichtimpulse im Wellenlängenbereich des Farbfilters empfängt. Der Filter orientiert sich dabei am Aufbau des menschlichen Auges. Auch hier gibt es speziell auf diese drei Farben ausgerichtete *Rezeptoren*. Dabei kommt *Grün*  doppelt so oft vor wie Rot und Blau.
2. Neutraldichtefilter Lückentext
   - Der Neutraldichtefilter (engl.: *neutral density*) ist ein zumeist im Kameraobjektiv eingebauter *Graufilter* der aktiviert werden kann, wenn das Bild trotz maximal geschlossener *Blende* durch eine zu helle Aufnahmeumgebung überbelichtet wird. Der Filter nimmt die Funktion einer   *Sonnenbrille*  an einem Sommertag mit strahlender Sonne ein und reduziert die eintreffende Lichtmenge. Meist kann der Kameramann zwischen zwei Stufen wählen, die je nach Kamera manuell verändert werden können. Wie auch die   *Gain*  -Funktion sollten Sie den Neutraldichtefilter jedoch sparsam einsetzen und auch nur, wenn unbedingt nötig!
3. Wer erfand die Kathodenstrahlröhre?
   - Karl Ferdinand Braun
4. Was ist eine Blende und welche Funktion hat sie?
   - @@html:<font color = "red">@@X eine Aufbewahrungsbox, um die Kamera vor Sonnenlicht zu schützen@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark reguliert Lichteinfall@@html:</font>@@
   - @@html:<font color = "red">@@X reguliert die Datenmenge@@html:</font>@@
   - @@html:<font color = "red">@@X umgangssprachlich für Lichtschalter einer Kamera@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark nimmt Einfluss auf die Schärfentiefe@@html:</font>@@
   - @@html:<font color = "red">@@X Die Blende entscheidet über das Aufnahmeformat.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark besteht aus mehreren Lamellen@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark kreisförmige, verstellbare Öffnung im Objektiv@@html:</font>@@
   - @@html:<font color = "red">@@X überlagert zu dunkle Bildbereiche mit einem Sternchenmuster@@html:</font>@@
5. Was ist Phantomspannung?
   - @@html:<font color = "red">@@X Röntgenstrahlung bei alten Röhren-Fernsehgeräten@@html:</font>@@
   - @@html:<font color = "red">@@X Geistererscheinung auf dem LCD der Kamera@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark notwendige Betriebsspannung eines Kondensatormikrofons@@html:</font>@@
   - @@html:<font color = "red">@@X unbeabsichtigte elektrische Spannung bei Kameras@@html:</font>@@
   - @@html:<font color = "red">@@X Geräusche, die während einer Aufnahme entstanden sind, aber real nicht vorhanden waren@@html:</font>@@
   - @@html:<font color = "red">@@X Erzählstil in amerikanischen Comics@@html:</font>@@
   - @@html:<font color = "red">@@X Werbetitel für das "Phantom der Oper"@@html:</font>@@
   - @@html:<font color = "red">@@X elektrische Spannung, mit der Phantome aufgespürt werden können@@html:</font>@@
6. Aus welchen drei Grundbestandteilen ist eine moderne Kamera aufgebaut?
   - Bildwandler, Kameraelektronik, Objektiv
7. Blendenzahl Lückentext
   - Je  *größer* die Blendenzahl, desto   *kleiner*  die Blendenöffnung und desto   *weniger*  Licht fällt in das Objekt und desto *größer* wird der Bereich der Schärfentiefe.
8. Was ist der Brennpunkt?
   - @@html:<font color = "red">@@X Abstand zwischen Linse und Netzhaut@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark steht im Zusammenhang mit der Brennweite@@html:</font>@@
   - @@html:<font color = "red">@@X reguliert Lichteinfall im Objektiv@@html:</font>@@
   - @@html:<font color = "red">@@X Bildaufbauverfahren@@html:</font>@@
   - @@html:<font color = "red">@@X Sendung im ZDF, die aktuelle Nachrichten und Hintergründe zum Zeitgeschehen liefert@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Stelle, an der die Lichtstrahlen auf einen Punkt auf der Sensorebene gebündelt werden@@html:</font>@@
   - @@html:<font color = "red">@@X Entfernung zwischen Linsenmittelpunkt und Brennpunkt in mm@@html:</font>@@
   - @@html:<font color = "red">@@X Abstand zwischen Record-Taste einer Kamera und dem Kassetten-Laufwerk@@html:</font>@@
   - @@html:<font color = "red">@@X Sammelstelle bei Feueralarm@@html:</font>@@
   - @@html:<font color = "red">@@X Stelle, an der das Filmmaterial in Brand gerät@@html:</font>@@
9. Was ist der Bayer-Filter und wie ist er aufgebaut?
   - @@html:<font color = "red">@@X ist aus 1xRot und 1xBlau aufgebaut@@html:</font>@@
   - @@html:<font color = "red">@@X nicht für alpine Motive geeignet@@html:</font>@@
   - @@html:<font color = "red">@@X filtert Bayerische Landsleute aus dem Bild heraus@@html:</font>@@
   - @@html:<font color = "red">@@X ist aus 2xRot, 3xGrün und 2xBlau aufgebaut@@html:</font>@@
   - @@html:<font color = "red">@@X ist aus 2xWeiß und 1xBlau aufgebaut@@html:</font>@@
   - @@html:<font color = "red">@@X ist ein Blau-Weiß-Filter@@html:</font>@@
   - @@html:<font color = "red">@@X ist abwechselnd aus Blau und Weiß aufgebaut (Bayerische Flagge)@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark ist aus 1xRot, 2xGrün und 1xBlau aufgebaut@@html:</font>@@
   - @@html:<font color = "red">@@X filtert störende blau-weiße Strukturen heraus@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark ist ein Farbfilter@@html:</font>@@
10. Geschichte Kameratechnik Lückentext
    - *Paul   Nipkow*  patentierte 1884 sein Konzept zur Bildzerlegung und Informationsreduktion, die *Nipkow-Scheibe*  . Eine drehbare Scheibe wurde mit   *Löchern*  in spiralförmiger Anordnung versehen, deren Anzahl der  *Zeilen*  anzahl des zerlegten Bildes entsprach. Mit einer Photozelle, die hinter der Scheibe montiert wurde, konnten Helligkeitsinformationen in elektrische   *Signale*  umgesetzt und an einen Empfänger mit einer reaktionsschnellen Lichtquelle weitergeleitet werden. Durch eine weitere (  *synchron*  bewegte) Scheibe konnte das zuvor abgetastete Bild wiedergegeben werden.
11. Was macht ein Zebramustergenerator?
    - @@html:<font color = "red">@@X Belichtungswert wird in Promille angegeben@@html:</font>@@
    - @@html:<font color = "green">@@\checkmark Muster wird nicht aufgezeichnet, aber angezeigt@@html:</font>@@
    - @@html:<font color = "green">@@\checkmark überlagert überbelichtete Bereiche mit einem Zebramuster@@html:</font>@@
    - @@html:<font color = "red">@@X Belichtungswert wird in cm angegeben@@html:</font>@@
    - @@html:<font color = "red">@@X Muster wird nicht angezeigt, aber aufgezeichnet@@html:</font>@@
    - @@html:<font color = "red">@@X überlagert unterbelichtete Bereiche mit einem Zebramuster@@html:</font>@@
    - @@html:<font color = "green">@@\checkmark Belichtungswert wird in Prozent angegeben@@html:</font>@@
    - @@html:<font color = "red">@@X überlagert unterbelichtete Bereiche mit einem Karomuster@@html:</font>@@
    - @@html:<font color = "red">@@X überlagert optimal belichtete Bereiche mit einem Linienmuster@@html:</font>@@
12. Was sind typische Blendenzahlen und wie berechnen sie sich?
   - @@html:<font color = "green">@@\checkmark 8@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark 2.8@@html:</font>@@
   - @@html:<font color = "red">@@X 25@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark 32@@html:</font>@@
   - @@html:<font color = "red">@@X a^2+b^2@@html:</font>@@
   - @@html:<font color = "red">@@X 1.7724532@@html:</font>@@
   - @@html:<font color = "red">@@X 1920@@html:</font>@@
   - @@html:<font color = "red">@@X 0.1@@html:</font>@@
   - @@html:<font color = "red">@@X Farbtemperatur:Farbtiefe@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Brennweite:Durchmesser@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark 5.6@@html:</font>@@
   - @@html:<font color = "red">@@X Länge der Kamera:Diagonale des Objektivs@@html:</font>@@






   - @@html:<font color = "green">@@\checkmark D@@html:</font>@@
   - @@html:<font color = "red">@@X D@@html:</font>@@

* Kapitel 5: Audio
Die Bezeichnung Audio lässt sich aus dem i-konjugierten lateinischen Wort audire ableiten und bedeutet soviel wie "ich höre". Audio umfasst im weitesten Sinne die Phänomenlogie aller, d.h. sowohl bewusst erzeugter (z. B. Musik, Alarmsignale etc.) als auch auf natürlichem Weg (z. B. Vogelgezwitscher, Bachrauschen etc.) oder ungewollt (z. B. Straßenlärm) entstandener Sinnesreize der menschlichen Umwelt, die primär mit Hilfe des Gehörs wahrgenommen wird.

Das Hören ist eines unserer wichtigsten Sinne, denn es ermöglicht es uns nicht nur, Geräusche wahrzunehmen und diese räumlich zuzuordnen, sondern im Ohr verbirgt sich auch unser Gleichgewichtssinn. Das Ohr ist ein Achtungsorgan, das immer eingeschaltet ist, sogar nachts. Weghören ist also wesentlich schwieriger als wegsehen.

Musik eignet sich am Besten, um Emotionen mitzuteilen: Tanzmusik, Musik zu spirituellen Handlungen, Trinklieder, Schlachtgesänge etc. Die meisten Filme wären leblos, wenn Musik und Geräusche nicht die zum Bild passende Stimmung vermitteln würden. Auditive Sinnesreize werden zuerst im Stamm- und Zwischenhirn (also den für die Körpergrundfunktionen, Gefühle etc. zuständigen Hirnteilen) verarbeitet und dann erst ins Großhirn für die rationale Auswertung weitergeleitet. Aus diesem Grund vermag Musik auch unmittelbare Körperreaktionen wie z. B. Änderungen von Pulsfrequenz oder Bluthochdruck hervorzurufen.

Eine Vielzahl von menschlichen Handlungen, natürlichen Vorgängen und Bewegungen bewirkt stets auch charakteritische akutsische Ereignisse. Die Handlung als Ursache und das akustische Ereignis als deren Wirkung sind dabei fest miteinander verbunden.

Dem Faktor Zeit kommt hierbei eine besondere Bedeutung zu da hierin ein wesentlicher Unterschied zur visuellen Wahrnehmung liegt. Akustische Ereignisse sind grundsätzlich flüchtig und nicht dauerhaft, sie benötigen die zeitliche Veränderung, um überhaupt existieren zu können. Das Ohr nimmt zusätzliche Eindrücke aus dem gesamten umgebenden Raum wahr. Räumliche Wahrnehmung geschieht daher zu einem großen Teil über das Hören.

Das folgende Kapitel vermittelt grundlegendes Wissen zum Thema Akustik und Audioverarbeitung. Es führt in die physikalischen Grundlagen von Schallereignissen ein und behandelt die Verarbeitung mit Hilfe des Werkzeugs Adobe Soundbooth. Soundbooth ist der kleine Bruder von Adobe Audition und eignet sich für die unkomplizierte Bearbeitung von Audioströmen für Video- und Animationsdaten.
** Aufbau Ohr
Das menschliche Gehör besteht aus der Ohrmuschel (Pinna), welche das Außenohr darstellt, dem Mittelohr, bestehend aus Hammer, Amboß und Steigbügel, der am ovalen Fenster ansetzt, und dem Innenohr, das Schnecke (Cochlea) und Hörnerv zusammenfasst.\\
Das Außenohr besteht aus der Ohrmuschel (Pinna), welche eine charakteristische Außenohrübertragungsfunktion erzeugt, die zur Richtungswahrnehmung verwendet wird. Der Gehörgang leitet wie ein einseitig geschlossenes Rohr (Länge: 2 cm) Frequenzen zwischen drei und fünf kHz besonders gut zum Trommelfell. Der Schalldruck bringt das Trommelfell stark zum Schwingen. Die Gehörknöchelchen Hammer, Ambos und Steigbügel leiten im Mittelohr die Schallwelle zum ovalen Fenster. Die Gehörknöchelchen bewirken wie eine hydraulische Presse eine Anpassung des geringen Schallwellenwiderstandes der Luft an den hohen Wellenwiderstand der Flüssigkeit.

Die Umwandlung der Schallwellen in Nervenimpulse erfolgt im Innenohr. Die Cochlea (Schnecke) beinhaltet in zweieinhalb Windungen das eigentliche Sinnesorgan und besteht aus drei parallelen Kanälen (Skalen). Der Steigbügel lenkt das ovale Fenster aus, das die Bewegungsenergie auf die Flüssigkeit der Scala vestibuli (Vorhoftreppe) überträgt, die die Scala media anregt und dadurch schließlich die Scala tympani (Paukentreppe) bewegt, die sich über das runde Fenster als Druckausgleich verformt, da die Lymphflüssigkeit inkompressibel ist. Scala vestibuli und scala tympani sind über das Helicotrema am Ende der Schnecke miteinander verbunden.\\
#+attr_html: :width 400px
[[./aufbau-ohr.jpg]]

Auf der Basilarmembran (Länge: 32 mm) sitzt das Cortische Organ, in dem die Schallschwingung über 3.000 aneinandergereihte innere und 12.000 in drei Reihen angeordnete äußere Haarzellen in Nervenimpulse umgewandelt wird. Beim Schwingen der Basilarmembran entstehen Scherkräfte (Reiz) auf die Haarzellen, die in Aktionspotenziale umgewandelt werden und durch 30.000 Nervenfasern zur Auswertung ins Gehirn weitergeleitet werden. Hohe Frequenzen erregen die Basilarmembran nahe des ovalen Fensters, tiefe Frequenzen nah am Heliocotrema. Dadurch entsteht eine Frequenz-Orts-Transformation. Daher ist die Tonhöhenempfindung eine Positionsempfindung. Die Sinneszellen auf der Basilarmembran sind nur für die ihnen aufgrund ihrer Position zugeordneten Frequenzen empfindlich. Eine Schallschwingung wandert auf der Basilarmembran entlang und verursacht nur bei einer bestimmten Position einen Maximalausschlag der Basilarmembran (Wanderwelle).

Der wahrnehmbare Frequenzbereich von 20 bis 20.000 Hz entspricht einer Wellenlänge von 17 bis 0,017 m in Luft. Über die Länge von 32 mm kann das menschliche Gehör 640 Frequenzstufen wahrnehmen. Damit kann eine Verschiebung des relativ breiten Schwingungsmaximums auf der Basilarmembran von nur 50 Mikrometern (entspricht der Breite eines Haares) als Tonhöhenveränderung wahrgenommen werden. Bei 0 dB hören wir nichts, Geräusche bis 30 dB empfinden als ruhig, dauernder Lärm über 85 dB macht schwerhörig und bei 130 dB ist die Schmerzgrenze erreicht. 1 dB Pegeländerung ist gerade noch hörbar. Bei mittleren Frequenzen und Pegeln ergibt ein Pegelunterschied von 10 dB eine Verdoppelung oder Halbierung des Lautstärkeeindrucks.

Die äußeren Haarzellen besitzen motorische Eigenschaften und verstärken schwache bzw. dämpfen starke Auslenkungen, indem sie von efferenten Nervenfasern entsprechend gesteuert werden. Ihre Längenänderung kann mit Frequenzen bis 100 kHz durchgeführt werden. Durch ihre Verbindung mit der Deckmembran können sie diese entsprechend beeinflussen, sodass neben der passiven auch eine aktive Basilarmembranauslenkung stattfindet. Der Ort der maximalen Längenänderung hängt wieder von der anregenden Frequenz ab. Die Deckmembran erhält dadurch eine wippförmige Bewegung um die Pfeilerzellen als Drehpunkt. Die inneren Haarzellen hingegen werden durch die membrana tectoria rein passiv angeregt und geben ihr Signal in Form von Nervenimpulsen über die afferenten Nervenfasern ins Gehirn weiter.

Das räumliche Hören kann auf unser räumliches Sehen adaptiert werden: Linkes und rechtes Ohr arbeiten als separate Systeme, deren Signalanalysen erst im Gehirn gemeinsam ausgewertet werden. Durch das binaurale Hören ist eine genaue Lokalisation der Schallquelle möglich und somit kann räumliches Hören entstehen. Im Kopfbezogenen Koordinatensystem werden die drei Hörebenen visualisiert (siehe Bild).\\
#+attr_html: :width 400px
[[./koord.jpg]]

Wer sein Sinnesorgan nicht ausreichend schützt, muss mit irreversiblen Schäden rechnen (Abb. unten). Bei einer Dauerbelastung mit Pegeln über 85 Dezibel oder einer Kurzzeitbelastung mit 130 Dezibel folgt ein Hörverlust i.d.R. bei Frequenzen zwischen 3 und 6 kHz - es kommt zu Tinnitus, Taubheitsgefühl, TTS oder PTS. Zerstörte Haarzellen richten sich i.d.R. nicht wieder auf und es tritt der Verlust der Hörbarkeit der entsprechenden Frequenzen auf. Da die äußeren Haarzellen meist zuerst geschädigt werden und bei leisen Signalen verstärkend und bei lauten dämpfend wirken, ergibt sich bei Hörschädigungen im Innenohr im betroffenen Frequenzbereich ein Dynamikverlust. Durch Wegfall der Verstärkung ist die Hörschwelle des geschädigten Ohres höher und Signale unter dem Hörschwellenpegel werden nicht wahrgenommen, Signale darüber werden leise gehört. Mit zunehmendem Pegel steigt deren Lautheit rasch an, da die dämpfende Wirkung der äußeren Haarzellen fehlt, sodass das im Pegel zunehmende Signal sehr schnell als zu laut empfunden wird und die Unbehaglichkeitsschwelle tiefer liegt als beim normalen Gehör. Diesen schnellen Lautheitsanstieg bezeichnet man als Recruitment oder Lautheitsausgleich.

Man kann sich verschiedener Maßnahmen bedienen, um seine Ohren an einem lauten Standort zu schützen: Watte vermindert die Lautstärke um 10 bis 25 dB, Ohrenstöpsel sogar bis 35 dB und ein Helm reduziert die Lautstärke bis um zu 50 dB.\\
#+attr_html: :width 400px
[[./stoerung.jpg]]

Schauen Sie sich bitte abschließend folgendes Video an, in dem in einer sehr einfachen Form der Hörvorgang beschrieben wird https://youtu.be/0Mbu68pQIx4.

** Grundbegriffe
Die Akustik beschäftigt sich mit der Wissenschaft vom Schall. Schall sind mechanische Schwingungen und Wellen eines elastischen Mediums. Die Akustik wendet die allgemeine Wellenlehre auf mechanische Longitudinalwellen im Hörbereich an. Schall ist an Materie gebunden, im Vakuum gibt es keinen Schall. Wichtige Eigenschaften akustischer Ereignisse sind Zeit und räumliche Richtung. Im Gegensatz zu Bildern ist bei Luftschall der dreidimensionale Ortsbereich und das Zeitverhalten zu berücksichtigen.

Die grundlegenden Eigenschaften von Wellen sind durch Größe und Häufigkeit ihres Ausschlags definiert. Bei Schallwellen wird die Größe des Ausschlags Amplitude genannt. Sie ist verantwortlich für die empfundene Lautstärke. Die Häufigkeit des Ausschlags wird Frequenz genannt und ist für die Tonhöhe verantwortlich.

Um die Entstehung eines Hörvorgang noch besser verstehen zu können, ist es notwendig, einen genaueren Blick in die Psychoakustik werfen. Um deren Phänomene genauer definieren zu können, müssen zunächst drei zentrale Begriffe und ihr Zusammenspiel erläutert werden: Reiz (stimulus) - Empfindung (sensation) - Wahrnehmung (perception).

*Reiz*\\
Mit Reiz bezeichnen wir eine physikalische Erregung eines unserer Sinnesorgane. Diese Erregung ist spezifisch auf die einzelnen Sinnesorgane ausgerichtet. So reagiert das Auge auf Lichtreize und das Ohr auf Schall. Die Haut wiederum reagiert auf Berührung mit anderen Körpern. In jedem Fall muss der Reiz eine gewisse Stärke haben, um eine Reaktion auszulösen. Man spricht hier von der Reizschwelle (stimulus threshold), die überschritten werden muss.

Die Empfindung ist der Prozess, der durch den Reiz ausgelöst wird. Die Sensoren des Körpers wandeln den Reiz in elektrische Signale um, die über die Nerven an das Gehirn weitergeleitet werden. Am Ende steht die Wahrnehmung, d.h. das bewusste Erkennen der realen Umwelt. Wahrnehmung ist immer auch bereits Interpretation des Signals.

Stellen Sie sich einmal vor, Sie betrachten eine grüne Wiese in der Mittagssonne im Sommer. Es wird hier ein starker Reiz durch eine bestimmte Wellenlänge des Lichtes ausgeübt. Um Mitternacht wäre dieser Reiz nicht ganz verschwunden, aber so schwach, dass er Ihr Auge nicht mehr stimulieren kann: Sie sehen schwarz wo eigentlich eine grüne Wiese ist. Mittags aber empfängt das Auge einen starken Reiz und wandelt ihn in ein klares Signal um.

In der Mittagssonne ist der Reiz sehr stark und ähnlich stark ist auch das Empfinden des Reizes: Das Auge wird geblendet und schmerzt. Um der Blendung zu entgehen schließt sich die Pupille des Auges und lässt weniger Licht durch. Der Reiz und seine Empfindung wird abgemildert. Bis hierher reicht die Empfindung aus. Erreichen die Signale das Gehirn beginnt der interpretatorische Teil: Die Lichtwellen, die das Auge erreichen, werden analysiert und einer Farbe zugeordnet. Die Farbe gilt als Wahrnehmung. Mehr als Interpretation ist sie auch nicht, denn in der Natur existieren keine Farben, sondern nur unterschiedliche Wellenlängen.

*Reizschwelle und Schmerzgrenze*\\
Wie Sie bereits wissen, funktioniert es in ähnlicher Weise nicht nur beim Sehen, sondern auch beim Hören. Auch hier existiert eine Reizschwelle, die überschritten werden muss, bevor das Ohr die Signale empfindet. Und auch hier existiert eine Schwelle, jenseits der Schmerzen empfunden werden (Abb. unten). Die grüne Linie gibt die Ruhehörschwelle an. Jedes Schallereignis, dass leiser ist als diese Schwelle, wird vom menschlichen Gehör nicht registriert. Interessant an der Linie ist ihre Formgebung: offensichtlich ist die Wahrnehmung im mittleren Frequenzbereich von 0.5 bis 5 kHz besonders gut. Die Erklärung dafür finden Sie im blauen Bereich in der Mitte der Abbildung. Besonders gut wird in dem Frequenzbereich gehört, in dem auch die menschliche Sprache liegt. Die Evolution lässt grüßen.

Im Bereich von 120 bis 130 dB befindet sich die Schmerzgrenze. Hier werden die Rezeptoren des Ohres so stark beansprucht, dass sie uns schmerzen und Gefahr laufen, dauerhaft geschädigt zu werden.\\
#+attr_html: :width 400px
[[./schwelle.jpg]]

Durch den besprochenen Aufbau des Ohres wissen Sie welche Frequenzen und ihre Lautstärke vom menschlichen Hörsinn besser und welche schlechter bis gar nicht wahrgenommen werden. Mit diesem Wissen können Sie nun bei der Codierung darauf achten, dass die gut wahrnehmbare Frequenzen genauer codiert werden als die übrigen.

Diesen Umstand machen sich auch die Hersteller von Stereoanlagen zunutze. Sehr tiefe Bassfrequenzen beispielsweise werden vom Menschen sehr schlecht gehört. Sie sind vom menschlichen Gehör nicht zu lokalisieren. Weshalb auch aufwändige Surroundsysteme für die Bässe einen einzigen Subwoofer verwenden, der in irgendeiner Ecke des Raums untergebracht werden kann.


*Maskierung*\\
Aber selbst in den gut hörbaren Bereichen gibt es häufig Frequenzen, die wir bei der Codierung vernachlässigen können. Ein Beispiel dafür bildet das Phänomen der Maskierung (Abb. unten). Entlang der X-Achse sind die zu einem bestimmten Zeitpunkt abgetasteten Frequenzen gezeigt. Entlang der Z-Achse sind die Werte weiterer Abtastzeitpunkte angezeigt. Die senkrechten durchgezogenen und gepunkteten Linien zeigen die Amplitude, also die Lautstärke der jeweiligen Frequenz an. Das Phänomen der Maskierung ist anhand der orangen Linien veranschaulicht: Sie spannen ein Art akustisches Zeltdach auf. Alle Frequenzen, die es nicht schaffen, dieses Dach zu durchbrechen, werden nicht wahrgenommen.

Sie können sich das leicht veranschaulichen, wenn Sie an den Überflug eines Düsenjägers denken. Dieser ist so laut, dass sie kein Gespräch mehr führen können, egal wie laut Sie selbst auch schreien. Das Schallereignis des Düsentriebwerkes spannt ein akustisches Dach über die gesamte Breite unserer Wahrnehmung auf.\\
#+attr_html: :width 400px
[[./mask.jpg]]

Es gibt verschiedene Sorten der Maskierung. MPEG Audio Layer III nutzt übrigens alle drei Varianten der Maskierung, während Layer I & II nur die Frequenzmaskierung verwenden.
1. Frequenzmaskierung: Hierbei handelt es sich um die oben beschriebene Maskierung. Laute Frequenzen überlagern benachbarte leisere Frequenzen.
2. Zeitliche Vorwärtsmaskierung: Der Effekt der Maskierung ist nicht auf die Dimensionen Frequenz und Schalldruck beschränkt, sondern wird auch von der zeitlichen Dimension beeinflusst. Laute Frequenzen wirken sich nicht nur zum Zeitpunkt ihres Auftretens auf die benachbarten Frequenzen aus, sondern auch noch verzögert. Bis zu 200 Millisekunden später kann diese Maskierung nachfolgende Schallereignisse übertönen.
3. Zeitliche Rückwärtsmaskierung: Der Effekt funktioniert auch umgekehrt, was zunächst merkwürdig erscheinen mag. Plötzlich auftretende starke Reize werden vom Menschen schneller verarbeitet als gleichförmige unspektakuläre Reize. Der Wahrnehmungsapparat schaltet auf Alarm um und räumt den plötzlichen Reizen eine Art Überholspur ein. So kann es vorkommen, dass ein starker Reiz bis zu 20 Millisekunden schneller verarbeitet ist, als ein schwacher Reiz. Im zeitlichen Ablauf überholt der starke Reiz den schwachen. Für die Maskierung bedeutet das, dass bestimmte Schallereignisse andere maskieren können, auch wenn diese bis zu 20 Millisekunden vorher stattgefunden haben.
Passend zum Thema beschäftigt sich das Fechter´sche sowie das Weber'sche Gesetz mit der Reizempfindung.

*Schall*\\
Elastische oder mechanische Schwingungen und Wellen in einem elastischen Medium werden als Schall bezeichnet. Schall ist an die Luftdruckänderungen der schwingenden Teilchen gebunden und funktioniert nicht im Vakuum.

Ein freies Schallfeld ist eine ideale Vorstellung, die nur relativ selten in der Natur vorkommt. Bei einem diffusen Schallfeld wird der Schall reflektiert, gebeugt, gebrochen und absorbiert. Im Folgenden werden die einzelnen Bestandteile einer einfachen Schallwelle und ihre Wirkung betrachtet.

*Luftdruck*\\
Eine Schallwelle ist eine Veränderung des Luftdrucks (Abb. unten). Diese können als Wellenform mit einem Audiobearbeitungsprogramm sichtbar gemacht werden. Der normale Umgebungsluftdruck gekennzeichnet, wird abwechselnd erhöht und abgeschwächt. Diesen Wechsel nehmen wir als Ton wahr. Bei großen Wellen, wie sie zum Beispiel bei tiefen Bässen auftreten, können wir den Druckunterschied sogar als Vibration spüren.\\
#+attr_html: :width 400px
[[./druck.jpg]]



*Zyklus und Wellenlänge*\\
Mit Zyklus bezeichnen wir eine komplette Folge von Druckänderungen beginnend bei der Nulllinie über eine Phase hohen Drucks und eine Phase niedrigen Drucks bis wieder hin zur Nulllinie. Unabhängig von der Messung an der Nulllinie können wir auch von Wellenlänge sprechen. Diese beginnt an einer beliebigen Stelle der Welle, also zum Beispiel der Nulllinie oder dem Höhepunkt oder dem Tiefpunkt, und endet an eben dieser Stelle wieder. Die Wellenlänge wird in Zentimeter bzw. Meter gemessen.

*Amplitude und Schalldruck*\\
Die Ausschlaghöhe der Welle nach oben und unten wird Amplitude oder, da es sich ja eigentlich um Veränderungen des Luftdrucks handelt, Schalldruckpegel genannt. Sie beeinflusst unsere Wahrnehmung von Lautstärke. Je größer der Ausschlag ist, als desto lauter empfinden wir das Schallereignis. Der Schalldruckpegel wird in Dezibel (dB) gemessen. Dabei handelt es sich um ein logarithmisches Verhältnismaß. Das bedeutet in der Praxis, dass eine Erhöhung des Schalldrucks um 10 dB eine Verdopplung der Lautstärke beschreibt.\\
#+attr_html: :width 400px
[[./laut.png]]

*Frequenz*\\
Je kürzer die Wellenlänge, desto höher die Frequenz. Sie bezeichnet die Häufigkeit der Zyklen und wird in Hertz gemessen. Hertz (Hz) gibt die Anzahl der Schwingungen pro Sekunde an. Ein Ton mit 440 Hz hat 440 Zyklen pro Sekunde. Je höher die Frequenz, desto höher empfinden wir den entsprechenden Ton. 440 Hz entspricht übrigens dem Kammerton A.

Der Mensch nimmt Frequenzen von 20 Hz bis 20 kHz (= 20.000 Hz) wahr. Am besten hören wir Frequenzen im Bereich 300 Hz bis 4 kHz. Das ist der Bereich in dem sich auch die menschliche Sprache bewegt.

*Phase*\\
Die Phase schließlich gibt die Position der Wellenform innerhalb eines Zyklus an. Sie wird in Grad gemessen. 0° entsprechen der Nulllinie als Ausgangspunkt, 90° dem Höhepunkt, 180° wieder der Nulllinie, 270° dem Tiefpunkt und 360° wieder der Nulllinie.

*Ton und Klang*\\
Ein Ton ist zunächst einmal einfach eine Schallwelle. Diese entsteht, wenn ein Medium wie beispielsweise die Luft in Schwingung versetzt wird. Diese Schwingungen wiederum sind kleinste Veränderungen im Luftdruck. Töne sind also reine Sinusschwingungen, aus deren Zusammensetzung sich Klang ergibt. Dabei wird in Grund- und Obertöne unterschieden. Ein harmonischer Klang setzt sich aus einem Grundton t1 und Obertönen nach dem Schema n x t1 zusammen.

Die Quelle eines Tones ist immer ein mechanisch vibrierender Körper, sei es die Membran eines Lautsprechers, die Stimmbänder des menschlichen Kehlkopfes oder der Motor eines Automobils. Durch die Vibration des Körpers werden die dem Körper nächsten Luftmoleküle periodisch, also immer wieder, zusammengedrückt. Diese Moleküle geben den Druck weiter an die nächstliegenden Moleküle. So setzt sich der Schall als Druckunterschied weiter fort bis er beispielsweise unser Ohr erreicht.

Später schauen wir uns Töne und Klänge mit der Bearbeitungssoftware Soundbooth genauer an.

** Schallwandler
Schallwandler, auch elektro-akustische Wandler genannt, sind Systeme, die Schallenergie in elektrische Energie umwandeln. Im Allgemeinen wird zur Aufnahme der Schallwellen aus einem Raum und für die Abstrahlung der Schallwellen in einem Raum ein schwingungsfähiges mechanisches System als Membrane eingeschaltet. In der Tonstudiotechnik gehören Mikrofone, Lautsprecher und Kopfhörer zu den Schallwandlern.

Reversible Schallwandler können in beiden Wirkungsrichtungen betrieben werden, also als Mikrofon und als Lautsprecher, irreversible Wandler können nur in einer Richtung als Wandler benutzt werden.

Für die Umwandlung von aus dem Schallfeld entnommener mechanischer Energie in elektrische Energie und umgekehrt gibt es mehrere Möglichkeiten, die nach Art der mechanisch-elektrischen bzw. elektrisch-mechanischen Energiewandlung bezeichnet und eingeteilt werden:
- Elektromagnetischer Wandler
- Elektrodynamischer Wandler
- Magnetostriktiver Wandler
- Piezoelektrischer Wandler

Mikrofone können nach Empfänger- oder Wandlerprinzip, Leitungstechnik oder Richtcharakteristik unterschieden werden, z. B. Kondensatormikrofon, Elektretmikrofon, Bändchenmikrofon, Elektrodynamisches Mikrofon, Lavalier-Mikrofon, Grenzflächenmikrofon, Körperschallmikrofon, Großmembranmikrofon, Röhrenmikrofon, Kohlemikrofon etc.

Lautsprecher können ebenfalls nach Bau- und Antriebsform unterschieden werden, z. B. Breitband-Lautsprecher, Lautsprecherkombinationen, Tieftonsysteme, Mitteltonsysteme, Hochtonsysteme, Frequenzweichen, Aktivboxen etc.

*** Mikrofone
Alle Mikrofone verfügen über eine Membran, die durch Schall in Bewegung versetzt wird. Dieser Schall wird in elektrische Schwingungen umgewandelt. Mikrofone lassen sich in zwei unterschiedliche Arten teilen, deren Eigenschaften sich jeweils für andere Schallquellen und Aufnahmesituationen eignen. Der Unterschied liegt im Wandlerprinzip, also der Art, wie die mechanischen Schallschwingungen in elektrischen Strom umgewandelt werden. Weiterhin können Mikrofone nach Empfängerprinzip, Leitungstechnik und Richtcharakteristik unterschieden werden (Abb. unten)\\
#+attr_html: :width 400px
[[./mikro.jpg]]

Mikrofone wandeln innerhalb der Mikrofonkapsel Schallschwingungen in elektrische Wechselspannungen um. Dieser Umwandlungsprozess erfolgt in zwei Stufen (Abb. 1): Zunächst wird die dünne, elastische Membran als Schallempfänger von Schallwellen zu erzwungenen Schwingungen angeregt, d.h. akustische Schwingungen werden in mechanische umgewandelt (Empfängerprinzip). Anschließend werden diese mechanischen Schwingungen vom an den Schallempfänger gekoppelten Wandlersystems des Mikrofons in elektrische Schwingungen umgewandelt (Wandlerprinzip).\\
#+attr_html: :width 400px
[[./mikro-aufgbau.png]]

Das Empfängerprinzip eines Mikrofons wird in erster Linie von der Konstruktion der Mikrofonkapsel und der Art des Einbaus der Membran bestimmt. Aus dem Empfängerprinzip ergibt sich wiederum die Richtcharakteristik. Die Membranauslenkung soll möglichst groß sein, jedoch entstehen bei zu großer Auslenkung durch die Trägheit und Steifigkeit der Membran Verzerrungen

*Druckempfänger*\\
Die Membran des Mikrofons ist einseitig durch die Kapsel abgeschlossen, sodass nur dessen Vordereite dem Schall ausgesetzt ist. Eine kleine Öffnung im Gehäuse sorgt dafür, dass sich der Luftdruck innerhalb der Kapsel dem umgebenden statischen Luftdruck anpasst. Schnelle Luftdruckschwankungen werden aber nicht ausgeglichen, sodass sich die Membran mit diesen schnellen Schwankungen mitbewegt. Sie wird entsprechend ausgelenkt, wenn der Luftdruck der Membran im Schallfeld vom Luftdruck des Kapselinneren abweicht. Da Schalldruckschwankungen an der Membranvorderseite im Schallfeld unabhängig von der Schalleinfallsseite sind, besitzt der Druckempfänger keine Richtungswirkung, d. h. es ergibt sich eine kugelförmige Richtcharakteristik, die später näher erläutert wird.

*Druckgradientenempfänger*\\
Die Membran des Mikrofons ist im Gegensatz zum Druckempfänger beidseitig dem Schallfeld ausgesetzt. Dabei erfolgt die Membranbefestigung ringförmig um sie selbst herum (Kapsel). Durch diese Konstruktion wird die Membran nur bewegt, wenn im Schallfeld eine Druckdifferenz vor und hinter der Membran existiert. Die Empfindlichkeit für die Schalleinfallsrichtung von vorn und von hinten ist am größten, während bei seitlichem Schalleinfall die Membran nicht ausgelenkt wird. Daraus können sich die Richtcharakteristiken Acht und Niere ableiten

Die Richtcharakteristik gibt an, wie empfindlich ein Mikrofon gegenüber Schall aus verschiedenen Richtungen ist bzw. wie laut es die Schallquellen aus verschiedenen Richtungen „hört“ (Bild weiter unten).\\
*Kugel*\\
Die Mikrofonmembran bildet eine Seite einer akustisch dichten Mikrofonkapsel und wird durch Schalldruckschwankungen bewegt. Sie ist für alle Schalleinfallsrichtungen in gleicher Weise empfindlich, solange das Mikrofon wesentlich kleiner als die Wellenlänge der Schallwelle ist. Aus diesem Grund hat das Mikrofon eine Kugelcharakteristik. Oberhalb von etwa 5 kHz wird der Schall wegen der geringen Wellenlänge nicht mehr vollständig um die Mikrofonkapsel herumgebeugt, sodass sich die Kugelcharakteristik mit steigender Frequenz mehr und mehr über eine Art Nieren- bis hin zur Keulencharakteristik einengt.

Mikrofone mit Kugelcharakteristik sind i.d.R. reine Druckempfänger, d.h. es wird nur der Schalldruck der Membran gemessen. Das Luftvolumen hinter der Membran ist bis auf eine kleine Öffnung zum statischen Luftdruckausgleich abgeschlossen (1. Bild unten).

*Acht*\\
Beim Richtmikrofon mit nur Achterrichtcharakteristik kommt die Richtwirkung dadurch zustande, dass Schall auch von hinten an die Membran gelangen kann und daher eine seitlich einfallende Schallwelle keine Auslenkung hervorruft. Daher sind Mikrofone, die von hinten schalldurchlässig sind, Gradientenempfänger.

Die achtförmige Richtcharakteristik ist weitgehend unabhängig von der Frequenz. Seitlicher Schall wird um 20 bis 25 Dezibel ausgeblendet. Die Acht ist die Richtcharakteristik mit der besten Trennung einzelner nebeneinander liegender Schallquellen. Die Acht bietet vor allem beim Einzel- und Stützmikrofonverfahren eine interessante Alternative zu den verschiedenen Nieren (2.Bild unten).

#+attr_html: :width 400px
[[./chara.jpg]]

*Keule*\\
Richtrohrmikrofone haben eine keulenförmige Richtcharakteristik, die besonders bei hohen Frequenzen den Aufnahmebereich auf einen engen Winkelbereich konzentrieren. Auf größeren Entfernungen können die Mikrofone nur dann einen Gewinn bringen, wenn ausreichend Direktschall ankommt, also bei kurzer Nachhallzeit und in größeren Räumen. Sie arbeiten als Druckgradientenempfänger mit einem für Frequenzen über etwa 1 kHz verbessernden Rohrvorsatz. Da der Schall, der seitlich auf das geschlitzte Richtrohr trifft, sich mit unterschiedlichen Phasenlagen der im Rohr sich ausbreitenden Schallwelle überlagert, wird seitlicher Schall durch Interferenz teilweise ausgelöscht. Daher kommt auch die Bezeichnung Interferenzempfänger. Rohrichtmikrofone werden oftmals bei Kameraaufnahmen eingesetzt, wenn das Mikrofon relativ weit vom Sprecher entfernt sein muss. (3.Bild oben)

*Niere*\\
Mikrofone mit Nierencharakteristik sind Druckgradientenempfänger. Die Richtwirkung wird dadurch realisiert, dass von hinten eintreffender Schall einen gleich langen Weg zur Vorder- und Rückseite der Membran zurücklegen muss und so die Membran nicht auslenken kann. Die Ausblendung rückwärtigen Schalls (aber nur bei 180°) ist bei der Nierencharakteristik am besten. Im 90°-Winkel eintreffender Schall wird dabei nur um 6 Dezibel gedämpft. In einem Bereich von etwa 45° um die Hauptrichtung hat das Mikrofon praktisch dieselbe Empfindlichkeit. Gesangs- bzw. Bühnenmikrofone haben wegen der Rückkopplungsgefahr i.d.R. eine Nierencharakteristik. (4.Bild oben)

*Hyper- und Superniere*\\
Die Richtwirkungen dieser Mikrofone können als unsymmetrische Achterrichtcharakteristik betrachtet werden. Die Hyperniere nimmt bei Ausrichtung auf die Schallquelle von allen Richtcharakteristiken den geringsten Diffusschallanteil auf. Die maximale Ausblendung liegt bei der Hyperniere bei einem Winkel von 110°.

Die Superniere hat eine geringfügig breitere Richtcharakteristik als die Hyperniere, weist dafür aber eine bessere Rückwärtsdämpfung auf. Die maximale Ausblendung liegt bei der Superniere bei 135°.

**** Bauformen
*Dynamische Mikrofone*\\
Bei dynamischen Mikrofonen wird durch Induktion Strom erzeugt. Das Tauchspulmikrofon z. B. besitzt an der Membran eine Spule, die von einem Dauermagneten umschlossen ist. Wird die Membran durch den Schall in Schwingung versetzt, bewegt sich die Spule im Magnetfeld und gibt (geringfügig) Strom ab. Bändchenmikrofone arbeiten ähnlich, wobei die Membran selbst der Leiter ist und zwischen den Polen eines Magneten schwingt. Als Richtcharakteristik sind Kugel und Niere, Hyperniere und Superniere möglich. Dynamische Mikrofone benötigen keine Speisespannung und sind robust, sie arbeiten auch bei hohen Lautstärken verzerrungsarm.

Dynamische Mikrofone sind robust, langlebig, einfach zu handhaben und zu transportieren. Deswegen findet man sie oft im Live-Bereich auf Veranstaltungen, Podien und Konzerten (wo schon mal beim Auf- oder Abbau etwas herunterfällt). Es ist keine Stromversorgung für die Mikrofone nötig, daher können sie immer und überall sofort eingesetzt werden. Sie bleiben auch bei hohen Pegeln verzerrungsarm und eignen sich daher für laute Schallquellen bzw. Musikinstrumente (Trommeln, Bläser). Zudem sind sie relativ preiswert.

Die Membran ist schwer und dadurch auch schwerer in Schwingung zu versetzen (schlechteres Impulsverhalten). Das wirkt sich einerseits negativ auf die Klangqualität aus, andererseits sind dynamische Mikrofone unempfindlicher und für geringere Pegel nicht geeignet. Aufnahmen aus größerer Entfernung sind kaum möglich. Sie sind anfällig für Störgeräusche, z. B. Windgeräusche, Anfassen am Gehäuse oder Schritte, die sich über den Mikrofonständer auf das Mikrofon übertragen können.

Schauen Sie sich zum besseren Verständnis das folgende Video an: https://youtu.be/1xhonGenops

*Kondensatormikrofone*\\
Das auf dem elektrostatischen Wandlerprinzip beruhende Kondensatormikrofon ist der in der Tonstudiotechnik am meisten eingesetzte Mikrofontyp. Hier bildet die metallbedampfte Folie der etwa 1 bis 10 Mikrometer dicken Membran zusammen mit einer etwa 50 Mikrometer dicken Gegenelektrode einen Kondensator. Schwingt die Membran, ändert sich der 5 bis 50 Mikrometer große Abstand zur Gegenelektrode und damit die Kapazität des Kondensators, denn die Kapazität eines Kondensators hängt u.a. vom Abstand der Kondensatorplatten ab. Dieses Wandlungsprinzip erfordert eine Stromversorgung, um die Ladung des Kondensators aufrecht zu erhalten und den geringen Signalpegel vorzuverstärken. Der Vorverstärker ist dabei in den Griff des Mikrofones eingebaut.

Die Membran eines Kondensatormikrofons ist dünner und um das zehn- bis zwanzigfache leichter als beim dynamischen Mikrofon, sodass sie wesentlich empfindlicher ist und kleinste Schalldrücke umwandeln kann.Es bietet hohe Qualität, hat einen weitgehend frequenzunabhängigen Übertragungsfaktor sowie geringe Verzerrungen und Körperschallempfindlichkeit.

Es können alle Richtcharakteristiken durch verschiedene Bauweisen erzielt werden und tw. sogar umgeschalten werden. Durch rein mechanisch umschaltbare Elemente ist die Mikrofonkapsel entweder von hinten verschlossen (Kugel), offen (Acht) oder über ein Laufzeitglied offen (Niere, Hyperniere). Beim Doppelmembranmikrofon werden zwei entgegengesetzt ausgerichtete Nierenmikrofone, die eine gemeinsame Gegenelektrode haben, entweder in Phase zusammengschaltet (Kugel, breite Niere), gegenphasig zusammengschaltet (Acht, Hyperniere) oder es wird nur eine Membran benutzt (Niere). Erhältlich sind Kleinmembran- und Großmembran-Kondensatormikrofone sowie Röhrenmikrofone, denen ein besonders weicher, warmer Klang zugesprochen wird und die deshalb in der Musikproduktion und TV-Studios gerne verwendet werden.

Kondensatormikrofone besitzen bessere Klangeigenschaften und einen lineareren bzw. breitbandigen Frequenzgang und können teilweise auch hohe Qualitätsansprüche erfüllen. Sie sind ca. 10 dB empfindlicher und liefern einen höheren Pegel, eignen sich also auch für leise Schallquellen. Je nach Richtwirkung können auch bis auf 2 bis 3 Meter Entfernung brauchbare Aufnahmen gemacht werden.

Sie sind teurer als Dynamische Mikrofone. Sie vertragen keine gröberen Erschütterungen und keine Feuchtigkeit – derartige Einwirkungen würden die Lebensdauer stark verkürzen. Sie benötigen eine Stromversorgung mit Gleichspannung, im Normalfall 48 Volt-Phantomspeisung, bei manchen älteren Modellen Tonaderspeisung. Mittlerweile haben aber die meisten (auch tragbare) Mischpulte Phantomspeisung, sodass die Stromversorgung kaum Probleme bereitet. Extremer Schalldruck (über ca.120 dB) kann zu Verzerrungen führen. Sie sind empfindlich gegenüber Windgeräuschen (im Freien oder bei Sprach-/Gesangsaufnahmen) und unterschiedliche Einsprechwinkel führen zu Klangveränderungen (was z. B. bei einem Sprecher, der den Kopf bewegt, auffällt). Im Live-Einsatz sind sie auf Grund ihrer höheren Empfindlichkeit anfällig gegenüber Rückkopplungen.

--------

In Ton- und Fernsehstudios werden für die Überbrückung kurzer Entfernungen Mikrofone mit drahtloser Hochfrequenz-Übertragung benutzt. Der Einsatz der drahtlosen Mirkofone ist angebracht, wenn das Mikrofonkabel die Aufnahme behindert, zur Stolperfalle wird oder aus optischen Gründen unerwünscht ist.

Da die Übertragung des Signals per Funk erfolgt, ist die Nutzung drahtloser Mikrofone und Mikrofonanlagen durch die Bundesnetzagentur (früher Regulierungsbehörde für Telekommunikation und Post) gesetzlich geregelt. Um einen möglichst störungsfreien Betrieb mit anderen Funkdiensten (Fernsehen/Radio/Mobilfunk etc.) gewährleisten zu können, hat der Gesetzgeber allen Diensten bestimmte Frequenzen und Sendeleistungen zugeordnet. Wie und an wen die Vergabe der Frequenzen erfolgt, ist im § 55 des Telekommunikationsgesetzes der Bundesnetzagentur nachzulesen.

Schauen Sie sich bitte abschließend das folgende Video an: https://youtu.be/RBEs-3yq0_E
*** Lautsprecher
Lautsprecher sind praktisch Schallwandler - wie Mikrofone, nur umgekehrt. Lautsprecher sind elektroakustische Wandler, die elektrische Schwingungen in akustische Wellen, also in Schall umwandeln. Sie umfassen sowohl einzelne als auch eine Kombination mehrerer Lautsprechersysteme. Anordnungen zur Schallwiedergabe sind im Regelfall nur zusammen mit akustischen Schallführungen wie Schallwänden, Boxen oder Trichtern qualitativ befriedigend realisierbar. Entsprechend dem Wandlerprinzip unterscheidet man zwischen dynamischen und elektrostatischen sowie piezoelektrischen und magnetischen Lautsprechern. Die beiden letzteren sind nur der Vollständigkeit halber genannt, da sie in der Tonstudiotechnik keine Bedeutung haben. Lautsprechersysteme können außerdem nach ihrem Übertragungsbereich unterschieden werden in Breitbandlautsprecher, Tieftonlautsprecher, Hochtonlautsprecher und Mitteltonlautsprecher.

Lautsprecherboxen weisen i.d.R. eine kugelförmiges Abstrahlverhalten für tiefe Frequenzen und zunehmende Bündelung mit ansteigender Wiedergabefrequenz auf. Um den Ausgleich zwischen dem Schalldruck und den Schallsog vor bzw. hinter der sich bewegenden Membran bei tiefen Frequenzen zu verhindern muss der Lautsprecher in eine entsprechend große Schallwand eingebaut werden. Die Verwendung einer geschlossenen Box vermeidet den akustischen Kurzschluss. Die nach innen abgestrahlte Energie wird bei der geschlossenen Box nicht genutzt, sondern durch Dämmmaterial absorbiert.

Je nach Abstimmung und verwendetem Volumen ergibt sich eine unterschiedliche Güte der Box. Eine hohe Güte bedeutet eine geringe Dämpfung des Wiedergabeverhaltens im Tieftonbereich und damit eine Überbetonung der Bässe sowie ein langes Ausschwingverhalten der Box. Informationen zu den gängigen Lautsprechergehäusetypen sowie weitere Empfehlungen werden hier in einfacher Form erläutert.
**** Bauformen
*Dynamische Lautsprecher*\\
Der dynamische Lautsprecher beruht in seiner Wirkungsweise auf der Kraftwirkung, die ein stromdurchflossener Leiter in einem Magnetfeld erfährt. Diese Kraftwirkung wird zur Anregung von Membranen oder anderen, zur Schallabstrahlung geeigneten Systemen genutzt. Zur Gruppe der dynamischen Lautsprecher gehören die Konus-, Kalotten-, Druckkammer- und Bändchenlautsprecher sowie verschiedene Flachmembranlautsprecher.

Der dynamische Lautsprecher mit Schwingspule ist seit einigen Jahrzehnten der am weitesten verbreitete Wandler zur Wiedergabe von Musik und Sprache. Mit ihm lassen sich im Gegensatz zu anderen Systemen verhältnismäßig einfach und wirtschaftlich große Schallpegel breitbandig und bei relativ geringen Verzerrungen erzeugen.\\
#+attr_html: :width 400px
[[./auf.jpg]]

Die meisten Lautsprecher sind als Tauchspulenlautsprecher konstruiert. Hierbei wird der Leiter zu einer Spule aufgewickelt und in ein ringförmiges Magnetfeld eingetaucht. Die Spule ist mit der Membran verbunden und bildet mit ihr ein schwingungsfähiges System. Vom Verstärker fließt nun Wechselstrom, dessen Stärke im Takte der zu übertragenden Sprache oder Musik schwankt, durch die Spule. Auf den stromdurchflossenen Leiter im Magnetfeld wirkt eine Kraft, wodurch die Spule in horizontaler Richtung (in Bezug auf Abb. unten vertikal) hin- und hergeschoben wird und die mitbewegte Membran Schall abstrahlt.


*Elektrotastische Lautsprecher*\\
Beim elektrostatischen Lautsprecher wird die Kraftwirkung, die elektrische Ladungen aufeinander ausüben, für die Wandlung von elektrischen Schwingungen in akustische Schwingungen benutzt. Der Lautsprecher stellt im Prinzip einen Kondensator dar, der mit einer oder zwei festen und einer beweglichen Elektrode aufgebaut ist (Abb. unten). Das Tonfrequenzsignal (Audiosignal) wird auf die mittlere, leitende Folie gegeben. Ist die Folie gerade positiv geladen, so wird sie von der negativ vorgespannten Gitterelektrode angezogen. Ist die Folie negativ geladen, bewegt sie sich in Richtung der positiv vorgespannten Gitterelektrode. Aufgrund der Folienbewegung gerät die Luft in Schwingungen und wir hören den Schall.

Die Kraft, die die Schwingungen der beweglichen Membran verursacht, ist dem Quadrat der angelegten Spannung direkt und dem Quadrat des Elektrodenabstands umgekehrt proportional. Für den Betrieb ist eine relativ hohe Vorspannung erforderlich, denn ohne sie würde das Signal in seiner Frequenz verdoppelt und wegen der quadratischen Spannungsabhängigkeit der Antriebskraft sehr stark verzerrt werden.

Der Lautsprecher konnte sich zumindest für den Tieftonbereich nicht durchsetzen, was der aufwendigen Betriebsschaltung mit der hohen Vorspannung und dem begrenzten Membranausschlag im tieferen Frequenzbereich zuzuschreiben ist. Für leistungsstarke Lautsprecher werden erhebliche Membrangrößen notwendig. Wegen der sehr leichten Membran zeigt der Lautsprecher ein ausgezeichnetes Impulsverhalten und kann sehr hohe Frequenzen bis 100 kHz wiedergeben.\\
#+attr_html: :width 400px
[[./a.jpg]]

Befinden sich Mikrofone und Lautsprecher gleichzeitig in einem Raum, so besteht die Gefahr einer Selbstregelung der Beschallungsanlage durch akustische Rückkopplung (schriller Pfeifton). Die Voraussetzung dafür ist ein in sich geschlossener Übertragungskreis. Das Mikrofon nimmt ein Schallereignis auf, das verstärkt und vom Lautsprecher abgestrahlt wird. Die Schallwellen des Lautsprechers treffen wieder auf das Mikrofon, werden wieder verstärkt und wiederum dem Lautsprecher zugeführt usw. Wenn der vom Lautsprecher stammende Schall am Mikrofonort einen geringfügig höheren Pegel als das primäre Schallereignis hat und die Phasendifferenz zwischen diesen Signalen gegen Null geht, tritt Selbsterregung durch Rückkopplung ein. Sie ist stark frequenzabhängig.

*Sterofonie*\\
Mit Stereofonie (griechisch: stereos = räumlich, ausgedehnt) wird die Technik zur Erzeugung eines räumlichen Höreindrucks bezeichnet, die sich am menschlichen Hörempfinden orientiert. Dazu sind zwei Schallquellen nötig, deren abgestrahlte Schallsignale sich überlagern (Summenlokalisation). Durch die Summation des linken und rechten Lautsprechersignals am Abhörplatz, Signallaufzeiten und Pegelunterschiede nimmt der Höher eine virtuelle Schallquelle zwischen den beiden Lautsprechern wahr. (Phantomschallquelle). Ist dieser relativ gleich, dann bildet sich eine punktförmige Quelle zwischen den beiden Lautsprechern aus, weichen die beiden Audiosignale stark voneinander ab, bildet sich eine breitere Schallquelle aus. Für Vertikal- und Tiefenabbildung spielen Klangverfärbungen sowie das Verhältnis zwischen Direktschallanteilen und Diffusschallanteilen des Aufnahmeraumes eine Rolle.\\
#+attr_html: :width 400px
[[./stero.jpg]]

Aus diesem Phänomen der Summenlokalisation lässt sich auch das bekannte Stereodreieck zur Aufstellung der Lautsprecher und der idealen Sitzposition erklären.\\
Hinweis: Neben der Stereofonie kommt außerdem die Monofonie oder das Mehrkanal-Tonsystem zum Einsatz, auf die hier allerdings nicht weiter eingegangen werden soll.

*** Digitalisierung
Für die Arbeit am Computer werden Schallwellen digitalisiert. Dabei wird die kontinuierliche Wellenform in wert- und zeitdiskrete Signale transformiert. Die Genauigkeit der Diskretisierung hat Einfluss auf Qualität und Datenmenge.

Wenn Sie die Welle in der Audiosoftware mit der Lupe weit genug vergrößern, werden Sie einen seltsamen Effekt sehen. Die Welle löst sich bei einer sehr detaillierten Darstellung auf, und es sind nur noch einzelne Punkte zu erkennen (Abb. unten).\\
#+attr_html: :width 400px
[[./d.gif]]
Diese Punkte sind die eigentlichen Daten der Audiodatei und der Digitalisierung geschuldet. Im Unterschied zu analogen Datenträgern wie der Schallplatte können digitale Datenträger niemals die eigentliche Schallwelle erfassen, sondern immer nur zu bestimmten Zeitpunkten bestimmen, welche Ausprägung die Schallwelle gerade hat. Man spricht hier von abtasten und Abtastpunkten. Die Art und Weise, wie das Signal abgetastet wird, ist ausschlaggebend für die Qualität der Audiodaten.

Bei der Digitalisierung wird ein analoges Signal, die Schallwelle, in ein digitales Signal umgewandelt. Analoge Signale werden auch als zeit- und wertkontinuierlich bezeichnet. Zeitkontinuierlich bezeichnet die Eigenschaft des Signals über den gesamten Zeitraum des Schallereignisses kontinuierlich vorhanden zu sein. Wertkontinuierlich bedeutet, dass die Ausschläge der Welle jede beliebige Größe annehmen können.

*Diskretisierung*\\
Bei der Digitalisierung geht es darum, diese beiden Kontinuitäten zu durchbrechen. Aus dem kontinuierlichen Signal wird ein diskretes: Zeitdiskret bedeutet, dass das Signal nur noch zu bestimmten Zeitpunkten abgetastet wird. Wertdiskret bedeutet, dass das Signal nur bestimmte vorgegebene Werte annehmen darf (Abb. 2).

Auf dem originalen wert- und zeitkontinuierlichen Ausgangssignal werden zwei Diskretisierungsschritte vorgenommen. Zunächst wird das Signal abgetastet, also zeitdiskretisiert. Daneben wird die Anzahl der Werte reduziert, die ein Signal annehmen darf. Man spricht hier von Quantisierung. Aus der Wellenform wird eine Treppenform, es entsteht ein wertdiskretes und zeitkontinuierliches Signal. Generell gilt: Je weniger Treppenstufen ein Signal annehmen darf, desto kleiner wird seine Codierung. Allerdings verschlechtert sich auch die Qualität entsprechend. D.h. im Umkehrschluss: Je feiner, desto qualitativ hochwertiger und desto größer die entstehenden Daten.\\
#+attr_html: :width 400px
[[./dis.jpg]]

Sie kennen das Prinzip bereits aus der Bildverarbeitung: Dort können Sie die Datengrößen enorm reduzieren, indem Sie die Anzahl der möglichen Farben reduzieren. Die Codierung erfolgt über 1-Bit, 8-Bit, 16-Bit oder 24-Bit Farbtiefen. Genauso verhält es sich hier, nur dass Sie die möglichen Amplitudenwerte, also der Ausschläge der Welle, reduzieren. Analog zur Farbtiefe wird hier von Bittiefe gesprochen. Klassisch sind Tiefen von 8-Bit, mit denen Sie 28 = 256 Werte, oder 16-Bit, mit denen Sie 216 = 65.536 Werte vergeben können.

Bei der Digitalisierung können Probleme auftreten: Eine klassische einfache Welle wird in bestimmten Zeitintervallen abgetastet und das digitale Abbild abgespeichert (Abb. 3.). Die Häufigkeit dieser Abtastungen nennt man Abtastrate. Wenn Sie dieses Schallereignis nun wieder abspielen wollen, dann müssen Sie aus den digitalisierten Werten die Schallwelle wieder rekonstruieren. Eine solche Rekonstruktion könnte so aussehen, jedoch hat das rekonstruierte Signal im Vergleich eine deutlich größere Wellenlänge das das Original. Es klingt tiefer. Die Rekonstruktion ist fehlerhaft.
Der Grund für die fehlerhafte Rekonstruktion liegt in einer mehrdeutigen Codierung. Sie entsteht aufgrund der zu geringen Abtastrate (engl.: sampling-rate). Diese bewirkt, dass mit den digitalisierten Werten verschiedene Wellen rekonstruiert werden können.

*Sampling Theorem*\\
Diese Problematik zu verhindern ist sehr einfach: Die Abtastrate muss mehr als das doppelte der abgetasteten Frequenz betragen. Bei einer Frequenz von 500 Hz muss die Abtastrate also mindestens 1001 Hz betragen. Da wir in klassischen Schallereignissen verschiedene Frequenzen vorfinden, nehmen wir immer die maximal erreichte Frequenz als Ausgangspunkt. Wir sprechen hier vom Sampling-Theorem bzw. vom Nyquist-Theorem, benannt nach dem Physiker Harry Nyquist (1889-1976):\\
#+attr_html: :width 400px
[[./ny.jpg]]

Ein Beispiel: Der Mensch hört Töne zwischen 20 Hz und 20 kHz. Wie groß muss die Abtastrate sein, damit alle wahrnehmbaren Töne codiert werden können?

Die Antwort berechnet sich sehr einfach: Sie nehmen die höchste Frequenz und multiplizieren diese mit zwei.\\
#+attr_html: :width 400px
[[./bsp.jpg]]

Die Abtastrate muss also größer als 40 kHz sein. Doch Vorsicht: die Angabe "mindestens 40 kHz" reicht noch nicht aus, die Frequenz muss größer als 40 kHz sein. Übrigens: Audio-CDs haben eine Abtastrate von 44.1 kHz.

Ein weiteres Beispiel: Für das Telefon ist eine Abtastrate von nur 8 kHz üblich. Können Sie sich vorstellen, warum keine höhere Rate verwendet werden muss?
Die Antwort ist hier sehr pragmatisch: Die menschliche Sprache liegt zwischen 300 Hz und 4 kHz. Daher ist eine Abtastrate von 8 kHz völlig ausreichend.

Bei der Digitalisierung von Audio gilt es also zwei Parameter zu berücksichtigen: Die Häufigkeit der Abtastung, genannt Abtastrate, und die Genauigkeit der Abtastung, die Bitrate. Eine bislang vernachlässigte Größe ist die Anzahl der verwendeten Kanäle. Diese geben an, wie oft das Signal parallel aufgenommen wird. Klassische Einstellungen sind 1-Kanal für Monoaufnahmen und 2-Kanal für Stereoaufnahmen. Es können aber durchaus noch mehr Kanäle für qualitativ hochwertige Soundsysteme aufgenommen werden.\\
#+attr_html: :width 400px
[[./para.png]]

Die Berechnung der Dateigröße F erfolgt einfach über die Multiplikation dieser Parameter:\\
#+attr_html: :width 400px
[[./re.jpg]]


Ein 3:30 minütiges Schallereignis wird für eine Audio-CD digitalisiert. Wie aus Tab. 1 ersichtlich beträgt die Abtastrate 44.1 kHz, die Bitttiefe 16 Bit und zwei Kanäle stehen zur Verfügung. Die Berechnung lautet also:\\
#+attr_html: :width 400px
[[./r2.jpg]]

Auf eine klassische Audio-CD passen 747 MB Daten. Wie lange darf eine Aufnahme maximal sein, um auf einer CD unterzukommen?

Hinweis: Achten Sie auf die Vereinheitlichung der MB- und Bit-Angaben. Dazu muss man wissen, dass ein MB aus 1024 KB besteht und ein KB aus 1024 Byte und ein Byte aus 8 Bit. 747 MB sind also 747·1024·1024·8 Bit.

Zunächst muss die obige Formel zur Zeit t hin aufgelöst werden. Dazu werden s, b und c durch Division auf die andere Seite des Gleichheitszeichens gebracht:\\
#+attr_html: :width 400px
[[./r23.jpg]]

Das entspricht ca. 74 Minuten. Übrigens: Die Legende berichtet, dass der Vizepräsident von Sony, Norio Ohga, diese Länge festgelegt haben soll, damit die neunte Sinfonie von Ludwig van Beethoven vollständig auf die CD passt.

** Dateiformate
Für unterschiedliche Anwendungszwecke existieren auch unterschiedliche Dateiformate, die in Qualität und Streamingfähigkeit auf den jeweiligen Anwendungszusammenhang hin optimiert sind. MP3 und WAV bilden dabei die wohl bekanntesten und gleichzeitig zeigen sie auch deutlich wie unterschiedlich die Codierung vorgenommen werden kann.

Es gibt für Audiodaten zahlreiche Dateiformate. Dies liegt zum einen an den zahlreichen Möglichkeiten der Digitalisierung, zum anderen aber auch daran, dass die verschiedenen Einsatzbereiche und Anforderungen völlig unterschiedliche Dateiformate erzwingen. Die Menge ist definitiv zu groß als dass hier eine vollständige Übersicht gegeben werden könnte. Als weitere Beispiele dienen die folgenden Formate:
- Rohdaten (.pcm)
- Audio Interchange File Format (AIFF, Containerformat) (.aif, .aiff, .aifc, .ief, .snd)
- Advanced Digital Audio (ADA)
- Apple Lossless Audio Codec (ALAC), auch: Apple Lossless Encoding Apple Lossless (.m4a, .mp4)
- Windows Media Audio (auch als Lossless) (.wma)
- MPEG 1/2/2.5 Audio, z. B.MPEG-1 Audio Layer 1 (.mp1) oder Audio Layer 2 (.mp2)
Wir werden uns daher zwei Formate, die Sie wahrscheinlich bereits vom Namen her kennen, beschränken: Wave und MP3. Beide Formate unterscheiden sich sehr stark voneinander.
- Wave ist als Austauschformat konzipiert, d.h. das Audiosignal wird vollständig rekonstruierbar codiert. Die Dateien werden immer als Ganzes weitergegeben und können zur Weiterverarbeitung genutzt werden.
- MP3 geht einen ganz anderen Weg: Es legt höchsten Wert auf eine große Kompression. Dafür verzichtet es auf große Anteile des Ausgangssignals. Sie werden einfach weggeworfen. Aufgrund der resultierenden hohen Kompression und auch der Dateistruktur eignet es sich zum Streaming im Internet. Eine Weiterverarbeitung einer MP3-Datei ist aber aus Qualitätsgründen wenig sinnvoll.

*** Wave
Das Dateiformat für Wave-Dateien codiert die Schallwellen nach der Pulse Code Modulation. So kann fast die vollständige Information erhalten bleiben. Das Dateiformat eignet sich nicht für das Streaming und führt auch zu sehr großen Dateien.

*Pulse Code Modulation* (PCM)\\
Wave-Dateien mit der Extension .wav wurden bereits in den 1980er Jahren von Microsoft gemeinsam mit IBM definiert. Sie können aber auch über alternative Betriebssysteme wie Linux oder Mac OS abgespielt werden. Gespeichert wird die digitale Kopie des Schalldrucks. Die Daten werden dabei verlustfrei digitalisiert. Entsprechend sind die entstehenden Dateien sehr groß und qualitativ sehr hochwertig. Das vollständige Verfahren zur Digitalisierung nennt sich Pulse Code Modulation (PCM), das auch in anderen Einsatzbereichen verwendet wird.

Die Pulse Code Modulation ist das klassische Verfahren zur Digitalisierung von Audiodaten. Die PCM basiert auf den oben beschriebenen Schritten der Digitalisierung und wandelt das Ergebnis in ein Signal aus Einsen und Nullen um. Im Beispiel (Abb. 1) wird jeder Quantisierungsstufe eine 3-Bit-Folge zugewiesen. Die Abtastung erhält den Wert der Quantisierungsstufe, die der Amplitude entspricht. Die erste Abtastung erreicht die zweite Stufe. Diese weist der Abtastung den Wert 001 zu. Die zweite Abtastung erreicht die dritte Stufe und erhält den Wert 010. Der entstandene Code aus Nullen und Einsen lässt sich anschließend einfach als digitales Signal weiterverarbeiten und transportieren.\\
#+attr_html: :width 400px
[[./pcm.jpg]]

Die PCM ist die grundlegende Methode für Technologien in digitalen Netzen zur Audioübertragung. PCM gibt es inzwischen in verschiedenen Varianten. Die bisher besprochene Variante wird auch lineare PCM genannt: sie zeichnet sich durch gleiche (lineare) Quantisierungsschritte aus. Sie wird bei der Fernsehübertragung bei der europäischen Fernsehnorm PAL und dem amerikanischen Pendant NTSC eingesetzt. Als PCM 1630 ist sie der Standard für Audio-CDs. Als Standard G.711 legte die Internationale Fernmeldeunion ITU (International Telecommunication Union) 1988 die Parameter für die Digitalisierung von Sprache für die Festnetztelefonie fest. Einsatzbeispiel ist ISDN, aber auch einige Voice-over-IP Lösungen. G.711 digitalisiert mit einer Samplerate von 8 kHz und einer Bittiefe von 8 Bit Sprachfrequenzen von 300 bis 3.400 Hz.

Eine fortgeschrittenere Variante ist die Differential Pulse Code Modulation (*DPCM*): Sie macht sich zunutze, dass es meist nur geringe Änderungen zwischen den Abtastwerten einer Welle gibt. Geringfügige Änderungen lassen sich gut mit weniger Speicheraufwand codieren. Die DPCM verzichtet daher auf die Codierung der eigentlichen Abtastwerte und codiert nur die Veränderung eines Abtastwertes zu seinem Vorgänger.

Die ADPCM (*Adaptive DPCM*) ist eine vorausschauende Variante der DPCM: Sie schätzt auf der Basis der bisherigen Abtastwerte den aktuellen Wert und codiert den eigenen Irrtum, d.h. den Unterschied zwischen der Schätzung und dem tatsächlichen Wert. Ferner nutzt sie unterschiedlich viele Bits zur Codierung in Abhängigkeit von der Größe des Unterschieds. Die ADPCM ist eine deutlich bessere Codierung als die DPCM.

Eine Wave-Datei besteht im Wesentlichen aus zwei Datenblöcken, sogenannten Chunks. Ein Block definiert die Codierung der Daten und der zweite Bock enthält die eigentlichen Daten. Zunächst jedoch muss sich eine Wave-Datei erst einmal als solche identifizieren. Dies geschieht im Dateikopf (engl.: header). Dieser besteht aus insgesamt drei Einträgen, von denen zwei die Codewörter »RIFF« und »WAVE« als Text enthalten. RIFF (resource interchange format) ist ein allgemeines Format zur Speicherung von Multimedia-Daten für Windows. »Wave setzt auf »RIFF« auf. Als drittes Element des Headers steht die Dateilänge (Tab. unten).\\
#+attr_html: :width 400px
[[./w1.png]]

Der anschließende Formatblock beginnt mit dem Codewort »fmt« (Tab. 2). Beachten Sie das Leerzeichen am Ende des Codewortes. Die Informationen werden immer mit zwei oder vier Byte gespeichert, was zwei bzw. vier Buchstaben entspricht. Daher muss dem Codewort hier das Leerzeichen als vierter Buchstabe hinzugefügt werden. Es folgt die Angabe über die Länge des Blocks. Wird die klassische lineare PCM verwendet, ist die Länge 16 Bit. Das Format der Digitalisierung ist hier mit »1« angegeben, was für eine lineare PCM steht. Eine andere Zahl würde auf irgendeine Form der Kompression schließen lassen. Anschließend stehen Informationen über die Digitalisierung, wie wir sie oben besprochen haben: Kanäle und Samplerate. Es folgt eine Angabe für die in der Netzwerkübertragung benötigte minimale Bandbreite in Bytes pro Sekunde. Werden zwei Kanäle digitalisiert, dann entstehen an jedem Abtastpunkt zwei Samples. Diese werden gemeinsam in einen Sample-Block (manchmal auch Sample Frame genannt) verpackt. Die Größe dieses Sample-Blocks wird ebenfalls angegeben. Der Formatblock endet mit der Angabe der Abtastrate.\\
#+attr_html: :width 400px
[[./w2.png]]

Der Datenblock ist deutlich einfacher, allerdings auch deutlich größer. Er beginnt mit dem Codewort »data« und einer Angabe zu seiner Länge. Anschließend folgt die Codierung des eigentlichen Audiostroms (Tab. 3).\\
#+attr_html: :width 400px
[[./w3.png]]

Neben dem Formatblock und dem Datenblock kann eine Wave-Datei noch eine Reihe weiterer Blöcke beinhalten, wie beispielsweise den fact chunk, der Informationen über eventuell verwendete Kompressionsverfahren enthält, oder den list chunk, der Informationen zum Copyright aufnimmt. Diese Blöcke sind hier außen vorgelassen, da sie eher selten verwendet werden und für unsere Problemstellung nebensächlich sind.

Das Wave-Format eignet sich nicht zu Streaming im Internet, da es einen Dateikopf hat, der gelesen werden muss, bevor der Rest der Datei interpretiert werden kann. Streamingfähige Formate benötigen diese Informationen auch, können sie aber nicht nur einmal am Anfang der Datei senden, sondern müssen sie regelmäßig verschicken. Hörer eines Internetradios könnten sich sonst niemals in einen laufenden Stream einschalten.

*** MP3
Das heute wohl populärste Audioformat ist MP3. Es handelt sich dabei um ein Verfahren zur Audiokompression, welches von der Moving Pictures Expert Group (MPEG) für audiovisuelle Medien definiert wurde. Es ist Teil der Videostandards MPEG-1 und MPEG-2. Die vollständige Bezeichnung ist MPEG Audio Layer III. Ursprünglich wurde es also für den ein Video begleitenden Audiostrom konzipiert, hat sich aber längst verselbständigt.\\
Das Kürzel MP steht dabei für Moving Picture. Die 3 bezeichnet den sogenannten Layer. MPEG definiert drei Layer, die sich in Qualität und Kompression voneinander unterscheiden. Layer III ist den beiden anderen Layern hier deutlich überlegen, aber auch technisch aufwändiger (Abb. unten).\\
#+attr_html: :width 400px
[[./mp31.jpg]]

Das eingehende Audiosignal wird mit Hilfe einer Filterbank in 32 Subbänder unterteilt. Das komplette Frequenzspektrum wird also in 32 verschiedene Spektren unterteilt. Diese einzelnen Subbänder werden im Folgenden separat codiert. Der Vorteil dieser Vorgehensweise liegt darin, dass Subbänder in Frequenzbereichen, die sowieso nur schlecht gehört werden, qualitativ auch nur mäßig codiert werden müssen. Qualitativ schlechter heißt immer auch mit weniger Speicherplatz verbunden.\\
Welche Frequenzen weniger aufwändig codiert werden können, wird im psychoakustischen Modell beschrieben. Dabei handelt es sich um eine Beschreibung der menschlichen Audio-Wahrnehmung. Es nimmt direkten Einfluss auf die Quantisierung der einzelnen Subbänder. Anschließend werden die Subbänder entropiecodiert. Unter Entropiecodierung versteht man ein Verfahren, welches ein Signal so komprimiert, dass kein Informationsverlust entsteht, das ursprüngliche Signal also vollständig rekonstruiert werden kann. Hierbei handelt es sich um die Huffman-Codierung, welche auch im Grafikbereich im GIF-Format verwendet wird.

Im Fall von MPEG Audio erfolgt die verlustfreie Codierung nach dem verlustbehafteten Eingriff des psychoakustischen Modells. Es kann also nur das quantisierte Signal der 32 Teilbänder vollständig rekonstruiert werden, nicht aber das ursprüngliche Ausgangssignal. Das unterscheidet MP3 von Wave, welches das vollständige Ausgangssignal verlustfrei rekonstruieren kann. Den Abschluss bildet der Aufbau des Bitstreams in Form von Frames. Dazu kommen wir am Ende des Kapitels.

MP3 erreicht wesentlich kleinere Dateigrößen als Wave-Dateien, selbst wenn diese noch mit Kompressionen versehen werden. Der Clou liegt darin, dass MP3 nur das codiert, was für den Menschen wahrnehmbar ist. Frequenzen, die jenseits der bewussten Wahrnehmung liegen, werden einfach verworfen. Und das sind gar nicht so wenige. Das psychoakustische Modell liefert nach der Aufteilung des Ausgangssignals in die Subbänder dem Encoder Informationen, wie genau mit den einzelnen Subbändern zu verfahren ist.
MP3 löst das bei Wave vorhandene Streaming-Problem durch sogenannte Frames. Das sind einzelne Abschnitte, aus denen die MP3-Datei zusammengesetzt wird. Jeder Frame kennt die notwendigen Informationen und führt einen Teil der digitalisierten Daten mit sich.
Ein Frame besteht aus einem 32 Bit (4 Byte) großen Kopf und insgesamt 1152 Audio Samples (Abb. 2). Wenn Sie den Kopf eines Frames allein schon in Bezug zur Größe mit dem Kopf einer Wave-Datei vergleichen, sehen Sie den Unterschied.\\
#+attr_html: :width 400px
[[./frame.gif]]

Sie können sich den Unterschied in der Struktur einer Wave-Datei und einer MP3-Datei recht einfach anhand des Vergleichs von Zügen und Lastwagen verdeutlichen. Bei einem Zug haben Sie eine Lokomotive und eine Reihe von Waggons. Fällt die Lokomotive aus, bleibt der Zug stehen und Sie können nichts mehr mit ihm anfangen. Haben Sie aber eine Kolonne von Lastwagen mit Ihrer Ladung bestückt, dann können die ersten drei Laster ausfallen, die übrigen bringen ihre Ladung dennoch ans Ziel. Die Ladung dieses Vergleichs sind die Audiodaten, das Ziel ist Ihr Ohr.

Der Kopf eines Frames beinhaltet die in folgender Tab. aufgelisteten Einträge:\\
#+attr_html: :width 400px
[[./aufbaump3.png]]

Der Kopf beginnt mit dem Sync-Signal welches aus 11 Bits besteht, die allesamt auf 1 geschaltet sind. Es folgt die Identifizierung der zugrundeliegenden MPEG-Version und Layer. Um die Integrität der Daten zu gewährleisten, kann nach dem Kopf eine Prüfsumme (engl.: cyclic redundancy check - CRC) gesetzt werden. Dies muss aber im Kopf mit dem Protection-Bit angekündigt werden. Es folgen Bitrate (8 - 448 Kilobits pro Sekunde) und Samplingfrequenz (8 - 48 kHz). Da die Bitrate mitunter codierungstechnisch nur theoretisch getroffen werden kann, gibt ein Padding-Bit den eventuell auftretenden Versatz an. Das anschließende Private-Bit kann für eigene Zwecke verwendet werden. MP3 kann unterschiedliche Kanalkombinationen codieren:

*Kanäle*\\
- Stereo Coding: Hierbei handelt es sich um den klassischen 2-Kanal-Stereoton. Beide Kanäle werden separat codiert. Das bedeutet die doppelte Menge an Audiodaten gegenüber einem einfachen Monosignal.
- Joint Stereo Coding: Hier wird die Datenredundanz beider Kanäle ausgenutzt. Nur einer der Stereokanäle wird voll codiert. Vom zweiten Kanal wird nur die Differenz zum ersten Kanal codiert. Die die Werte der Differenz zwischen zwei Signalen sind meistens deutlich kleiner und weniger variabel als die Werte eines Originalsignals, so dass diese Methode zu einer deutlich stärkeren Kompression führt.
- Dual Channel Coding: Hier werden zwei völlig unabhängige Kanäle codiert, die im Unterschied zu Stereoton auch unabhängig voneinander abgespielt werden können. Genutzt wird dies beispielsweise beim Zweikanalton um verschiedene Sprachen anzubieten.
- Single Coding: Das einfachste Verfahren schließlich bietet die Möglichkeit, Monosignale aufzunehmen.
Die Mode Extension wird beim Joint Stereo zur Optimierung des Kompression genutzt und vom Encoder automatisch eingestellt. Am Ende des Kopfes können noch Angaben gemacht werden, ob die Audiodatei durch Copyright geschützt ist und ob es sich um ein Original handelt. Die Emphasis ist ein Relikt aus der Frühzeit der Audio-CD und diente damals der Rauschunterdrückung. Inzwischen ist das Verfahren obsolet.
** Bearbeitung
Um Audiodateien klanglich zu verbessern, aufzuwerten und kreativ zu gestalten ist eine Bearbeitung des Materials notwendig. Dies geschieht in aller Regel mit Filtern und Effektgeräten entweder bereits im Mischpult oder bei der späteren Bearbeitung mit einer Schnittsoftware.
- Mischpult: Das Mischpult ist die zentrale Schaltstelle in jedem Tonstudio und gehört zu den wichtigsten Werkzeugen im Audiodesign. Die Mikrofonsignale mit ihren verschiedenen Pegeln laufen hier zusammen und werden in Klang und Lautstärke aufeinander abgestimmt, nachbearbeitet und schließlich zu Lautsprechern oder Aufnahmegeräten geleitet.
- Filter: Um ein Musikstück technisch zu verbessern oder kreativ zu gestalten können unterschiedliche Filter zum Einsatz kommen: Pegelorientierte Effektgeräte wie Limiter, Gate oder Kompressor beziehen sich auf die Dynamik eines Signals. Verzerrende Effektgeräte verändern das Signal, indem Obertöne hinzugefügt werden. Bei zeitverzerrenden Effektgeräten wie Delay, Reverb oder Chorus wird eine Veränderung des Klanges durch das Mischen des Originalsignals mit einem verzögerten Double erzeugt.
- Adobe Soundbooth: Die Software von Adobe ist leicht zu bedienen und soll hier stellvertretend für weitere Programme wie Audacity, Cubase, Nuendo, Soundtrack Pro, Logic, ProTools etc. behandelt werden. Künftig wird Adobe Audition CS5.5 Soundbooth in der Adobe Creative Suite 5.5 Production Premium ersetzen. Eine Testversion ist hier erhältlich.

*** Mischpult
Das Mischpult ist die zentrale Schaltstelle in jedem Tonstudio und gehört zu den wichtigsten Werkzeugen im Audiodesign. Die Mikrofonsignale mit ihren verschiedenen Pegeln laufen hier zusammen und werden in Klang und Lautstärke aufeinander abgestimmt, nachbearbeitet und schließlich zu Lautsprechern oder Aufnahmegeräten geleitet. Ein Mischpult mischt mehrere Signale rückwirkungsfrei zusammen, deren Ausspielung verschiedenen Kanälen zugeordnet werden kann (PAN, AUX, Routing etc.). Der Pegel kann vorverstärkt, gedämpft oder dynamischer gestaltet werden. Ebenso ermöglichen mehrere Filter und pultabhängige Funktionen die individuelle Gestaltung des Klanges. Jedes in das Pult eingehende Signal durchläuft zuerst einen Eingangskanal und wird dann zu Subgruppen und der Stereo-Summe weitergeleitet. Mischpulte werden nach der Zahl der Eingangskanäle Subgruppen eingeteilt. Die Bezeichnung 32/8/2-Mischpult bedeutet 32 Eingangskanäle mit 8 Subgruppen und eine Stereo-Summe.

Mischpulte werden in den verschiedensten Gebieten eingesetzt. Angefangen beim Hobby-Musiker, der seine musikalischen Ideen in die Tat umsetzen will, bis hin zum professionellen Ton-, Fernseh- oder Filmvertonungsstudio.

*Kanalzug*\\
Ein Mischpult (z. B. Yamaha DM 100 in Abb 1.) besteht in der Regel aus mehreren Kanälen (Channels), die mit Hilfe des Faders zusammengemischt und als Stereo-Summe (Main Mix) ausgegeben werden. Mit dem Mute-Knopf kann der Eingangskanal stumm geschaltet werden, mit dem Solo-Knopf werden alle Kanäle unterdrückt.  Auf dem Weg zur Summe können die Audiosignale in verschiedenster Form beeinflusst werden.

*Eingänge und Pegel*\\
Um die Signale unterschiedlicher Quellen ins Mischpult zu führen, besitzt es auf der Rückseite sowohl XLR-Buchsen für die Mikrofon-Eingänge als auch Klinkenbuchsen für Signale mit Line-Pegel. In einem Vorverstärker werden die unterschiedlichen Eingangspegel auf den Pegel des Mischpultes angeglichen. Null Dezibel entsprechen dabei dem genormten Studiowert von 1,55 Volt. Schwache Mikrofonpegel werden dabei besser verstärkt als die hohen Line-Pegel von elektrischen Instrumenten. Daher stellt das Mischpult durch den Phantomspannungs-Schalter für Kondensatormikrofone die zum Betrieb erforderliche Phantomspeisung von +48 V zur Verfügung. Die Pegelanhebung kann übrigens mit der PFL-Funktion (Pre Fader Listening) überwacht werden, die das Signal unabhängig vom Fader auf die Solo-Schiene legt. Jetzt kann man anhand des Level-Meters das Signal mit dem Gain-Regler auf 0 dB (analog) einpegeln. Die Verstärkung muss so eingestellt werden, dass die maximale Eingangsspannung gerade kein Übersteuern des Mischpultes erzeugt und dennoch ein gutes Signal-/Rauschverhältnis erzielt wird. Dreht man den Gain-Regler zu weit auf, so kommt es zu Verzerrungen des Audio-Signals, dem sogenannten Clipping. Das Level-Meter ist eine Multi-Segment-Anzeige, die den kompletten Arbeitsbereich eines Pultes visuell darstellt. Die Einteilung dieses Meters reicht bei analogen Pulten z. B. von -30 bis +22 dB, bei digitalen Pulten i.d.R. von -72 bis 0 dB.

Weil in einem Mischpult viele Signale zusammengemischt werden, sollte man immer auf einen genügend großen Abstand zur Vollaussteuerung (Headroom von 12 dB) achten. Über den Einschleifweg (Insert send return), der sich normalerweise nach dem Gain-Regler befindet, lässt sich das Signal zur weiteren Verarbeitung mit anderen Klang- und Dynamikregelungs-Geräten (Kompressoren, Gates, parametrische Equalizer, etc.) aus dem Mischpult führen und wieder zurückholen. Danach passiert das Audio-Signal den Kanal-Equalizer, der ein Anheben oder Absenken verschiedener Frequenzbereiche möglich macht.

*Ausgänge*\\
Um Audio-Signale mit einem Effekt, wie z. B. Hall, zu versehen, kann ein Signal vom Kanal aus per AUX-Weg zusätzlich abgegriffen werden und zu einem Effektgerät außerhalb des Mischpults geschickt werden. Dieser Abgriff kann sowohl vor dem Fader (Pre-Fader) als auch nach dem Fader (Post-Fader) erfolgen. Prefaded Sends befinden sich vor dem Kanalregler, sodass der Pegel des Sendsignals ausschließlich vom Send-Regler bestimmt wird. Sie sind für das Monitoring im Live-Betrieb von Nutzen. Der Spieler hört hierbei das noch unbearbeitete bzw. ein für ihn abgemischtes Signal. Postfaeded Sends befinden sich hinter dem Kanalregler und senden daher nur bei nicht ganz zurückgeregeltem Kanalregler (Fader) ein Signal. Der Pegel des Send-Signals wird durch Send- und Kanalregler festgelegt. Wichtige Anwendung ist hier der Einsatz von Effekten wie Hall, Chorus oder Delay. Die Summenregler, die die einzelnen AUX-Wege zusammenfassen, heißen Effect-Sends (auch AUX-Sends) und legen den Pegel fest, mit dem das Effektgerät angesteuert wird. Mit dem Effect-Return wird der Pegel des vom Effektgerät erzeugten Signal bestimmt und dem Gesamtsignal zugemischt. Übrigens ermöglicht eine Talkback-Sektion im Mischpult die Kommunikation mit den spielenden Musikern, um mit ihnen Absprachen zu treffen.

Am Ende des Kanalzugs ist es möglich, das Audio-Signal auf verschiedene Sammel-Schienen (Busse) zu schicken (Routing). Dabei gibt es je nach Pult verschiedene Möglichkeiten. Man kann zum Beispiel mehrere Kanäle auf sogenannte Subgruppen zusammenführen, um diese dann gemeinsam in der Lautstärke zu regeln. Häufig werden zwei Busse zu einem Stereopaar zusammengefasst. In diesem Fall bestimmt der Panorama-Regler, auf welchen Bus des Paares das Eingangssignal mit welchem Pegel geleitet wird.

Schauen Sie sich abschließend bitte diese Videoreihe an, um einen guten Überblick über den Aufbau analoger Mischpulte zu erhalten. Eine etwas andere und unterhaltende Erklärung zu der Funktionsweise eines Mischpultes finden Sie hier.

*** Filter
Um ein Musikstück technisch zu verbessern oder kreativ zu gestalten können unterschiedliche Filter zum Einsatz kommen: Pegelorientierte Effektgeräte wie Limiter, Gate oder Kompressor beziehen sich auf die Dynamik eines Signals. Verzerrende Effektgeräte verändern das Signal, indem Obertöne hinzugefügt werden. Bei zeitverzerrenden Effektgeräten wie Delay, Reverb oder Chorus wird eine Veränderung des Klanges durch das Mischen des Originalsignals mit einem verzögerten Double erzeugt. Hier soll nur auf die wichtigsten Filter und Effektgeräte eingegangen werden.\\
Der Equalizer wird auch als Entzerrer bezeichnet und setzt sich aus mehreren Filtern zusammen, mit denen das Spektrum des Eingangssignal beeinflusst werden kann. Er wird einerseits dazu verwendet, um bekannte lineare Verzerrungen der akustischen Übertragungskette (Mikrofonfrequenzgang, Lautsprecherfrequenzgang, Raumübertragungsfunktion) zu korrigieren und stellt andererseits ein wichtiges Werkzeug zur kreativen Klanggestaltung dar.

Einige Mischpulte biete zusätzlich die Möglichkeit, ein Hochpass- bzw. Low-Cut-Filter mit einer Grenzfrequenz von meist 75 Hz in den Kanalzug einzuschalten, um tieffrequente Störungen wie Trittschall, Wind- und Popgeräusche und Brummen abzusenken. Hier wird unterschieden in Parametrischen, Graphischen und Dynamischer Equalizer. Prinzipiell sollten Frequenzveränderungen des Signals mit Hilfe eines Equalizers nur in Maßen durchgeführt werden, da die Gefahr der Klangverfälschung sehr groß ist und je nach Qualität des Equalizers auch entsprechend starke Phasenveränderungen im Signal durchgeführt werden.

*Normalizing*\\
Das Normalisieren gehört zu den Effektgeräten, die sich mit der Dynamik-Bearbeitung beschäftigen. Darunter versteht man die nachträgliche Verstärkung des digital aufgenommenen Signals auf Vollaussteuerung, sodass die größte im Signal vorkommende Amplitude gerade nicht verzerrt wird. Es wird dabei aber nicht nur der Pegel des Signals verstärkt, sondern auch das Rauschen. Es kann daher Vollaussteuerung oder aber auch nur ein bestimmter Dezibelwert bzw. Prozentsatz eingestellt werden.

*Kompressor*\\
Die Lautheit eines Signals hängt nicht vom maximalen Pegel, sondern von seiner Gesamtintensität ab. Schallereignisse mit großer Dynamik, d.h. mit großen Unterschieden zwischen maximaler und minimaler Signalamplitude werden somit bei gleichem Signalpegel leiser wahrgenommen als Signale mit weitgehend konstantem Pegel. Ein Kompressor verringert die Signaldynamik, indem alle Amplituden über einen bestimmten einstellbaren Wert (Threshold) um einen einstellbaren Faktor (Kompressionsverhältnis, ratio) abgeschwächt werden. Das gesamte Signal wird dann wieder um einen einstellbaren Faktor auf Vollaussteuerung verstärkt.

Da die menschliche Stimme eine sehr große Dynamik besitzt und sich im Zusammenhang mit anderen akustischen Ereignissen oft nur schwer durchsetzten kann, werden bei vielen Musikproduktionen Sprache und Gesang mit einem Kompressor bearbeitet, um lauter zu wirken.

Schauen Sie sich bitte dieses Video an http://www.delamar.de/video-workshops/effekt-grundlagen-kompressor-2717/.

*Limiter*\\
Der Limiter ruft eine Dynamikveränderung des Signals hervor. Der Regelvorgang eines Kompressors verläuft weich und kontinuierlich, sodass der Klang noch etwas von seinem dynamischen Ursprungscharakter hat. Natürliche Dynamik-Spitzen von Schlagzeug und Percussion sind noch vorhanden und können beim Digitalisierungsvorgang zu Übersteuerungen führen. Nach dem gestalterisch wirkenden Kompressor wird daher häufig ein Peak-Limiter verwendet, der die technische Sicherheit des Musikstückes gewährleistet und Übersteuerungen verhindert, indem er einen eingestellten Maximalpegel nicht überschreitet.

*Expander*\\
Ein Expander hat im Gegensatz zu einem Kompressor die Aufgabe, die Dynamik des Signals zu erhöhen, die Signalamplitude wird also oberhalb einer einstellbaren Grenze erhöht. Lüftergeräusche oder sonstige Störgeräusche in der Mikrofonaufnahme, die in den Pausen vom Nutzsignal nicht verdeckt werden, können durch einen Abwärts-Expander unter die Hörschwelle abgesenkt werden. Der Expander wirkt nur in den Pausen und trennt damit Nutzsignal vom Störsignal. Auch beim Expander ist ein Einsatzpunkt oder Schwelle (engl. Treshold) und ein Faktor (engl. ratio) einzustellen. Die Release Time bestimmt die Zeit, innerhalb der die Ausblendung auf Normalposition zurückgeregelt werden soll.

Bitte schauen Sie sich nun dieses Video an http://www.delamar.de/musikproduktion/video-workshop-effekt-grundlagen-expander-noise-gate-2875/.

*** Adobe Soundbooth
https://bildungsportal.sachsen.de/opal/auth/RepositoryEntry/1006567462/CourseNode/86920580767669
** Selbsttest
1. Diskretisierung Lückentext
   - Auf dem originalen wert- und zeitkontinuierlichen   *Ausgangssignal*  werden zwei Diskretisierungsschritte vorgenommen. Zunächst wird das Signal abgetastet, also zeitdiskretisiert. Daneben wird die Anzahl der Werte reduziert, die ein Signal annehmen darf. Man spricht hier von *Quantisierung*  . Aus der Wellenform wird eine *Treppenform*  , es entsteht ein wertdiskretes und zeitkontinuierliches Signal. Generell gilt: Je weniger Treppenstufen ein Signal annehmen darf, desto  *kleiner*  wird seine Codierung. Allerdings verschlechtert sich auch die   *Qualität*  entsprechend. D.h. im Umkehrschluss: Je feiner, desto qualitativ hochwertiger und desto   *größer*  die entstehenden Daten.
2. Was zählt nicht zu Filtern bzw. Effektgeräten?
   - @@html:<font color = "red">@@X Reverb@@html:</font>@@
   - @@html:<font color = "red">@@X Kompressor@@html:</font>@@
   - @@html:<font color = "red">@@X Delay@@html:</font>@@
   - @@html:<font color = "red">@@X Limiter@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Aux@@html:</font>@@
   - @@html:<font color = "red">@@X Gate@@html:</font>@@
3. Nennen Sie Richtcharakteristiken eines Mikrofons!
   - @@html:<font color = "red">@@X Superquader@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Keule@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Kugel@@html:</font>@@
   - @@html:<font color = "red">@@X Ball@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Hyperniere@@html:</font>@@
   - @@html:<font color = "red">@@X Neun@@html:</font>@@
   - @@html:<font color = "red">@@X Sieben@@html:</font>@@
4. Was bedeutet die Bezeichnung 32/8/2-Mischpult?
   - @@html:<font color = "red">@@X Die Bezeichnung bedeutet 32 Subgruppen mit 8 Eingangskanäle und eine Stereo-Summe.@@html:</font>@@
   - @@html:<font color = "red">@@X Die Bezeichnung bedeutet 32 Ausgangskanäle mit 8 Subgruppen und eine Stereo-Summe.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Die Bezeichnung bedeutet 32 Eingangskanäle mit 8 Subgruppen und eine Stereo-Summe.@@html:</font>@@
   - @@html:<font color = "red">@@X Die Bezeichnung bedeutet 32 Ausgangskanälen mit 8 Stereo-Summen und 1 Subgruppe.@@html:</font>@@
5. Was ist Bestandteil der Gehörknöchelchen?
   - @@html:<font color = "red">@@X Stäbchen@@html:</font>@@
   - @@html:<font color = "red">@@X Retina@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Amboß@@html:</font>@@
   - @@html:<font color = "red">@@X Scala vestibuli@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Hammer@@html:</font>@@
   - @@html:<font color = "red">@@X Scala media@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Steigbügel@@html:</font>@@
6. Wo liegt der für den Menschen wahrnehmbare Frequenzbereich?
   - zwischen 20 bis 20.000 Hz
7. Maskierung Lückentext
   - *Zeitliche* Vorwärtsmaskierung: Der Effekt der   *Maskierung*  ist nicht auf die Dimensionen   Frequenz  und Schalldruck beschränkt, sondern wird auch von der zeitlichen Dimension beeinflusst. Laute  *Frequenzen*  wirken sich nicht nur zum Zeitpunkt ihres Auftretens auf die benachbarten Frequenzen aus, sondern auch noch verzögert. Bis zu 200 Millisekunden   *später*  kann diese Maskierung nachfolgende Schallereignisse übertönen.
8. Eine  Wave-Datei  besteht  aus  einem  Kopf  und  einem  Rumpf.  Im  Kopf  werden  die  Informationen  abgelegt, die einem Abspielgerät mitteilen, wie die Audiodaten im Rumpf codiert wurden. MP3-Dateien  haben  keinen  einzelnen  Kopf,  sondern  bringen  diese  Information  in  der  Datei  gleich  mehrfach in immerhin 32 Bit großen Bereichen unter. Dadurch entstehen Redundanzen, die die Datei  größer werden lassen. Weshalb sind diese Redundanzen dennoch erwünscht?
   - @@html:<font color = "red">@@X Die Beseitigung der Redundanzen wäre zu aufwändig.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Netzwerkausfälle und Übertragungsfehler können kompensiert werden.@@html:</font>@@
   - @@html:<font color = "red">@@X Keiner weiß von den Redundanzen, daher wird auch nichts dagegen unternommen.@@html:</font>@@
   - @@html:<font color = "red">@@X Redundanzen verhindern den Absturz des eigenen PCs beim Streaming.@@html:</font>@@
   - @@html:<font color = "green">@@\checkmark Ein Hörer kann sich in einen Broadcast‐Stream zuschalten, z.B. beim Internetradio.@@html:</font>@@
   - @@html:<font color = "red">@@X Die redundanten Informationen werden benötigt, damit PCs über eine Remotedesktopverbindung gesteuert werden können.@@html:</font>@@
   - @@html:<font color = "red">@@X Technisch gibt es keine Möglichkeit, diese Redundanzen zu entfernen.@@html:</font>@@
9. Nennen Sie drei Dateiformate für Audiomaterial!
   - MP3, ADA, Wave
10. Welche dieser Aussagen sind korekt?
    - @@html:<font color = "red">@@X Schall breitet sich bevorzugt im Vakuum aus.@@html:</font>@@
    - @@html:<font color = "green">@@\checkmark Das menschliche Gehör besteht aus Außen, Mittel- und Innenohr.@@html:</font>@@
    - @@html:<font color = "red">@@X Bei einer Dauerbelastung mit Pegeln über 85 Dezibel oder einer Kurzzeitbelastung mit 130 Dezibel hören wir besonders gut.@@html:</font>@@
    - @@html:<font color = "red">@@X Durch das monaurale Hören ist eine genaue Lokalisation der Schallquelle möglich und somit kann räumliches Hören entstehen.@@html:</font>@@
    - @@html:<font color = "green">@@\checkmark Die Umwandlung der Schallwellen in Nervenimpulse erfolgt im Innenohr.@@html:</font>@@
    - @@html:<font color = "red">@@X Scala vestibuli und scala tympani sind über die Pinna miteinander verbunden.@@html:</font>@@
    - @@html:<font color = "green">@@\checkmark Die Tonhöhenempfindung ist eine Positionsempfindung. Die Sinneszellen auf der Basilarmembran sind nur für die ihnen aufgrund ihrer Position zugeordneten Frequenzen empfindlich.@@html:</font>@@
    - @@html:<font color = "red">@@X Bei der Frequenzmaskierung überlagern leise Frequenzen benachbarte lautere Frequenzen.@@html:</font>@@
    - @@html:<font color = "green">@@\checkmark Die äußeren Haarzellen wirken bei leisen Signalen verstärkend und bei lauten dämpfend.@@html:</font>@@
11. Psychoakustik Lückentext
    - Mit   *Reiz*  bezeichnen wir eine physikalische Erregung eines unserer Sinnesorgane. Diese Erregung ist spezifisch auf die einzelnen Sinnesorgane ausgerichtet. So reagiert das Auge auf Lichtreize und das  *Ohr*  auf Schall. Die Haut wiederum reagiert auf Berührung mit anderen Körpern. In jedem Fall muss der Reiz eine gewisse Stärke haben, um eine Reaktion auszulösen. Man spricht hier von der   *Reizschwelle*  (stimulus threshold), die überschritten werden muss.
